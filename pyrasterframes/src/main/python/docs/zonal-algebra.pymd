# Zonal Map Algebra

```python setup, echo=False
from IPython.display import display

from docs import folium_map

import pyrasterframes
from pyrasterframes.rasterfunctions import *
import pyrasterframes.rf_ipython

import folium 

from pyspark.sql.functions import udf, lit
from geomesa_pyspark.types import MultiPolygonUDT

# This job is more memory bound, so reduce the concurrent tasks.
spark = pyrasterframes.get_spark_session("local[4]")
```

## Definition

Zonal map algebra refers to operations over raster cells based on the definition of a _zone_. In concept, a _zone_ is like a mask: a raster with a special value designating membership of the cell in the zone. In general, we assume that _zones_ are defined by @ref[vector geometries](vector-data.md).

## Analysis Plan

We will compute _a stat_ on NDVI for two national parks

```python get_park_boundary
import requests
nps_filepath = '/tmp/2parks.geojson'
nps_data_query_url = 'https://services1.arcgis.com/fBc8EJBxQRMcHlei/arcgis/rest/services/' \
                     'NPS_Park_Boundaries/FeatureServer/0/query' \
                     '?geometry=-87.601,40.923,-81.206,41.912&inSR=4326&outSR=4326' \
                     "&where=UNIT_TYPE='National Park'&outFields=*&f=geojson"
r = requests.get(nps_data_query_url)
with open(nps_filepath,'wb') as f:
    f.write(r.content)
```

```python folium, results='paradox', echo=False
folium_map(nps_filepath)
```

Now we read the park boundary vector data as a Spark DataFrame using the built-in @ref:[geojson DataSource](vector-data.md#geojson-datasource). The geometry is very detailed, and the EO cells are relatively coarse. To speed up the processing, the geometry is "simplified" by combining vertices within about 100 meters of each other. For more on this see the section on Shapely support in @ref:[user defined functions](vector-data.md#shapely-geometry-support).

```python read_cuya_vector
park_vector = spark.read.geojson(nps_filepath)

@udf(MultiPolygonUDT())
def simplify(g, tol):
    return g.simplify(tol)

park_vector = park_vector.withColumn('geo_simp', simplify('geometry', lit(0.005))) \
                         .select('geo_simp', 'OBJECTID', 'UNIT_NAME') \
                         .hint('broadcast')
```

## Catalog Read

Both parks are entirerly contained in MODIS granule h11 v04. We will simply filter on this granule, rather than using a @ref:[spatial relation](vector-data.md#geomesa-functions-and-spatial-relations). The time period selected should contain the highest plant vigor of the late spring and early summer seasons. 

```python query_catalog
cat = spark.read.format('aws-pds-modis-catalog').load().repartition(200)
park_cat = cat \
            .filter(
                    (cat.granule_id == 'h11v04') &
                    (cat.acquisition_date >= lit('2018-07-01')) &
                    (cat.acquisition_date < lit('2018-08-01'))            
                    ) \
            .crossJoin(park_vector)
                
park_cat.printSchema()
```

Now we have a catalog with several months of MODIS data for a single granule. However, the granule is larger than our park boundary. We will combine the park geometry with the catalog, and read only the bands of interest to compute NDVI, which we discussed in a @ref:[previous section](local-algebra.md#computing-ndvi).

We then [reproject](https://gis.stackexchange.com/questions/247770/understanding-reprojection) the park geometry to the same @ref:[CRS](concepts.md#coordinate-reference-system--crs-) as the imagery. Then we will filter to only the _tiles_ intersecting the park.

```python read_catalog
raster_cols = ['B01', 'B02',] # red and near-infrared respectively
park_rf = spark.read.raster(
        park_cat.select(['acquisition_date', 'granule_id'] + raster_cols + park_vector.columns),
        catalog_col_names=raster_cols) \
    .withColumn('park_native', st_reproject('geo_simp', lit('EPSG:4326'), rf_crs('B01'))) \
    .filter(st_intersects('park_native', rf_geometry('B01'))) 

park_rf.printSchema()
```

## Define Zone

Now we have the vector representation of the park boundary alongside the _tiles_ of red and near infrared bands. Next, we need to create a _tile_ representation of the park to allow us to limit the raster analysis to pixels within the park _zone_. This is similar to the masking operation demonstrated in @ref:[NoData handling](nodata-handling.md#masking).

We do this using two transformations. The first one will reproject the park boundary from coordinates to the MODIS sinusoidal projection. The second one will create a new _tile_ aligned with the imagery containing a value of 1 where the pixels are contained within the park _zone_ and NoData elsewhere. 

```python burn_in
rf_park_tile = park_rf \
    .withColumn('dims', rf_dimensions('B01')) \
    .withColumn('park_zone_tile', rf_rasterize('park_native', rf_geometry('B01'), 'OBJECTID', 'dims.cols', 'dims.rows')) \
    .persist()

rf_park_tile.printSchema()
```

## Compute Zonal Statistics

Next, we will compute NDVI as the normalized difference of near infrared (band 2) and red (band 1). The _tiles_ are masked by the `park_zone_tile`, limiting the cells to those in the _zone_. Next, we compute our desired statistics over the NVDI _tiles_ that are limited by the _zone_.

```python ndvi_zonal
from pyspark.sql.functions import col
from pyspark.sql import functions as F

rf_ndvi = rf_park_tile \
    .withColumn('ndvi', rf_normalized_difference('B02', 'B01')) \
    .withColumn('ndvi_masked', rf_mask('ndvi', 'park_zone_tile'))

zonal_max = rf_ndvi \
        .groupby('UNIT_NAME') \
        .agg(F.max(rf_tile_max('ndvi'))) \

zonal_max
```

