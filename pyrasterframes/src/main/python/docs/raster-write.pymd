# Writing Raster Data

RasterFrames is oriented toward large scale analyses of spatial data. The primary output of these analyses could be a @ref:[statistical summary](aggregation.md), a @ref:[machine learning model](machine-learning.md), or some other result that is generally much smaller than the input data set.

However, there are times in any analysis where writing a representative sample of the work in progress provides invaluable feedback on the current state of the process and results.

```python imports, echo=False
import pyrasterframes
from pyrasterframes.rasterfunctions import *
from IPython.display import display
import os.path

spark = pyrasterframes.get_spark_session()
```

## Tile Samples

When collecting a _tile_ (see discussion of the RasterFrame @ref:[schema](raster-read.md#single-raster) for orientation to the concept) to the Python Spark driver, we have some convenience methods to quickly visualize the _tile_.

In an IPython or Jupyter interpreter, a `Tile` object will be displayed as an image with limited metadata.

```python tile_sample
def scene(band):
    b = str(band).zfill(2) # converts int 2 to '02'
    return 'https://modis-pds.s3.amazonaws.com/MCD43A4.006/11/08/2019059/' \
             'MCD43A4.A2019059.h11v08.006.2019072203257_B{}.TIF'.format(b)
spark_df = spark.read.raster(scene(2), tile_dimensions=(128, 128))
tile = spark_df.select(rf_tile('proj_raster').alias('tile')).first()['tile']
tile
```

```python display_tile, echo=False, output=True
display(tile) # IPython.display function
```

## DataFrame Samples

Within an IPython or Jupyter interpreter, a Pandas DataFrame containing a column of _tiles_ will be rendered as the samples discussed above. Simply import the `rf_ipython` submodule to enable enhanced HTML rendering of a Pandas DataFrame.

In the example below, notice the result is limited to a small subset. For more discussion about why this is important, see the @ref:[Pandas and NumPy discussion](numpy-pandas.md).

```python to_pandas, evaluate=True
import pyrasterframes.rf_ipython

pandas_df = spark_df.select(
                    rf_extent('proj_raster').alias('extent'),
                    rf_tile('proj_raster').alias('tile'),
                ).limit(5).toPandas()
```

Viewing the DataFrame in Jupyter looks like this.

```python show_pandas, evaluate=False
pandas_df
```

@@include[df-samples-output.md](static/df-samples-output.md)

## GeoTIFFs

GeoTIFF is one of the most common file formats for spatial data, providing flexibility in data encoding, representation, and storage. RasterFrames provides a specialized Spark DataFrame writer for rendering a RasterFrame to a GeoTIFF.

One downside to GeoTIFF is that it is not a big data native format. To create a GeoTIFF all the data to be encoded has to be in the memory of one compute node (in Spark parlance, this is a "collect"), limiting it's maximum size substantially compared to that of a full cluster environment. When rendering GeoTIFFs in RasterFrames, you either need to specify the dimensions of the output raster, or be aware of how big the collected data will end up being.

Fortunately, we can use the cluster computing capability to downsample the data into a more manageable size. For sake of example, let's render an overview our scene's red band as a small raster, reprojecting it to latitude and longitude coordinates on the [WGS84](https://en.wikipedia.org/wiki/World_Geodetic_System) reference ellipsoid (aka [EPSG:4326](https://spatialreference.org/ref/epsg/4326/)). 

```python write_geotiff
outfile = os.path.join('/tmp', 'geotiff-overview.tif')
spark_df.write.geotiff(outfile, crs='EPSG:4326', raster_dimensions=(256, 256))
```

View it with `rasterio` to check the results:

```python view_geotiff
import rasterio
from rasterio.plot import show, show_hist

with rasterio.open(outfile) as src:
    show(src, adjust='linear')
    show_hist(src, bins=50, lw=0.0, stacked=False, alpha=0.6,
        histtype='stepfilled', title="Overview Histogram")
```

If there are many tile or projected raster columns in the DataFrame, the GeoTIFF writer will write each one as a separate band in the file. Each band in the output will be tagged the input column names for reference.

```python, echo=False
os.remove(outfile)
```

## GeoTrellis Layers

[GeoTrellis][GeoTrellis] is one of the key libraries that RasterFrames builds upon. It provides a Scala language API to working with large raster data with Apache Spark. Ingesting raster data into a Layer is one of the key concepts for creating a dataset for processing on Spark. RasterFrames write data from an appropriate DataFrame into a [GeoTrellis Layer](https://geotrellis.readthedocs.io/en/latest/guide/tile-backends.html). RasterFrames provides a `geotrellis` DataSource that supports both reading and writing of GeoTrellis layers.

> An example is forthcoming.

## Parquet

You can write the Spark DataFrame to an [Apache Parquet][Parquet] "file". This format is designed to work across different projects in the Hadoop ecosystem. It also provides a variety of optimizations for query against data written in the format.

```python write_parquet, evaluate=False
spark_df.withColumn('exp', rf_expm1('proj_raster')) \
    .write.mode('append').parquet('hdfs:///rf-user/sample.pq')
```

[GeoTrellis]: https://geotrellis.readthedocs.io/en/latest/
[Parquet]: https://spark.apache.org/docs/latest/sql-data-sources-parquet.html
