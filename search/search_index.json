{"docs":[{"location":"/paradox.json","text":"","title":""},{"location":"/index.html","text":"","title":"RasterFrames"},{"location":"/index.html#rasterframes","text":"RasterFrames® brings together Earth-observation (EO) data access, cloud computing, and DataFrame-based data science. The recent explosion of EO data from public and private satellite operators presents both a huge opportunity and a huge challenge to the data analysis community. It is Big Data in the truest sense, and its footprint is rapidly getting bigger.\nRasterFrames provides a DataFrame-centric view over arbitrary geospatial raster data, enabling spatiotemporal queries, map algebra raster operations, and interoperability with Spark ML. By using the DataFrame as the core cognitive and compute data model, RasterFrames is able to deliver an extensive set of functionality in a form that is both horizontally scalable as well as familiar to general analysts and data scientists. It provides APIs for Python, SQL, and Scala.\nThrough its custom Spark DataSource, RasterFrames can read various raster formats – including GeoTIFF, JP2000, MRF, and HDF – and from an array of services, such as HTTP, FTP, HDFS, S3 and WASB. It also supports reading the vector formats GeoJSON and WKT/WKB. RasterFrame contents can be filtered, transformed, summarized, resampled, and rasterized through 200+ raster and vector functions.\nAs part of the LocationTech family of projects, RasterFrames builds upon the strong foundations provided by GeoMesa (spatial operations) , GeoTrellis (raster operations), JTS (geometry modeling) and SFCurve (spatiotemporal indexing), integrating various aspects of these projects into a unified, DataFrame-centric analytics package.","title":"RasterFrames"},{"location":"/index.html#license","text":"RasterFrames is released under the commercial-friendly Apache 2.0 open source license.\nTo learn more, please see the Getting Started section of this manual.\nThe source code can be found on GitHub at locationtech/rasterframes.","title":"License"},{"location":"/index.html#commercial-support","text":"As the sponsors and developers of RasterFrames, Astraea, Inc. is uniquely positioned to expand its capabilities. If you need additional functionality or just some architectural guidance to get your project off to the right start, we can provide a full range of consulting and development services around RasterFrames. We can be reached at info@astraea.io.","title":"Commercial Support"},{"location":"/index.html#related-links","text":"Gitter Channel Scala API Documentation GitHub Repository Astraea, Inc., the company behind RasterFrames","title":"Related Links"},{"location":"/index.html#detailed-contents","text":"Overview Context Benefit Architecture Getting Started pip install pyrasterframes Next Steps Other Options Using Jupyter Notebook Using pyspark shell Scala Development Installing GDAL Support Installing on MacOS Installing on Linux Testing For GDAL Concepts Raster Cell Cell Type NoData Scene Band Coordinate Reference System (CRS) Extent Tile Raster Data I/O Raster Catalogs Creating a Catalog Using External Catalogs Using Built-in Catalogs Reading Raster Data Single Rasters Multiple Singleband Rasters Multiband Rasters URI Formats Raster Catalogs Lazy Raster Reading Spatial Indexing and Partitioning GeoTrellis Writing Raster Data IPython/Jupyter Overview Rasters GeoTIFFs GeoTrellis Layers Parquet Vector Data GeoJSON DataSource GeoPandas and RasterFrames Shapely Geometry Support GeoMesa Functions and Spatial Relations Raster Processing Local Map Algebra Computing NDVI “NoData” Handling What is NoData? Cell Types NoData and Local Arithmetic Changing a Tile’s NoData Values Combining Tiles with Different Data Types NoData Values in Aggregation Masking Masking Sentinel 2 Masking Landsat 8 Clipping Zonal Map Algebra Definition Analysis Plan Get Vector Data Catalog Read Define Zone Tiles Compute Zonal Statistics Aggregation Tile Mean Example Cell Counts Example Statistical Summaries Histogram Time Series Analysis Plan Catalog Read Vector and Raster Data Interaction Create Time Series Raster Join Description Example Code Additional Options Machine Learning Unsupervised Machine Learning Imports and Data Preparation Create ML Pipeline Fit the Model and Score Visualize Prediction Supervised Machine Learning Create and Read Raster Catalog Data Prep Masking Poor Quality Cells Create ML Pipeline Train the Model Model Evaluation Visualize Prediction NumPy and Pandas Performance Considerations The Tile Class DataFrame toPandas User Defined Functions Creating a Spark DataFrame IPython/Jupyter Extensions Tile Samples DataFrame Samples Changing Number of Rows Pandas Sample Colorization Color Composite Custom Color Ramp Scala and SQL Python Step 1: Load the catalog Step 2: Down-select data by month Step 3: Read tiles Step 4: Compute aggregates SQL Step 1: Load the catalog Step 2: Down-select data by month Step 3: Read tiles Step 4: Compute aggregates Scala Step 1: Load the catalog Step 2: Down-select data by month Step 3: Read tiles Step 4: Compute aggregates Function Reference List of Available SQL and Python Functions Vector Operations st_reproject st_extent st_geometry rf_xz2_index rf_z2_index Tile Metadata and Mutation rf_dimensions rf_cell_type rf_tile rf_extent rf_crs rf_proj_raster rf_mk_crs rf_convert_cell_type rf_interpret_cell_type_as rf_resample Tile Creation rf_make_zeros_tile rf_make_ones_tile rf_make_constant_tile rf_rasterize rf_array_to_tile rf_assemble_tile Masking and NoData rf_mask rf_mask_by_value rf_mask_by_values rf_mask_by_bit rf_mask_by_bits rf_inverse_mask rf_inverse_mask_by_value rf_is_no_data_tile rf_local_no_data rf_local_data rf_local_data rf_with_no_data Local Map Algebra rf_local_add rf_local_subtract rf_local_multiply rf_local_divide rf_normalized_difference rf_local_less rf_local_less_equal rf_local_greater rf_local_greater_equal rf_local_equal rf_local_unequal rf_local_is_in rf_local_extract_bits rf_local_min rf_local_max rf_local_clamp rf_where rf_rescale rf_standardize rf_round rf_abs rf_exp rf_exp10 rf_exp2 rf_expm1 rf_log rf_log10 rf_log2 rf_log1p rf_sqrt Tile Statistics rf_tile_sum rf_tile_mean rf_tile_min rf_tile_max rf_no_data_cells rf_data_cells rf_exists rf_for_all rf_tile_stats rf_tile_histogram Aggregate Tile Statistics rf_agg_mean rf_agg_data_cells rf_agg_no_data_cells rf_agg_stats rf_agg_approx_histogram rf_agg_approx_quantiles rf_agg_extent rf_agg_reprojected_extent Tile Local Aggregate Statistics rf_agg_local_max rf_agg_local_min rf_agg_local_mean rf_agg_local_data_cells rf_agg_local_no_data_cells rf_agg_local_stats Converting Tiles rf_explode_tiles rf_explode_tiles_sample rf_tile_to_array_int rf_tile_to_array_double rf_render_ascii rf_render_matrix rf_render_png rf_render_color_ramp_png rf_agg_overview_raster rf_rgb_composite Release Notes 0.9.x 0.9.1 0.9.0 0.8.x 0.8.5 0.8.4 0.8.3 0.8.2 0.8.1 0.8.0 0.7.x 0.7.1 0.7.0 0.6.x 0.6.1 0.6.0 0.5.x 0.5.12 0.5.11 0.5.10 0.5.9 0.5.8 0.5.7 0.5.6 0.5.5","title":"Detailed Contents"},{"location":"/description.html","text":"","title":"Overview"},{"location":"/description.html#overview","text":"RasterFrames® provides a DataFrame-centric view over arbitrary Earth-observation (EO) data, enabling spatiotemporal queries, map algebra raster operations, and compatibility with the ecosystem of Apache Spark ML algorithms. It provides APIs in Python, SQL, and Scala, and can scale from a laptop computer to a large distributed cluster, enabling global analysis with satellite imagery in a wholly new, flexible, and convenient way.","title":"Overview"},{"location":"/description.html#context","text":"We have a millennia-long history of organizing information in tabular form. Typically, rows represent independent events or observations, and columns represent attributes and measurements from the observations. The forms have evolved, from hand-written agricultural records and transaction ledgers, to the advent of spreadsheets on the personal computer, and on to the creation of the DataFrame data structure as found in R Data Frames and Python Pandas. The table-oriented data structure remains a common and critical component of organizing data across industries, and—most importantly—it is the mental model employed by data scientists across diverse forms of modeling and analysis.\nThe evolution of the DataFrame form has continued with Spark SQL, which brings DataFrames to the big data distributed compute space. Through several novel innovations, Spark SQL enables data scientists to work with DataFrames too large for the memory of a single computer. As suggested by the name, these DataFrames are manipulatable via standard SQL, as well as the more general-purpose programming languages Python, R, Java, and Scala.\nRasterFrames, an incubating Eclipse Foundation LocationTech project, brings together EO data access, cloud computing, and DataFrame-based data science. The recent explosion of EO data from public and private satellite operators presents both a huge opportunity and a huge challenge to the data analysis community. It is Big Data in the truest sense, and its footprint is rapidly getting bigger. According to a World Bank document on assets for post-disaster situation awareness[^1]:\nOf the 1,738 operational satellites currently orbiting the earth (as of 9/[20]17), 596 are earth observation satellites and 477 of these are non-military assets (i.e. available to civil society including commercial entities and governments for earth observation, according to the Union of Concerned Scientists). This number is expected to increase significantly over the next ten years. The 200 or so planned remote sensing satellites have a value of over 27 billion USD (Forecast International). This estimate does not include the burgeoning fleets of smallsats as well as micro, nano and even smaller satellites… All this enthusiasm has, not unexpectedly, led to a veritable fire-hose of remotely sensed data which is becoming difficult to navigate even for seasoned experts.","title":"Context"},{"location":"/description.html#benefit","text":"By using DataFrames as the core cognitive and compute data model for processing EO data, RasterFrames is able to deliver sophisticated computational and algorithmic capabilities in a tabular form that is familiar and accessible to the general computing public. Because it is built on Apache Spark, solutions prototyped on a laptop can be easily scaled to run on cluster and cloud compute resources. Apache Spark also provides integration between its DataFrame libraries and machine learning, with which RasterFrames is fully compatible.","title":"Benefit"},{"location":"/description.html#architecture","text":"RasterFrames builds upon several other LocationTech projects: GeoTrellis, GeoMesa, JTS, and SFCurve.\nRasterFrames introduces georectified raster imagery to Spark SQL. It quantizes scenes into chunks called tiles. Each tile contains a 2-D matrix of cell or pixel values along with information on how to numerically interpret those cells.\nAs shown in the figure below, a “RasterFrame” is a Spark DataFrame with one or more columns of type tile. A tile column typically represents a single frequency band of sensor data, such as “blue” or “near infrared”, but can also be quality assurance information, land classification assignments, or any other raster spatial data. Along with tile columns there is typically an extent specifying the geographic location of the data, the map projection of that geometry (crs), and a timestamp column representing the acquisition time. These columns can all be used in the WHERE clause when filtering.\ntimestamp crs extent tile 2019-02-28 [+proj=sinu +lon_0=0.0 +x_0=0.0 +y_0=0.0 +a=6371007.181 +b=6371007.181 +units=m ] [-6834789.194217826, 163086.07621782666, -6716181.138786679, 281694.1316489733] 2019-02-28 [+proj=sinu +lon_0=0.0 +x_0=0.0 +y_0=0.0 +a=6371007.181 +b=6371007.181 +units=m ] [-7427829.47137356, 281694.1316489733, -7309221.415942413, 400302.18708011997] 2019-02-28 [+proj=sinu +lon_0=0.0 +x_0=0.0 +y_0=0.0 +a=6371007.181 +b=6371007.181 +units=m ] [-7309221.415942414, 163086.07621782666, -7190613.360511267, 281694.1316489733] 2019-02-28 [+proj=sinu +lon_0=0.0 +x_0=0.0 +y_0=0.0 +a=6371007.181 +b=6371007.181 +units=m ] [-7665045.582235853, 637518.2979424134, -7546437.526804706, 756126.35337356]\nRasterFrames also includes support for working with vector data, such as GeoJSON. RasterFrames vector data operations let you filter with geospatial relationships like contains or intersects, mask cells, convert vectors to rasters, and more.\nRaster data can be read from a number of sources. Through the flexible Spark SQL DataSource API, RasterFrames can be constructed from collections of imagery (including Cloud Optimized GeoTIFFs or COGS), GeoTrellis Layers, and from catalogs of large datasets like Landsat 8 and MODIS data sets on the AWS Public Data Set (PDS).\n[^1]: Demystifying Satellite Assets for Post-Disaster Situation Awareness. World Bank via OpenDRI.org. Accessed November 28, 2018.","title":"Architecture"},{"location":"/getting-started.html","text":"","title":"Getting Started"},{"location":"/getting-started.html#getting-started","text":"Note If you are new to Earth-observing imagery, you might consider looking at the Concepts section first.\nRasterFrames® is a geospatial raster processing library for Python, Scala and SQL, available through several mechanisms.\nThe simplest way to get started with RasterFrames is via the Docker image, or from the Python shell. To get started with the Python shell you will need:\nPython installed. Version 3.6 or greater is recommended. pip installed. If you are using Python 3, pip may already be installed. Java JDK 8 installed on your system and java on your system PATH or JAVA_HOME pointing to a Java installation.","title":"Getting Started"},{"location":"/getting-started.html#pip-install-pyrasterframes","text":"$ python3 -m pip install pyrasterframes\nThen in a python interpreter of your choice, you can get a pyspark SparkSession using the local[*] master.\nimport pyrasterframes\nfrom pyrasterframes.utils import create_rf_spark_session\nspark = create_rf_spark_session()\nThen, you can read a raster and work with it in a Spark DataFrame.\nfrom pyrasterframes.rasterfunctions import *\nfrom pyspark.sql.functions import lit\n\n# Read a MODIS surface reflectance granule\ndf = spark.read.raster('https://modis-pds.s3.amazonaws.com/MCD43A4.006/11/08/2019059/MCD43A4.A2019059.h11v08.006.2019072203257_B02.TIF')\n\n# Add 3 element-wise, show some rows of the DataFrame\nsample = df.withColumn('added', rf_local_add(df.proj_raster, lit(3))) \\\n  .select(rf_crs('added'), rf_extent('added'), rf_tile('added'))\nsample\nShowing only top 5 rows.\nrf_crs(added) rf_extent(added) rf_tile(added) [+proj=sinu +lon_0=0.0 +x_0=0.0 +y_0=0.0 +a=6371007.181 +b=6371007.181 +units=m ] [-7072005.3050801195, 993342.4642358534, -6953397.249648972, 1111950.519667] [+proj=sinu +lon_0=0.0 +x_0=0.0 +y_0=0.0 +a=6371007.181 +b=6371007.181 +units=m ] [-7546437.526804707, 163086.07621782666, -7427829.47137356, 281694.1316489733] [+proj=sinu +lon_0=0.0 +x_0=0.0 +y_0=0.0 +a=6371007.181 +b=6371007.181 +units=m ] [-6834789.194217826, 281694.1316489733, -6716181.138786679, 400302.18708011997] [+proj=sinu +lon_0=0.0 +x_0=0.0 +y_0=0.0 +a=6371007.181 +b=6371007.181 +units=m ] [-7427829.47137356, 163086.07621782666, -7309221.415942413, 281694.1316489733] [+proj=sinu +lon_0=0.0 +x_0=0.0 +y_0=0.0 +a=6371007.181 +b=6371007.181 +units=m ] [-6953397.249648973, 874734.4088047068, -6834789.194217826, 993342.4642358534]\nThis example is extended in the getting started Jupyter notebook.","title":"pip install pyrasterframes"},{"location":"/getting-started.html#next-steps","text":"To understand more about how and why RasterFrames represents Earth observation in DataFrames, read about the core concepts and the project description. For more hands-on examples, see the chapters about reading and processing with RasterFrames.","title":"Next Steps"},{"location":"/getting-started.html#other-options","text":"You can also use RasterFrames in the following environments:\nJupyter Notebook pyspark shell","title":"Other Options"},{"location":"/getting-started.html#using-jupyter-notebook","text":"RasterFrames provides a Docker image for a Jupyter notebook server whose default kernel is already set up for running RasterFrames. To use it:\nInstall Docker Pull the image: docker pull s22s/rasterframes-notebook Run a container with the image, for example: docker run -p 8808:8888 -p 44040:4040 -v /path/to/notebooks:/home/jovyan/work rasterframes-notebook:latest In a browser, open localhost:8808 in the example above.\nSee the RasterFrames Notebook README for instructions on building the Docker image for this Jupyter notebook server.","title":"Using Jupyter Notebook"},{"location":"/getting-started.html#using-pyspark-shell","text":"You can use RasterFrames in a pyspark shell. To set up the pyspark environment, prepare your call with the appropriate --master and other --conf arguments for your cluster manager and environment. For RasterFrames support you need to pass arguments pointing to the various Java dependencies. You will also need the Python source zip, even if you have pip installed the package. You can download the source zip here: https://repo1.maven.org/maven2/org/locationtech/rasterframes/pyrasterframes_2.11/${VERSION}/pyrasterframes_2.11-${VERSION}-python.zip.\nThe pyspark shell command will look something like this.\npyspark \\\n    --master local[*] \\\n    --py-files pyrasterframes_2.11-${VERSION}-python.zip \\\n    --packages org.locationtech.rasterframes:rasterframes_2.11:${VERSION},org.locationtech.rasterframes:pyrasterframes_2.11:${VERSION},org.locationtech.rasterframes:rasterframes-datasource_2.11:${VERSION} \\\n    --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \\ # these configs improve serialization performance\n    --conf spark.kryo.registrator=org.locationtech.rasterframes.util.RFKryoRegistrator \\\n    --conf spark.kryoserializer.buffer.max=500m\nThen in the pyspark shell, import the module and call withRasterFrames on the SparkSession.\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.3.2\n      /_/\n\nUsing Python version 3.7.3 (default, Mar 27 2019 15:43:19)\nSparkSession available as 'spark'.\n>>> import pyrasterframes\n>>> spark = spark.withRasterFrames()\n>>> df = spark.read.raster('https://landsat-pds.s3.amazonaws.com/c1/L8/158/072/LC08_L1TP_158072_20180515_20180604_01_T1/LC08_L1TP_158072_20180515_20180604_01_T1_B5.TIF')\nNow you have the configured SparkSession with RasterFrames enabled.","title":"Using pyspark shell"},{"location":"/getting-started.html#scala-development","text":"There is first-class support for Scala in RasterFrames. See the Scala and SQL page for an example application, and the Scala API Documentation for function details.\nIf you would like to use RasterFrames in Scala, you’ll need to add the following resolvers and dependencies to your sbt project:\nresolvers ++= Seq(\n      \"Azavea Public Builds\" at \"https://dl.bintray.com/azavea/geotrellis\",\n      \"locationtech-releases\" at \"https://repo.locationtech.org/content/groups/releases\"\n)\nlibraryDependencies ++= Seq(\n    \"org.locationtech.rasterframes\" %% \"rasterframes\" % ${VERSION},\n    \"org.locationtech.rasterframes\" %% \"rasterframes-datasource\" % ${VERSION},\n    // This is optional. Provides access to AWS PDS catalogs.\n    \"org.locationtech.rasterframes\" %% \"rasterframes-experimental\" % ${VERSION}\n)\nRasterFrames is compatible with Spark 2.4.x.","title":"Scala Development"},{"location":"/getting-started.html#installing-gdal-support","text":"GDAL provides a wide variety of drivers to read data from many different raster formats. If GDAL is installed in the environment, RasterFrames will be able to read those formats. If you are using the Jupyter Notebook image, GDAL is already installed for you. Otherwise follow the instructions below. Version 2.4.1 or greater is required.","title":"Installing GDAL Support"},{"location":"/getting-started.html#installing-on-macos","text":"Using homebrew:\nbrew install gdal","title":"Installing on MacOS"},{"location":"/getting-started.html#installing-on-linux","text":"Using apt-get:\nsudo apt-get update\nsudo apt-get install gdal-bin","title":"Installing on Linux"},{"location":"/getting-started.html#testing-for-gdal","text":"gdalinfo --formats\nTo support GeoTIFF and JPEG2000 formats, you should look for the following drivers from the output above:\nGTiff -raster- (rw+vs): GeoTIFF JPEG2000 -raster,vector- (rwv): JPEG-2000 part 1 (ISO/IEC 15444-1), based on Jasper library\nDo the following to see if RasterFrames was able to find GDAL:\nfrom pyrasterframes.utils import gdal_version\nprint(gdal_version())\nThis will print out something like “GDAL x.y.z, released 20yy/mm/dd”. If it reports “not available”, then GDAL is not installed in a place where the RasterFrames runtime was able to find it. Please file an issue to get help resolving this problem.","title":"Testing For GDAL"},{"location":"/concepts.html","text":"","title":"Concepts"},{"location":"/concepts.html#concepts","text":"There are a number of Earth-observation (EO) concepts that crop up in the discussion of RasterFrames features. We’ll cover these briefly in the sections below. However, here are a few links providing a more extensive introduction to working with Earth observation data.\nFundamentals of Remote Sensing Newcomers Earth Observation Guide Earth Observation Markets and Applications","title":"Concepts"},{"location":"/concepts.html#raster","text":"A raster is a regular grid of numeric values. A raster can be thought of as an image, as is the case if the values in the grid represent brightness along a greyscale. More generally, a raster can measure many different phenomena or encode a variety of different discrete classifications.","title":"Raster"},{"location":"/concepts.html#cell","text":"A cell is a single row and column intersection in the raster grid. It is a single pixel in an image. A cell’s value often represents one sample from a sensor encoded as a scalar value associated with a specific location and time.","title":"Cell"},{"location":"/concepts.html#cell-type","text":"A numeric cell value may be encoded in a number of different computer numeric formats. There are typically three characteristics used to describe a cell type:\nword size (bit-width) signed vs unsigned integral vs floating-point\nThe most frequently encountered cell types in RasterFrames are below.\nName Abbreviation Description Range Byte int8 Signed 8-bit integral -128 to 127 Unsigned Byte uint8 Unsigned 8-bit integral 0 to 255 Short int16 Signed 16-bit integral -32,768 to 32,767 Unsigned Short uint16 Unsigned 16-bit integral 0 to 65,535 Int int32 Signed 32-bit integral -2,147,483,648 to 2,147,483,647 Unsigned Int uint32 Unsigned 32-bit integral 0 to 4,294,967,295 Float float32 32-bit floating-point -3.4028235E38 to 3.4028235E38 Double float64 64-bit floating-point -1.7976931348623157E308 to 1.7976931348623157E308\nSee the section on “NoData” Handling for additional discussion on cell types and more exhaustive coverage of available cell types.","title":"Cell Type"},{"location":"/concepts.html#nodata","text":"A “NoData” (or N/A) value is a specifically identified value for a cell type used to indicate the absence of data. See the section on “NoData” Handling for additional discussion on “NoData”.","title":"NoData"},{"location":"/concepts.html#scene","text":"A scene (or granule) is a discrete instance of EO raster data with a specific extent (region), date-time, and map projection (or CRS).","title":"Scene"},{"location":"/concepts.html#band","text":"A scene frequently defines many different measurements captured at the same date-time, over the same extent, and meant to be processed together. These different measurements are referred to as bands. The name comes from the varying bandwidths of light and electromagnetic radiation measured in many EO datasets.","title":"Band"},{"location":"/concepts.html#coordinate-reference-system-crs-","text":"A coordinate reference system (or spatial reference system) is a set of mathematical constructs used to translate locations on the three-dimensional surface of the earth to the two dimensional raster grid. A CRS typically accompanies any EO data so it can be precisely located.","title":"Coordinate Reference System (CRS)"},{"location":"/concepts.html#extent","text":"An extent (or bounding box) is a rectangular region specifying the geospatial coverage of a raster or tile, a two-dimensional array of cells within a single CRS.","title":"Extent"},{"location":"/concepts.html#tile","text":"A tile (sometimes called a “chip”) is a rectangular subset of a scene. As a scene is a raster, a tile is also a raster. A tile can conceptually be thought of as a two-dimensional array.\nSome EO data has many bands or channels. Within RasterFrames, this third dimension is handled across columns of the DataFrame, such that the tiles within DataFrames are all two-dimensional arrays.\nTiles are often square and the dimensions are some power of two, for example 256 by 256.\nThe tile is the primary discretization unit used in RasterFrames. The scene’s overall extent is carved up into smaller extents and spread across rows.","title":"Tile"},{"location":"/raster-io.html","text":"","title":"Raster Data I/O"},{"location":"/raster-io.html#raster-data-i-o","text":"The standard mechanism by which any data is brought in and out of a Spark Dataframe is the Spark SQL DataSource. RasterFrames provides specialized DataSources for geospatial raster data and maintains compatibility with existing general purpose DataSources, such as Parquet.\nCatalog Readers aws-pds-l8-catalog: built-in catalog over Landsat on AWS aws-pds-modis-catalog: built-in catalog over MODIS on AWS geotrellis-catalog: for enumerating GeoTrellis layers Raster Readers raster: the standard reader for most raster data, including single raster files or catalogs geotiff: a simplified reader for reading a single GeoTIFF file geotrellis: for reading a GeoTrellis layer Raster Writers geotiff: beta writer to GeoTiff file format geotrellis: creating a GeoTrellis layer parquet: general purpose writer for Parquet\nFurthermore, when in a Jupyter Notebook environment, you can view Tile and DataFrame samples.\nThere is also support for vector data for masking and data labeling.","title":"Raster Data I/O"},{"location":"/raster-catalogs.html","text":"","title":"Raster Catalogs"},{"location":"/raster-catalogs.html#raster-catalogs","text":"While interesting processing can be done on a single raster file, RasterFrames shines when catalogs of raster data are to be processed. In its simplest form, a catalog is a list of URLs referencing raster files. This list can be a Spark DataFrame, Pandas DataFrame, CSV file or CSV string. The catalog is input into the raster DataSource described in the next page, which creates tiles from the rasters at the referenced URLs.\nA catalog can have one or two dimensions:\nOne-D: A single column contains raster URLs across the rows. All referenced rasters represent the same band. For example, a column of URLs to Landsat 8 near-infrared rasters covering Europe. Each row represents different places and times. Two-D: Many columns contain raster URLs. Each column references the same band, and each row represents the same place and time. For example, red-, green-, and blue-band columns for scenes covering Europe. Each row represents a single scene with the same resolution, extent, CRS, etc across the row.","title":"Raster Catalogs"},{"location":"/raster-catalogs.html#creating-a-catalog","text":"This section will provide some examples of catalogs creation, as well as introduce some experimental catalogs built into RasterFrames. Reading raster data represented by a catalog is covered in more detail in the next page.","title":"Creating a Catalog"},{"location":"/raster-catalogs.html#one-d","text":"A single URL is the simplest form of a catalog.\nfile_uri = \"/data/raster/myfile.tif\"\n# Pandas DF\nmy_cat = pd.DataFrame({'B01': [file_uri]})\n\n# equivalent Spark DF\nfrom pyspark.sql import Row\nmy_cat = spark.createDataFrame([Row(B01=file_uri)])\n\n#equivalent CSV string\nmy_cat = \"B01\\n{}\".format(file_uri)\nA single column represents the same content type with different observations along the rows. In this example it is band 1 of MODIS surface reflectance, which is visible red. In the example the location of the images is the same, indicated by the granule identifier h04v09, but the dates differ: 2018185 (July 4, 2018) and 2018188 (July 7, 2018).\nscene1_B01 = \"https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/MCD43A4.A2018185.h04v09.006.2018194032851_B01.TIF\"\nscene2_B01 = \"https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018188/MCD43A4.A2018188.h04v09.006.2018198232008_B01.TIF\"\n\n# a pandas DF\none_d_cat_pd = pd.DataFrame({'B01': [scene1_B01, scene2_B01]})\n\n# equivalent spark DF\none_d_cat_df = spark.createDataFrame([Row(B01=scene1_B01), Row(B01=scene2_B01)])\n\n# equivalent CSV string\none_d_cat_csv = '\\n'.join(['B01', scene1_B01, scene2_B01])\nThis is what it looks like in DataFrame form:\none_d_cat_df\nB01 https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/MCD43A4.A2018185.h04v09.006.2018194032851_B01.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018188/MCD43A4.A2018188.h04v09.006.2018198232008_B01.TIF","title":"One-D"},{"location":"/raster-catalogs.html#two-d","text":"In this example, multiple columns representing multiple content types (bands) across multiple scenes. In each row, the scene is the same: granule id h04v09 on July 4 or July 7, 2018. The first column is band 1, red, and the second is band 2, near infrared.\nscene1_B01 = \"https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/MCD43A4.A2018185.h04v09.006.2018194032851_B01.TIF\"\nscene1_B02 = \"https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/MCD43A4.A2018185.h04v09.006.2018194032851_B02.TIF\"\nscene2_B01 = \"https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018188/MCD43A4.A2018188.h04v09.006.2018198232008_B01.TIF\"\nscene2_B02 = \"https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018188/MCD43A4.A2018188.h04v09.006.2018198232008_B02.TIF\"\n\n# Pandas DF\ntwo_d_cat_pd = pd.DataFrame([\n    {'B01': [scene1_B01], 'B02': [scene1_B02]},\n    {'B01': [scene2_B01], 'B02': [scene2_B02]}\n])     \n\n# or\ntwo_d_cat_df = spark.createDataFrame([\n    Row(B01=scene1_B01, B02=scene1_B02),\n    Row(B01=scene2_B01, B02=scene2_B02)\n])\n    \n# As CSV string\ntow_d_cat_csv = '\\n'.join(['B01,B02', scene1_B01 + \",\" + scene1_B02, scene2_B01 + \",\" + scene2_B02])\nThis is what it looks like in DataFrame form:\ntwo_d_cat_df\nB01 B02 https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/MCD43A4.A2018185.h04v09.006.2018194032851_B01.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/MCD43A4.A2018185.h04v09.006.2018194032851_B02.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018188/MCD43A4.A2018188.h04v09.006.2018198232008_B01.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018188/MCD43A4.A2018188.h04v09.006.2018198232008_B02.TIF","title":"Two-D"},{"location":"/raster-catalogs.html#using-external-catalogs","text":"The concept of a catalog is much more powerful when we consider examples beyond constructing the DataFrame, and instead read the data from an external source. Here’s an extended example of reading a cloud-hosted CSV file containing MODIS scene metadata and transforming it into a catalog. The metadata describing the content of each URL is an important aspect of processing raster data.\nfrom pyspark import SparkFiles\nfrom pyspark.sql import functions as F\n\nspark.sparkContext.addFile(\"https://modis-pds.s3.amazonaws.com/MCD43A4.006/2018-07-04_scenes.txt\")\n\nscene_list = spark.read \\\n    .format(\"csv\") \\\n    .option(\"header\", \"true\") \\\n    .load(SparkFiles.get(\"2018-07-04_scenes.txt\"))\nscene_list\nShowing only top 5 rows.\ndate download_url gid 2018-07-04 00:00:00 https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/index.html MCD43A4.A2018185.h04v09.006.2018194032851 2018-07-04 00:00:00 https://modis-pds.s3.amazonaws.com/MCD43A4.006/01/09/2018185/index.html MCD43A4.A2018185.h01v09.006.2018194032819 2018-07-04 00:00:00 https://modis-pds.s3.amazonaws.com/MCD43A4.006/06/03/2018185/index.html MCD43A4.A2018185.h06v03.006.2018194032807 2018-07-04 00:00:00 https://modis-pds.s3.amazonaws.com/MCD43A4.006/03/09/2018185/index.html MCD43A4.A2018185.h03v09.006.2018194032826 2018-07-04 00:00:00 https://modis-pds.s3.amazonaws.com/MCD43A4.006/08/09/2018185/index.html MCD43A4.A2018185.h08v09.006.2018194032839\nObserve the scenes list file has URIs to index.html files in the download_url column. The image URI’s are in the same directory. The filenames are of the form ${gid}_B${band}.TIF. The next code chunk builds these URIs, which completes our catalog.\nmodis_catalog = scene_list \\\n    .withColumn('base_url',\n        F.concat(F.regexp_replace('download_url', 'index.html$', ''), 'gid',)\n    ) \\\n    .withColumn('B01' , F.concat('base_url', F.lit(\"_B01.TIF\"))) \\\n    .withColumn('B02' , F.concat('base_url', F.lit(\"_B02.TIF\"))) \\\n    .withColumn('B03' , F.concat('base_url', F.lit(\"_B03.TIF\")))\nmodis_catalog\nShowing only top 5 rows.\ndate download_url gid base_url B01 B02 B03 2018-07-04 00:00:00 https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/index.html MCD43A4.A2018185.h04v09.006.2018194032851 https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/MCD43A4.A2018185.h04v09.006.2018194032851 https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/MCD43A4.A2018185.h04v09.006.2018194032851_B01.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/MCD43A4.A2018185.h04v09.006.2018194032851_B02.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/MCD43A4.A2018185.h04v09.006.2018194032851_B03.TIF 2018-07-04 00:00:00 https://modis-pds.s3.amazonaws.com/MCD43A4.006/01/09/2018185/index.html MCD43A4.A2018185.h01v09.006.2018194032819 https://modis-pds.s3.amazonaws.com/MCD43A4.006/01/09/2018185/MCD43A4.A2018185.h01v09.006.2018194032819 https://modis-pds.s3.amazonaws.com/MCD43A4.006/01/09/2018185/MCD43A4.A2018185.h01v09.006.2018194032819_B01.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/01/09/2018185/MCD43A4.A2018185.h01v09.006.2018194032819_B02.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/01/09/2018185/MCD43A4.A2018185.h01v09.006.2018194032819_B03.TIF 2018-07-04 00:00:00 https://modis-pds.s3.amazonaws.com/MCD43A4.006/06/03/2018185/index.html MCD43A4.A2018185.h06v03.006.2018194032807 https://modis-pds.s3.amazonaws.com/MCD43A4.006/06/03/2018185/MCD43A4.A2018185.h06v03.006.2018194032807 https://modis-pds.s3.amazonaws.com/MCD43A4.006/06/03/2018185/MCD43A4.A2018185.h06v03.006.2018194032807_B01.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/06/03/2018185/MCD43A4.A2018185.h06v03.006.2018194032807_B02.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/06/03/2018185/MCD43A4.A2018185.h06v03.006.2018194032807_B03.TIF 2018-07-04 00:00:00 https://modis-pds.s3.amazonaws.com/MCD43A4.006/03/09/2018185/index.html MCD43A4.A2018185.h03v09.006.2018194032826 https://modis-pds.s3.amazonaws.com/MCD43A4.006/03/09/2018185/MCD43A4.A2018185.h03v09.006.2018194032826 https://modis-pds.s3.amazonaws.com/MCD43A4.006/03/09/2018185/MCD43A4.A2018185.h03v09.006.2018194032826_B01.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/03/09/2018185/MCD43A4.A2018185.h03v09.006.2018194032826_B02.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/03/09/2018185/MCD43A4.A2018185.h03v09.006.2018194032826_B03.TIF 2018-07-04 00:00:00 https://modis-pds.s3.amazonaws.com/MCD43A4.006/08/09/2018185/index.html MCD43A4.A2018185.h08v09.006.2018194032839 https://modis-pds.s3.amazonaws.com/MCD43A4.006/08/09/2018185/MCD43A4.A2018185.h08v09.006.2018194032839 https://modis-pds.s3.amazonaws.com/MCD43A4.006/08/09/2018185/MCD43A4.A2018185.h08v09.006.2018194032839_B01.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/08/09/2018185/MCD43A4.A2018185.h08v09.006.2018194032839_B02.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/08/09/2018185/MCD43A4.A2018185.h08v09.006.2018194032839_B03.TIF","title":"Using External Catalogs"},{"location":"/raster-catalogs.html#using-built-in-catalogs","text":"RasterFrames comes with two experimental catalogs over the AWS PDS Landsat 8 and MODIS repositories. They are created by downloading the latest scene lists and building up the appropriate band URI columns as in the prior example.\nNote: The first time you run these may take some time, as the catalogs are large and have to be downloaded. However, they are cached and subsequent invocations should be faster.","title":"Using Built-in Catalogs"},{"location":"/raster-catalogs.html#modis","text":"modis_catalog = spark.read.format('aws-pds-modis-catalog').load()\nmodis_catalog.printSchema()\nroot\n |-- product_id: string (nullable = false)\n |-- acquisition_date: timestamp (nullable = false)\n |-- granule_id: string (nullable = false)\n |-- gid: string (nullable = false)\n |-- B01: string (nullable = true)\n |-- B01qa: string (nullable = true)\n |-- B02: string (nullable = true)\n |-- B02qa: string (nullable = true)\n |-- B03: string (nullable = true)\n |-- B03aq: string (nullable = true)\n |-- B04: string (nullable = true)\n |-- B04qa: string (nullable = true)\n |-- B05: string (nullable = true)\n |-- B05qa: string (nullable = true)\n |-- B06: string (nullable = true)\n |-- B06qa: string (nullable = true)\n |-- B07: string (nullable = true)\n |-- B07qa: string (nullable = true)","title":"MODIS"},{"location":"/raster-catalogs.html#landsat-8","text":"The Landsat 8 catalog includes a richer set of metadata describing the contents of each scene.\nl8 = spark.read.format('aws-pds-l8-catalog').load()\nl8.printSchema()\nroot\n |-- product_id: string (nullable = false)\n |-- entity_id: string (nullable = false)\n |-- acquisition_date: timestamp (nullable = false)\n |-- cloud_cover_pct: float (nullable = false)\n |-- processing_level: string (nullable = false)\n |-- path: short (nullable = false)\n |-- row: short (nullable = false)\n |-- bounds_wgs84: struct (nullable = false)\n |    |-- minX: double (nullable = false)\n |    |-- maxX: double (nullable = false)\n |    |-- minY: double (nullable = false)\n |    |-- maxY: double (nullable = false)\n |-- B1: string (nullable = true)\n |-- B2: string (nullable = true)\n |-- B3: string (nullable = true)\n |-- B4: string (nullable = true)\n |-- B5: string (nullable = true)\n |-- B6: string (nullable = true)\n |-- B7: string (nullable = true)\n |-- B8: string (nullable = true)\n |-- B9: string (nullable = true)\n |-- B10: string (nullable = true)\n |-- B11: string (nullable = true)\n |-- BQA: string (nullable = true)","title":"Landsat 8"},{"location":"/raster-read.html","text":"","title":"Reading Raster Data"},{"location":"/raster-read.html#reading-raster-data","text":"RasterFrames registers a DataSource named raster that enables reading of GeoTIFFs (and other formats when GDAL is installed) from arbitrary URIs. The raster DataSource operates on either a single raster file location or another DataFrame, called a catalog, containing pointers to many raster file locations.\nRasterFrames can also read from GeoTrellis catalogs and layers.","title":"Reading Raster Data"},{"location":"/raster-read.html#single-rasters","text":"The simplest way to use the raster reader is with a single raster from a single URI or file. In the examples that follow we’ll be reading from a Sentinel-2 scene stored in an AWS S3 bucket.\nrf = spark.read.raster('https://rasterframes.s3.amazonaws.com/samples/luray_snp/B02.tif')\nrf.printSchema()\nroot\n |-- proj_raster_path: string (nullable = false)\n |-- proj_raster: struct (nullable = true)\n |    |-- tile_context: struct (nullable = true)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\nThe file at the address above is a valid Cloud Optimized GeoTIFF (COG), which RasterFrames fully supports. RasterFrames will take advantage of the optimizations in the COG format to enable more efficient reading compared to non-COG GeoTIFFs.\nLet’s unpack the proj_raster column and look at the contents in more detail. It contains a CRS, a spatial extent measured in that CRS, and a two-dimensional array of numeric values called a tile.\ncrs = rf.select(rf_crs(\"proj_raster\").alias(\"value\")).first()\nprint(\"CRS\", crs.value.crsProj4)\nCRS +proj=utm +zone=17 +datum=WGS84 +units=m +no_defs\nrf.select(\n    rf_extent(\"proj_raster\").alias(\"extent\"),\n    rf_tile(\"proj_raster\").alias(\"tile\")\n)\nShowing only top 5 rows.\nextent tile [807480.0, 4207860.0, 809760.0, 4223220.0] [792120.0, 4269300.0, 807480.0, 4284660.0] [699960.0, 4223220.0, 715320.0, 4238580.0] [807480.0, 4190220.0, 809760.0, 4192500.0] [715320.0, 4223220.0, 730680.0, 4238580.0]\nYou can also see that the single raster has been broken out into many rows containing arbitrary non-overlapping regions. Doing so takes advantage of parallel in-memory reads from the cloud hosted data source and allows Spark to work on manageable amounts of data per row. The map below shows downsampled imagery with the bounds of the individual tiles.\nNote The image contains visible “seams” between the tile extents due to reprojection and downsampling used to create the image. The native imagery in the DataFrame does not contain any gaps in the source raster’s coverage.\nLet’s select a single tile and view it. The tile preview image as well as the string representation provide some basic information about the tile: its dimensions as numbers of columns and rows and the cell type, or data type of all the cells in the tile. For more about cell types, refer to this discussion.\ntile = rf.select(rf_tile(\"proj_raster\")).first()[0]\ndisplay(tile)","title":"Single Rasters"},{"location":"/raster-read.html#multiple-singleband-rasters","text":"In this example, we show the reading two bands of Landsat 8 imagery (red and near-infrared), combining them with rf_normalized_difference to compute NDVI, a common measure of vegetation health. As described in the section on catalogs, image URIs in a single row are assumed to be from the same scene/granule, and therefore compatible. This pattern is commonly used when multiple bands are stored in separate files.\nbands = [f'B{b}' for b in [4, 5]]\nuris = [f'https://landsat-pds.s3.us-west-2.amazonaws.com/c1/L8/014/032/LC08_L1TP_014032_20190720_20190731_01_T1/LC08_L1TP_014032_20190720_20190731_01_T1_{b}.TIF' for b in bands]\ncatalog = ','.join(bands) + '\\n' + ','.join(uris)\n\nrf = (spark.read.raster(catalog, bands) \n    # Adding semantic names \n    .withColumnRenamed('B4', 'red').withColumnRenamed('B5', 'NIR')  \n    # Adding tile center point for reference \n    .withColumn('longitude_latitude', st_reproject(st_centroid(rf_geometry('red')), rf_crs('red'), lit('EPSG:4326')))\n    # Compute NDVI  \n    .withColumn('NDVI', rf_normalized_difference('NIR', 'red'))\n    # For the purposes of inspection, filter out rows where there's not much vegetation  \n    .where(rf_tile_sum('NDVI') > 10000) \n    # Order output \n    .select('longitude_latitude', 'red', 'NIR', 'NDVI')) \ndisplay(rf)\nShowing only top 5 rows.\nlongitude_latitude red NIR NDVI POINT (-74.54798498452405 40.38740603406… POINT (-74.27356358544107 40.66276020383… POINT (-74.64174679709124 39.76498729658… POINT (-75.08989515249583 39.62710756189… POINT (-73.99582313850216 41.00659730217…","title":"Multiple Singleband Rasters"},{"location":"/raster-read.html#multiband-rasters","text":"A multiband raster is represented by a three dimensional numeric array stored in a single file. The first two dimensions are spatial, and the third dimension is typically designated for different spectral bands. The bands could represent intensity of different wavelengths of light (or other electromagnetic radiation), or they could measure other phenomena such as time, quality indications, or additional gas concentrations, etc.\nMultiband rasters files have a strictly ordered set of bands, which are typically indexed from 1. Some files have metadata tags associated with each band. Some files have a color interpetation metadata tag indicating how to interpret the bands.\nWhen reading a multiband raster or a catalog describing multiband rasters, you will need to know ahead of time which bands you want to read. You will specify the bands to read, indexed from zero, as a list of integers into the band_indexes parameter of the raster reader.\nFor example, we can read a four-band (red, green, blue, and near-infrared) image as follows. The individual rows of the resulting DataFrame still represent distinct spatial extents, with a projected raster column for each band specified by band_indexes.\nmb = spark.read.raster(\n    'https://rasterframes.s3.amazonaws.com/samples/naip/m_3807863_nw_17_1_20160620.tif',\n    band_indexes=[0, 1, 2, 3],\n)\ndisplay(mb)\nShowing only top 5 rows.\nproj_raster_path proj_raster_b0 proj_raster_b1 proj_raster_b2 proj_raster_b3 https://rasterframes.s3.amazonaws.com/samples/naip/m_3807863_nw_17_1_20160620.tif https://rasterframes.s3.amazonaws.com/samples/naip/m_3807863_nw_17_1_20160620.tif https://rasterframes.s3.amazonaws.com/samples/naip/m_3807863_nw_17_1_20160620.tif https://rasterframes.s3.amazonaws.com/samples/naip/m_3807863_nw_17_1_20160620.tif https://rasterframes.s3.amazonaws.com/samples/naip/m_3807863_nw_17_1_20160620.tif\nIf a band is passed into band_indexes that exceeds the number of bands in the raster, a projected raster column will still be generated in the schema but the column will be full of null values.\nYou can also pass a catalog and band_indexes together into the raster reader. This will create a projected raster column for the combination of all items in catalog_col_names and band_indexes. Again if a band in band_indexes exceeds the number of bands in a raster, it will have a null value for the corresponding column.\nHere is a trivial example with a catalog over multiband rasters. We specify two columns containing URIs and two bands, resulting in four projected raster columns.\nimport pandas as pd\nmb_cat = pd.DataFrame([\n    {'foo': 'https://rasterframes.s3.amazonaws.com/samples/naip/m_3807863_nw_17_1_20160620.tif',\n     'bar': 'https://rasterframes.s3.amazonaws.com/samples/naip/m_3807863_nw_17_1_20160620.tif'\n    },\n])\nmb2 = spark.read.raster(\n    spark.createDataFrame(mb_cat),\n    catalog_col_names=['foo', 'bar'],\n    band_indexes=[0, 1],\n    tile_dimensions=(64,64)\n)\nmb2.printSchema()\nroot\n |-- foo_path: string (nullable = false)\n |-- bar_path: string (nullable = false)\n |-- foo_b0: struct (nullable = true)\n |    |-- tile_context: struct (nullable = true)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- foo_b1: struct (nullable = true)\n |    |-- tile_context: struct (nullable = true)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- bar_b0: struct (nullable = true)\n |    |-- tile_context: struct (nullable = true)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- bar_b1: struct (nullable = true)\n |    |-- tile_context: struct (nullable = true)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)","title":"Multiband Rasters"},{"location":"/raster-read.html#uri-formats","text":"RasterFrames relies on three different I/O drivers, selected based on a combination of scheme, file extentions, and library availability. GDAL is used by default if a compatible version of GDAL (>= 2.4) is installed, and if GDAL supports the specified scheme. If GDAL is not available, either the Java I/O or Hadoop driver will be selected, depending on scheme.\nNote: The GDAL driver is the only one that can read non-GeoTIFF files.\nPrefix GDAL Java I/O Hadoop gdal://<vsidrv>// yes no no file:// yes yes no http:// yes yes no https:// yes yes no ftp:// /vsicurl/ yes no hdfs:// /vsihdfs/ no yes s3:// /vsis3/ yes no s3n:// no no yes s3a:// no no yes wasb:// /vsiaz/ no yes wasbs:// no no yes\nSpecific GDAL Virtual File System drivers can be selected using the gdal://<vsidrv>// syntax. For example If you have a archive.zip file containing a GeoTiff named my-file-inside.tif, you can address it with gdal://vsizip//path/to/archive.zip/my-file-inside.tif. Another example would be a MRF file in an S3 bucket on AWS: gdal://vsis3/my-bucket/prefix/to/raster.mrf. See the GDAL documentation for the format of the URIs after the gdal:/ scheme.","title":"URI Formats"},{"location":"/raster-read.html#raster-catalogs","text":"Consider the definition of a Catalog previously discussed, let’s read the raster data contained in the catalog URIs. We will start with the external catalog of MODIS surface reflectance.\nfrom pyspark import SparkFiles\nfrom pyspark.sql import functions as F\n\ncat_filename = \"2018-07-04_scenes.txt\"\nspark.sparkContext.addFile(\"https://modis-pds.s3.amazonaws.com/MCD43A4.006/{}\".format(cat_filename))\n\nmodis_catalog = spark.read \\\n    .format(\"csv\") \\\n    .option(\"header\", \"true\") \\\n    .load(SparkFiles.get(cat_filename)) \\\n    .withColumn('base_url',\n        F.concat(F.regexp_replace('download_url', 'index.html$', ''), 'gid')\n    ) \\\n    .drop('download_url') \\\n    .withColumn('red' , F.concat('base_url', F.lit(\"_B01.TIF\"))) \\\n    .withColumn('nir' , F.concat('base_url', F.lit(\"_B02.TIF\")))\n\nprint(\"Available scenes: \", modis_catalog.count())\nAvailable scenes:  297\nmodis_catalog\nShowing only top 5 rows.\ndate gid base_url red nir 2018-07-04 00:00:00 MCD43A4.A2018185.h04v09.006.2018194032851 https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/MCD43A4.A2018185.h04v09.006.2018194032851 https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/MCD43A4.A2018185.h04v09.006.2018194032851_B01.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/MCD43A4.A2018185.h04v09.006.2018194032851_B02.TIF 2018-07-04 00:00:00 MCD43A4.A2018185.h01v09.006.2018194032819 https://modis-pds.s3.amazonaws.com/MCD43A4.006/01/09/2018185/MCD43A4.A2018185.h01v09.006.2018194032819 https://modis-pds.s3.amazonaws.com/MCD43A4.006/01/09/2018185/MCD43A4.A2018185.h01v09.006.2018194032819_B01.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/01/09/2018185/MCD43A4.A2018185.h01v09.006.2018194032819_B02.TIF 2018-07-04 00:00:00 MCD43A4.A2018185.h06v03.006.2018194032807 https://modis-pds.s3.amazonaws.com/MCD43A4.006/06/03/2018185/MCD43A4.A2018185.h06v03.006.2018194032807 https://modis-pds.s3.amazonaws.com/MCD43A4.006/06/03/2018185/MCD43A4.A2018185.h06v03.006.2018194032807_B01.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/06/03/2018185/MCD43A4.A2018185.h06v03.006.2018194032807_B02.TIF 2018-07-04 00:00:00 MCD43A4.A2018185.h03v09.006.2018194032826 https://modis-pds.s3.amazonaws.com/MCD43A4.006/03/09/2018185/MCD43A4.A2018185.h03v09.006.2018194032826 https://modis-pds.s3.amazonaws.com/MCD43A4.006/03/09/2018185/MCD43A4.A2018185.h03v09.006.2018194032826_B01.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/03/09/2018185/MCD43A4.A2018185.h03v09.006.2018194032826_B02.TIF 2018-07-04 00:00:00 MCD43A4.A2018185.h08v09.006.2018194032839 https://modis-pds.s3.amazonaws.com/MCD43A4.006/08/09/2018185/MCD43A4.A2018185.h08v09.006.2018194032839 https://modis-pds.s3.amazonaws.com/MCD43A4.006/08/09/2018185/MCD43A4.A2018185.h08v09.006.2018194032839_B01.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/08/09/2018185/MCD43A4.A2018185.h08v09.006.2018194032839_B02.TIF\nMODIS data products are delivered on a regular, consistent grid, making identification of a specific area over time easy using (h,v) grid coordinates (see below).\nFor example, MODIS data right above the equator is all grid coordinates with v07.\nequator = modis_catalog.where(F.col('gid').like('%v07%')) \nequator.select('date', 'gid')\nShowing only top 5 rows.\ndate gid 2018-07-04 00:00:00 MCD43A4.A2018185.h07v07.006.2018194033213 2018-07-04 00:00:00 MCD43A4.A2018185.h08v07.006.2018194033224 2018-07-04 00:00:00 MCD43A4.A2018185.h09v07.006.2018194033547 2018-07-04 00:00:00 MCD43A4.A2018185.h26v07.006.2018194033904 2018-07-04 00:00:00 MCD43A4.A2018185.h12v07.006.2018194033912\nNow that we have prepared our catalog, we simply pass the DataFrame or CSV string to the raster DataSource to load the imagery. The catalog_col_names parameter gives the columns that contain the URI’s to be read.\nrf = spark.read.raster(equator, catalog_col_names=['red', 'nir'])\nrf.printSchema()\nroot\n |-- red_path: string (nullable = false)\n |-- nir_path: string (nullable = false)\n |-- red: struct (nullable = true)\n |    |-- tile_context: struct (nullable = true)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- nir: struct (nullable = true)\n |    |-- tile_context: struct (nullable = true)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- date: string (nullable = true)\n |-- gid: string (nullable = true)\n |-- base_url: string (nullable = true)\nObserve the schema of the resulting DataFrame has a projected raster struct for each column passed in catalog_col_names. For reference, the URI is now in a column appended with _path. Taking a quick look at the representation of the data, we see again each row contains an arbitrary portion of the entire scene coverage. We also see that for two-D catalogs, each row contains the same spatial extent for all tiles in that row.\nsample = rf \\\n    .select('gid', rf_extent('red'), rf_extent('nir'), rf_tile('red'), rf_tile('nir')) \\\n    .where(~rf_is_no_data_tile('red'))\nsample.limit(3)\ngid rf_extent(red) rf_extent(nir) rf_tile(red) rf_tile(nir) MCD43A4.A2018185.h16v07.006.2018194033835 [-1156428.54045364, 1630860.7621777998, -1111950.519667, 1749468.8176088398] [-1156428.54045364, 1630860.7621777998, -1111950.519667, 1749468.8176088398] MCD43A4.A2018185.h22v07.006.2018194034134 [4566410.13409804, 1111950.519667, 4685018.18952908, 1156428.54045364] [4566410.13409804, 1111950.519667, 4685018.18952908, 1156428.54045364] MCD43A4.A2018185.h33v07.006.2018194035136 [1.7628122238449175E7, 1275036.59588468, 1.774673029388032E7, 1393644.65131572] [1.7628122238449175E7, 1275036.59588468, 1.774673029388032E7, 1393644.65131572]","title":"Raster Catalogs"},{"location":"/raster-read.html#lazy-raster-reading","text":"By default, reading raster pixel values is delayed until it is absolutely needed. The DataFrame will contain metadata and pointers to the appropriate portion of the data until reading of the source raster data is absolutely necessary. This can save a significant of computation and I/O time for two reasons. First, a catalog may contain a large number of rows. Second, the raster DataSource attempts to apply spatial preciates (e.g. where/WHERE clauses with st_intersects, et al.) at row creation, reducing the chance of unneeded data being fetched.\nConsider the following two reads of the same data source. In the first, the lazy case, there is a pointer to the URI, extent and band to read. This will not be evaluated until the cell values are absolutely required. The second case shows the option to force the raster to be fully loaded right away.\nuri = 'https://rasterframes.s3.amazonaws.com/samples/luray_snp/B02.tif'\nlazy = spark.read.raster(uri).select(col('proj_raster.tile').cast('string'))\nlazy\nShowing only top 5 rows.\ntile RasterRefTile(RasterRef(JVMGeoTiffRasterSource(https://rasterframes.s3.amazonaws.com/samples/luray_snp/B02.tif),0,Some(Extent(807480.0, 4207860.0, 809760.0, 4223220.0)),Some(GridBounds(1792,1280,1829,1535)))) RasterRefTile(RasterRef(JVMGeoTiffRasterSource(https://rasterframes.s3.amazonaws.com/samples/luray_snp/B02.tif),0,Some(Extent(792120.0, 4269300.0, 807480.0, 4284660.0)),Some(GridBounds(1536,256,1791,511)))) RasterRefTile(RasterRef(JVMGeoTiffRasterSource(https://rasterframes.s3.amazonaws.com/samples/luray_snp/B02.tif),0,Some(Extent(699960.0, 4223220.0, 715320.0, 4238580.0)),Some(GridBounds(0,1024,255,1279)))) RasterRefTile(RasterRef(JVMGeoTiffRasterSource(https://rasterframes.s3.amazonaws.com/samples/luray_snp/B02.tif),0,Some(Extent(807480.0, 4190220.0, 809760.0, 4192500.0)),Some(GridBounds(1792,1792,1829,1829)))) RasterRefTile(RasterRef(JVMGeoTiffRasterSource(https://rasterframes.s3.amazonaws.com/samples/luray_snp/B02.tif),0,Some(Extent(715320.0, 4223220.0, 730680.0, 4238580.0)),Some(GridBounds(256,1024,511,1279))))\nnon_lazy = spark.read.raster(uri, lazy_tiles=False).select(col('proj_raster.tile').cast('string'))\nnon_lazy\nShowing only top 5 rows.\ntile ArrayTile(256,256,uint16raw) ArrayTile(38,256,uint16raw) ArrayTile(256,256,uint16raw) ArrayTile(256,256,uint16raw) ArrayTile(256,256,uint16raw)\nIn the initial examples on this page, you may have noticed that the realized (non-lazy) tiles are shown, but we did not change lazy_tiles. Instead, we used rf_tile to explicitly request the realized tile from the lazy representation.","title":"Lazy Raster Reading"},{"location":"/raster-read.html#spatial-indexing-and-partitioning","text":"Warning This is an experimental feature, and may be removed.\nIt’s often desirable to take extra steps in ensuring your data is effectively distributed over your computing resources. One way of doing that is using something called a “space filling curve”, which turns an N-dimensional value into a one dimensional value, with properties that favor keeping entities near each other in N-space near each other in index space. In particular RasterFrames support space-filling curves mapping the geographic location of tiles to a one-dimensional index space called xz2. To have RasterFrames add a spatial index based partitioning on a raster reads, use the spatial_index_partitions parameter. By default it will use the same number of partitions as configured in spark.sql.shuffle.partitions.\ndf = spark.read.raster(uri, spatial_index_partitions=True)\ndf\nShowing only top 5 rows.\nproj_raster_path proj_raster spatial_index https://rasterframes.s3.amazonaws.com/samples/luray_snp/B02.tif 55256634714 https://rasterframes.s3.amazonaws.com/samples/luray_snp/B02.tif 55256634714 https://rasterframes.s3.amazonaws.com/samples/luray_snp/B02.tif 55256634715 https://rasterframes.s3.amazonaws.com/samples/luray_snp/B02.tif 55256656560 https://rasterframes.s3.amazonaws.com/samples/luray_snp/B02.tif 55256722095\nYou can also pass a positive integer to the parameter to specify the number of desired partitions.\ndf = spark.read.raster(uri, spatial_index_partitions=800)","title":"Spatial Indexing and Partitioning"},{"location":"/raster-read.html#geotrellis","text":"","title":"GeoTrellis"},{"location":"/raster-read.html#geotrellis-catalogs","text":"GeoTrellis is one of the key libraries upon which RasterFrames is built. It provides a Scala language API for working with geospatial raster data. GeoTrellis defines a tile layer storage format for persisting imagery mosaics. RasterFrames provides a DataSource supporting both reading and writing GeoTrellis layers.\nA GeoTrellis catalog is a set of GeoTrellis layers. We can read a DataFrame giving details of the content of a catalog using the syntax below. The scheme is typically hdfs or file, or a cloud storage provider like s3.\ngt_cat = spark.read.geotrellis_catalog('scheme://path-to-gt-catalog')","title":"GeoTrellis Catalogs"},{"location":"/raster-read.html#geotrellis-layers","text":"The catalog will give details on the individual layers available for query. We can read each layer with the URI to the catalog, the layer name, and the desired zoom level.\ngt_layer = spark.read.geotrellis(path='scheme://path-to-gt-catalog', layer=layer_name, zoom=zoom_level)\nThis will return a RasterFrame with additional GeoTrellis-specific metadata, inherited from GeoTrellis, stored as JSON in the metadata of the tile column.","title":"GeoTrellis Layers"},{"location":"/raster-write.html","text":"","title":"Writing Raster Data"},{"location":"/raster-write.html#writing-raster-data","text":"RasterFrames is oriented toward large scale analyses of spatial data. The primary output of these analyses could be a statistical summary, a machine learning model, or some other result that is generally much smaller than the input dataset.\nHowever, there are times in any analysis where writing a representative sample of the work in progress provides valuable feedback on the current state of the process and results, or you are constructing a new dataset to be used in other analyses.\nThis will be our setup for the following examples:\nfrom pyrasterframes import *\nfrom pyrasterframes.rasterfunctions import *\nfrom pyrasterframes.utils import create_rf_spark_session\nimport pyrasterframes.rf_ipython\nfrom IPython.display import display\nimport os.path\nspark = create_rf_spark_session(**{\n    'spark.driver.memory': '4G',\n    'spark.ui.enabled': 'false'\n})\ndef scene(band):\n    b = str(band).zfill(2) # converts int 2 to '02'\n    return 'https://modis-pds.s3.amazonaws.com/MCD43A4.006/11/08/2019059/' \\\n             'MCD43A4.A2019059.h11v08.006.2019072203257_B{}.TIF'.format(b)\nrf = spark.read.raster(scene(2), tile_dimensions=(256, 256))","title":"Writing Raster Data"},{"location":"/raster-write.html#ipython-jupyter","text":"This section provides details on how Tiles and DataFrames with Tiles in them can be viewed in the IPython/Jupyter.","title":"IPython/Jupyter"},{"location":"/raster-write.html#overview-rasters","text":"In cases where writing and reading to/from a GeoTIFF isn’t convenient, RasterFrames provides the rf_agg_overview_raster aggregate function, where you can construct a single raster (rendered as a tile) downsampled from all or a subset of the DataFrame. This allows you to effectively construct the same operations the GeoTIFF writer performs, but without the file I/O.\nThe rf_agg_overview_raster function will reproject data to the commonly used “web mercator” CRS. You must specify an “Area of Interest” (AOI) in web mercator. You can use rf_agg_reprojected_extent to compute the extent of a DataFrame in any CRS or mix of CRSs.\nwm_extent = rf.agg(\n                  rf_agg_reprojected_extent(rf_extent('proj_raster'), rf_crs('proj_raster'), 'EPSG:3857')\n                  ).first()[0]\naoi = Extent.from_row(wm_extent)\nprint(aoi)\naspect = aoi.width / aoi.height\n\nov = rf.agg(\n    rf_agg_overview_raster('proj_raster', int(512 * aspect), 512, aoi)\n).first()[0]\nprint(\"`ov` is of type\", type(ov))\nov\nExtent(-7912574.135382183, -7.081154551613622E-10, -6679169.446996604, 1118889.9747567875)\n`ov` is of type <class 'pyrasterframes.rf_types.Tile'>","title":"Overview Rasters"},{"location":"/raster-write.html#geotiffs","text":"GeoTIFF is one of the most common file formats for spatial data, providing flexibility in data encoding, representation, and storage. RasterFrames provides a specialized Spark DataFrame writer for rendering a RasterFrame to a GeoTIFF. It is accessed by calling dataframe.write.geotiff.","title":"GeoTIFFs"},{"location":"/raster-write.html#limitations-and-mitigations","text":"One downside to GeoTIFF is that it is not a big-data native format. To create a GeoTIFF, all the data to be written must be collected in the memory of the Spark driver. This means you must actively limit the size of the data to be written. It is trivial to lazily read a set of inputs that cannot feasibly be written to GeoTIFF in the same environment.\nWhen writing GeoTIFFs in RasterFrames, you should limit the size of the collected data. Consider filtering the dataframe by time or spatial filters.\nYou can also specify the dimensions of the GeoTIFF file to be written using the raster_dimensions parameter as described below.","title":"Limitations and mitigations"},{"location":"/raster-write.html#parameters","text":"If there are many tile or projected raster columns in the DataFrame, the GeoTIFF writer will write each one as a separate band in the file. Each band in the output will be tagged the input column names for reference.\npath: the path local to the driver where the file will be written crs: the PROJ4 string of the CRS the GeoTIFF is to be written in raster_dimensions: optional, a tuple of two ints giving the size of the resulting file. If specified, RasterFrames will downsample the data in distributed fashion using bilinear resampling. If not specified, the default is to write the dataframe at full resolution, which can result in an out of memory error.","title":"Parameters"},{"location":"/raster-write.html#example","text":"See also the example in the unsupervised learning page.\nLet’s render an overview of a scene’s red band as a small raster, reprojecting it to latitude and longitude coordinates on the WGS84 reference ellipsoid (aka EPSG:4326).\noutfile = os.path.join('/tmp', 'geotiff-overview.tif')\nrf.write.geotiff(outfile, crs='EPSG:4326', raster_dimensions=(256, 256))\nWe can view the written file with rasterio:\nimport rasterio\nfrom rasterio.plot import show, show_hist\n\nwith rasterio.open(outfile) as src:\n    # View raster\n    show(src, adjust='linear')\n    # View data distribution\n    show_hist(src, bins=50, lw=0.0, stacked=False, alpha=0.6,\n        histtype='stepfilled', title=\"Overview Histogram\")\nWarning Attempting to write a full resolution GeoTIFF constructed from multiple scenes is likely to result in an out of memory error. Consider filtering the dataframe more aggressively and using a smaller value for the raster_dimensions parameter.","title":"Example"},{"location":"/raster-write.html#color-composites","text":"If the DataFrame has three or four tile columns, the GeoTIFF is written with the ColorInterp tags on the bands to indicate red, green, blue, and optionally alpha. Use a select statement to ensure the bands are in the desired order. If the bands chosen are red, green, and blue, the composite is called a true-color composite. Otherwise it is a false-color composite. If the number of tile columns is not 3 or 4, the ColorInterp tag will indicate greyscale.\nAlso see Color Composite in the IPython/Juptyer Extensions.","title":"Color Composites"},{"location":"/raster-write.html#png","text":"In this example we will use the rf_rgb_composite function, we will compute a three band PNG image as a bytearray. The resulting bytearray will be displayed as an image in either a Spark or pandas DataFrame display if rf_ipython has been imported.\n# Select red, green, and blue, respectively\ncomposite_df = spark.read.raster([[scene(1), scene(4), scene(3)]])\n\ncomposite_df = composite_df.withColumn('png',\n                    rf_render_png('proj_raster_0', 'proj_raster_1', 'proj_raster_2')).cache()\ncomposite_df.select('png').limit(1)\npng\nAlternatively the bytearray result can be displayed with pillow.\nimport io\nfrom PIL.Image import open as PIL_open\npng_bytearray = composite_df.first()['png']\npil_image = PIL_open(io.BytesIO(png_bytearray))\npil_image","title":"PNG"},{"location":"/raster-write.html#geotiff","text":"In this example we will write a false-color composite as a GeoTIFF\noutfile = os.path.join('/tmp', 'geotiff-composite.tif')\ncomposite_df = spark.read.raster([[scene(3), scene(1), scene(4)]])\ncomposite_df.write.geotiff(outfile, crs='EPSG:4326', raster_dimensions=(256, 256))\nwith rasterio.open(outfile) as src:\n    show(src)","title":"GeoTIFF"},{"location":"/raster-write.html#geotrellis-layers","text":"GeoTrellis is one of the key libraries upon which RasterFrames is built. It provides a Scala language API for working with geospatial raster data. GeoTrellis defines a tile layer storage format for persisting imagery mosaics. RasterFrames can write data from a RasterFrameLayer into a GeoTrellis Layer. RasterFrames provides a geotrellis DataSource that supports both reading and writing GeoTrellis layers.\nAn example is forthcoming. In the mean time referencing the GeoTrellisDataSourceSpec test code may help.","title":"GeoTrellis Layers"},{"location":"/raster-write.html#parquet","text":"You can write a RasterFrame to the Apache Parquet format. This format is designed to efficiently persist and query columnar data in distributed file system, such as HDFS. It also provides benefits when working in single node (or “local”) mode, such as tailoring organization for defined query patterns.\nrf.withColumn('exp', rf_expm1('proj_raster')) \\\n    .write.mode('append').parquet('hdfs:///rf-user/sample.pq')","title":"Parquet"},{"location":"/vector-data.html","text":"","title":"Vector Data"},{"location":"/vector-data.html#vector-data","text":"RasterFrames provides a variety of ways to work with spatial vector data (points, lines, and polygons) alongside raster data.\nDataSource for GeoJSON format Ability to convert between from GeoPandas and Spark DataFrames In PySpark, geometries are Shapely objects, providing a great deal of interoperability Many Spark functions for working with columns of geometries Vector data is also the basis for zonal map algebra operations.","title":"Vector Data"},{"location":"/vector-data.html#geojson-datasource","text":"from pyspark import SparkFiles\nadmin1_us_url = 'https://raw.githubusercontent.com/datasets/geo-admin1-us/master/data/admin1-us.geojson'\nspark.sparkContext.addFile(admin1_us_url)  # this lets us read http scheme uri's in spark\n\ndf = spark.read.geojson(SparkFiles.get('admin1-us.geojson'))\ndf.printSchema()\nroot\n |-- geometry: geometry (nullable = true)\n |-- ISO3166-1-Alpha-3: string (nullable = true)\n |-- country: string (nullable = true)\n |-- id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- state_code: string (nullable = true)\nThe properties of each discrete geometry are available as columns of the DataFrame, along with the geometry itself.","title":"GeoJSON DataSource"},{"location":"/vector-data.html#geopandas-and-rasterframes","text":"You can also convert a GeoPandas GeoDataFrame to a Spark DataFrame, preserving the geometry column. This means that any vector format that can be read with OGR can be converted to a Spark DataFrame. In the example below, we expect the same schema as the DataFrame defined above by the GeoJSON reader. Note that in a GeoPandas DataFrame there can be heterogeneous geometry types in the column, which may fail Spark’s schema inference.\nimport geopandas\nfrom shapely.geometry import MultiPolygon\n\ndef poly_or_mp_to_mp(g):\n    \"\"\" Normalize polygons or multipolygons to all be multipolygons. \"\"\"\n    if isinstance(g, MultiPolygon):\n        return g\n    else:\n        return MultiPolygon([g])\n\ngdf = geopandas.read_file(admin1_us_url)\ngdf.geometry = gdf.geometry.apply(poly_or_mp_to_mp)\ndf2 = spark.createDataFrame(gdf)\ndf2.printSchema()\nroot\n |-- name: string (nullable = true)\n |-- country: string (nullable = true)\n |-- ISO3166-1-Alpha-3: string (nullable = true)\n |-- state_code: string (nullable = true)\n |-- id: string (nullable = true)\n |-- geometry: multipolygon (nullable = true)","title":"GeoPandas and RasterFrames"},{"location":"/vector-data.html#shapely-geometry-support","text":"The geometry column will have a Spark user-defined type that is compatible with Shapely when working with Python via PySpark. This means that when the data is collected to the driver, it will be a Shapely geometry object.\nthe_first = df.first()\nprint(type(the_first['geometry']))\n<class 'shapely.geometry.polygon.Polygon'>\nSince it is a geometry we can do things like this:\nthe_first['geometry'].wkt\n'POLYGON ((-71.14789974636884 41.64758738867177, -71.1203820461734 41.49465098730397, -71.85382564969197 41.32003632258973, -71.79295081245215 41.46661652278563, -71.8009089830251 42.01324982356905, -71.37915178087496 42.02436025651181, -71.30507361518457 41.76241242122431, -71.14789974636884 41.64758738867177))'\nYou can also write user-defined functions that take geometries as input, output, or both, via user defined types in the geomesa_pyspark.types module. Here is a simple but inefficient example of a user-defined function that uses both a geometry input and output to compute the centroid of a geometry. Observe in a sample of the data the geometry columns print as well known text (wkt).\nfrom pyspark.sql.functions import udf\nfrom geomesa_pyspark.types import PointUDT\n\n@udf(PointUDT())\ndef inefficient_centroid(g):\n    return g.centroid\n\ndf.select(df.state_code, inefficient_centroid(df.geometry))\nShowing only top 5 rows.\nstate_code inefficient_centroid(geometry) RI POINT (-71.52928731203043 41.68199156750… FL POINT (-82.50257410032164 28.61692830512… OK POINT (-97.5041982257254 35.581496016431… MN POINT (-94.17743611908642 46.36007316339… TX POINT (-99.3286756609654 31.455508256185…","title":"Shapely Geometry Support"},{"location":"/vector-data.html#geomesa-functions-and-spatial-relations","text":"As documented in the function reference, various user-defined functions implemented by GeoMesa are also available for use. The example below uses a GeoMesa user-defined function to compute the centroid of a geometry. It is logically equivalent to the example above, but more efficient.\nfrom pyrasterframes.rasterfunctions import st_centroid\ndf.select(df.state_code, inefficient_centroid(df.geometry), st_centroid(df.geometry))\nShowing only top 5 rows.\nstate_code inefficient_centroid(geometry) st_centroid(geometry) RI POINT (-71.52928731203043 41.68199156750… POINT (-71.52928731203043 41.68199156750… FL POINT (-82.50257410032164 28.61692830512… POINT (-82.50257410032164 28.61692830512… OK POINT (-97.5041982257254 35.581496016431… POINT (-97.5041982257254 35.581496016431… MN POINT (-94.17743611908642 46.36007316339… POINT (-94.17743611908642 46.36007316339… TX POINT (-99.3286756609654 31.455508256185… POINT (-99.3286756609654 31.455508256185…\nThe RasterFrames vector functions and GeoMesa functions also provide a variety of spatial relations that are useful in combination with the geometric properties of projected rasters. In this example, we use the built-in Landsat catalog which provides an extent. We will convert the extent to a polygon and filter to those within approximately 50 km of a selected point.\nfrom pyrasterframes.rasterfunctions import st_geometry, st_bufferPoint, st_intersects, st_point\nfrom pyspark.sql.functions import lit\nl8 = spark.read.format('aws-pds-l8-catalog').load()\n\nl8 = l8.withColumn('geom', st_geometry(l8.bounds_wgs84))  # extent to polygon\nl8 = l8.withColumn('paducah', st_point(lit(-88.628), lit(37.072)))  # col of points\n\nl8_filtered = l8 \\\n                .filter(st_intersects(l8.geom, st_bufferPoint(l8.paducah, lit(50000.0)))) \\\n                .filter(l8.acquisition_date > '2018-02-01') \\\n                .filter(l8.acquisition_date < '2018-03-11')","title":"GeoMesa Functions and Spatial Relations"},{"location":"/raster-processing.html","text":"","title":"Raster Processing"},{"location":"/raster-processing.html#raster-processing","text":"Explore what you can do with RasterFrames to work with, analyze, manipulate, and summarize Earth observation data.\nLocal Map Algebra “NoData” Handling Masking Zonal Map Algebra Aggregation Time Series Raster Join","title":"Raster Processing"},{"location":"/local-algebra.html","text":"","title":"Local Map Algebra"},{"location":"/local-algebra.html#local-map-algebra","text":"Local map algebra raster operations are element-wise operations on a single tile, between a tile and a scalar, between two tiles, or among many tiles. These operations are common in processing of earth observation imagery and other image data.\nCredit: GISGeography","title":"Local Map Algebra"},{"location":"/local-algebra.html#computing-ndvi","text":"Here is an example of computing the Normalized Differential Vegetation Index (NDVI). NDVI is a vegetation index which emphasizes differences in relative biomass and vegetation health. The term index in Earth observation means a combination of many raster bands into a single band that highlights a phenomenon of interest. Various indices have proven useful visual tools and frequently appear as features in machine learning models using Earth observation data.\nNDVI is often used worldwide to monitor drought, monitor and predict agricultural production, assist in predicting hazardous fire zones, and map desert encroachment. The NDVI is preferred for global vegetation monitoring because it helps to compensate for changing illumination conditions, surface slope, aspect, and other extraneous factors (Lillesand. Remote sensing and image interpretation. 2004)\nWe will apply the catalog pattern for defining the data we wish to process. To compute NDVI we need to compute local algebra on the red and near infrared (nir) bands:\nnir - red\nNDVI = ---------\n       nir + red\nThis form of (x - y) / (x + y) is common in remote sensing and is called a normalized difference. It is used with other band pairs to highlight water, snow, and other phenomena.\nfrom pyspark.sql import Row\nuri_pattern = 'https://rasterframes.s3.amazonaws.com/samples/luray_snp/B0{}.tif'\ncatalog_df = spark.createDataFrame([\n    Row(red=uri_pattern.format(4), nir=uri_pattern.format(8))\n])\ndf = spark.read.raster(\n    catalog_df,\n    catalog_col_names=['red', 'nir']\n)\ndf.printSchema()\nroot\n |-- red_path: string (nullable = false)\n |-- nir_path: string (nullable = false)\n |-- red: struct (nullable = true)\n |    |-- tile_context: struct (nullable = true)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- nir: struct (nullable = true)\n |    |-- tile_context: struct (nullable = true)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\nObserve how the bands we need to use to compute the index are arranged as columns of our resulting DataFrame. The rows or observations are various spatial extents within the entire coverage area.\nRasterFrames provides a wide variety of local map algebra functions. There are several different broad categories, based on how many tiles the function takes as input:\nA function on a single tile is a unary operation; example: rf_log; A function on two tiles is a binary operation; example: rf_local_multiply; A function on a tile and a scalar is a binary operation; example: rf_local_less; or A function on many tiles is a n-ary operation; example: rf_agg_local_min\nWe can express the normalized difference with a combination of rf_local_divide, rf_local_subtract, and rf_local_add. Since the normalized difference is so common, there is a convenience method rf_normalized_difference, which we use in this example. We will append a new column to the DataFrame, which will apply the map alegbra function to each row.\ndf = df.withColumn('ndvi', rf_normalized_difference(df.nir, df.red))\ndf.printSchema()\nroot\n |-- red_path: string (nullable = false)\n |-- nir_path: string (nullable = false)\n |-- red: struct (nullable = true)\n |    |-- tile_context: struct (nullable = true)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- nir: struct (nullable = true)\n |    |-- tile_context: struct (nullable = true)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- ndvi: struct (nullable = true)\n |    |-- tile_context: struct (nullable = true)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\nWe can inspect a sample of the data. Yellow indicates very healthy vegetation, and purple represents bare soil or impervious surfaces.\nt = df.select(rf_tile('ndvi').alias('ndvi')).first()['ndvi']\ndisplay(t)\nWe continue examining NDVI in the time series section.","title":"Computing NDVI"},{"location":"/nodata-handling.html","text":"","title":"NoData Handling"},{"location":"/nodata-handling.html#handling","text":"","title":"“NoData” Handling"},{"location":"/nodata-handling.html#what-is-nodata-","text":"In raster operations, the preservation and correct processing of missing observations is very important. In most DataFrames and in scientific computing, the idea of missing data is expressed as a null or NaN value. However, a great deal of raster data is stored for space efficiency, which typically leads to use of integral values with a “sentinel” value designated to represent missing observations. This sentinel value varies across data products and bands. In a generic sense, it is usually called the “NoData” value.\nRasterFrames provides a variety of functions to inspect and manage NoData within tiles.","title":"What is NoData?"},{"location":"/nodata-handling.html#cell-types","text":"To understand how NoData is handled in RasterFrames, we first need to understand the different underlying types of data called cell types. RasterFrames cell types are GeoTrellis CellTypes, so the GeoTrellis documentation is a valuable resource on how these are defined.\nThe CellType class from the rf_types submodule allows us to create a representation of any valid cell type. There are convenience methods to create instances for a variety of basic types.\nfrom pyrasterframes.rf_types import CellType\nCellType.bool()\nCellType.int8()\nCellType.uint8()\nCellType.int16()\nCellType.uint16()\nCellType.int32()\nCellType.float32()\nCellType.float64()\nWe can also inspect the cell type of a given tile or proj_raster column.\ncell_types = spark.read.raster('https://rasterframes.s3.amazonaws.com/samples/luray_snp/B02.tif') \\\n    .select(rf_cell_type('proj_raster')).distinct()\ncell_types\nrf_cell_type(proj_raster) [uint16raw]","title":"Cell Types"},{"location":"/nodata-handling.html#understanding-cell-types-and-nodata","text":"We can use the methods on the CellType class to learn more about a specific cell type. Let’s consider the cell type of our sample data above.\nct = CellType('uint16raw')\nct, ct.is_floating_point(), ct.has_no_data()\n(uint16raw, False, False)\nWe can see that for the above data source, there is no defined NoData value. This means that each value is interpreted as a valid observation. Often such data is meant to be combined with another band indicating the quality of observations at each location. The lack of NoData is indicated by the raw at the end of the type name. Consider for contrast the uint16 cell type.\nfrom pyrasterframes.rf_types import CellType\nct = CellType('uint16')\nct, ct.is_floating_point(), ct.has_no_data(), ct.no_data_value()\n(uint16, False, True, 0)\nIn this case, the minimum value of 0 is designated as the NoData value. For integral-valued cell types, the NoData is typically zero, the maximum, or the minimum value for the underlying data type. The NoData value can also be a user-defined value. In that case the value is designated with ud.\nCellType.uint16().with_no_data_value(99).cell_type_name\n'uint16ud99'\nFloating point types have NaN as the NoData value by default. However, a user-defined NoData can be set.\nprint(CellType.float32().no_data_value())\nprint(CellType.float32().with_no_data_value(-99.9).no_data_value())\nnan\n-99.9","title":"Understanding Cell Types and NoData"},{"location":"/nodata-handling.html#nodata-and-local-arithmetic","text":"Let’s now explore how the presence of NoData affects local map algebra operations. To demonstrate the behavior, lets create two tiles. One tile will have values of 0 and 1, and the other will have values of just 0.\ntile_size = 100\nx = np.zeros((tile_size, tile_size), dtype='int16')\nx[:,tile_size//2:] = 1\nx = Tile(x)\ny = Tile(np.zeros((tile_size, tile_size), dtype='int16'))\n\nrf = spark.createDataFrame([Row(x=x, y=y)])\nprint('x')\ndisplay(x)\nx\nprint('y')\ndisplay(y)\ny\n/anaconda3/envs/rasterframes/lib/python3.7/site-packages/matplotlib/image.py:446: UserWarning: Warning: converting a masked element to nan.\n  dv = np.float64(self.norm.vmax) - np.float64(self.norm.vmin)\n/anaconda3/envs/rasterframes/lib/python3.7/site-packages/matplotlib/image.py:453: UserWarning: Warning: converting a masked element to nan.\n  a_min = np.float64(newmin)\n/anaconda3/envs/rasterframes/lib/python3.7/site-packages/matplotlib/image.py:458: UserWarning: Warning: converting a masked element to nan.\n  a_max = np.float64(newmax)\n/anaconda3/envs/rasterframes/lib/python3.7/site-packages/matplotlib/image.py:478: UserWarning: Warning: converting a masked element to nan.\n  dtype=scaled_dtype)\nNow, let’s create a new column from x with the value of 1 changed to NoData. Then, we will add this new column with NoData to the y column. As shown below, the result of the sum also has NoData (represented in white). In general for local algebra operations, Data + NoData = NoData.\nmasked_rf = rf.withColumn('x_nd', rf_mask_by_value('x', 'x', lit(1)) )\nmasked_rf = masked_rf.withColumn('x_nd_y_sum', rf_local_add('x_nd', 'y'))\nrow = masked_rf.collect()[0]\nprint('x with NoData')\ndisplay(row.x_nd)\nx with NoData\nprint('x with NoData plus y')\ndisplay(row.x_nd_y_sum)\nx with NoData plus y\nTo see more information about possible operations on tile columns, see the local map algebra page and function reference.","title":"NoData and Local Arithmetic"},{"location":"/nodata-handling.html#changing-a-tiles-nodata-values","text":"One way to mask a tile is to make a new tile with a user defined NoData value. We will explore this method below. First, lets create a DataFrame from a tile with values of 0, 1, 2, and 3. We will use numpy to create a 100x100 tile with vertical bands containing values 0, 1, 2, and 3.\ntile_size = 100\nx = np.zeros((tile_size, tile_size), dtype='int16')\n\n# setting the values of the columns\nfor i in range(4):\n    x[:, i*tile_size//4:(i+1)*tile_size//4] = i\nx = Tile(x)\n\nrf = spark.createDataFrame([Row(tile=x)])\ndisplay(x)\nFirst, we mask the value of 1 by making a new tile column with the user defined cell type ‘uint16ud1’. Then, we mask out the value of two by making a tile column with the cell type ‘uint16ud2’.\ndef get_nodata_ct(nd_val):\n\treturn CellType('uint16').with_no_data_value(nd_val)\n\nmasked_rf = rf.withColumn('tile_nd_1',\n                           rf_convert_cell_type('tile', get_nodata_ct(1))) \\\n              .withColumn('tile_nd_2',\n                          rf_convert_cell_type('tile_nd_1', get_nodata_ct(2))) \\\ncollected = masked_rf.collect()\nLet’s look at the new tiles we created. The tile named tile_nd_1 has the 1 values masked out as expected.\ndisplay(collected[0].tile_nd_1)\nAnd the tile named tile_nd_2 has the values of 1 and 2 masked out. This is because we created the tile by setting a new user defined NoData value to tile_nd_1, and the values previously masked out in tile_nd_1 stayed masked when creating tile_nd_2.\ndisplay(collected[0].tile_nd_2)","title":"Changing a Tile’s NoData Values"},{"location":"/nodata-handling.html#combining-tiles-with-different-data-types","text":"RasterFrames supports having tile columns with different cell types in a single DataFrame. It is important to understand how these different cell types interact.\nLet’s first create a RasterFrame that has columns of float and int cell type.\nx = Tile((np.ones((100, 100))*2), CellType.float64())\ny = Tile((np.ones((100, 100))*3), CellType.int32())\nrf = spark.createDataFrame([Row(x=x, y=y)])\n\ncell_types = rf.select(rf_cell_type('x'), rf_cell_type('y')).distinct()\ncell_types\nrf_cell_type(x) rf_cell_type(y) [float64] [int32]\nWhen performing a local operation between tile columns with cell types int and float, the resulting tile cell type will be float. In local algebra over two tiles of different “sized” cell types, the resulting cell type will be the larger of the two input tiles’ cell types.\nsums = rf.select(\n    rf_cell_type('x'),\n    rf_cell_type('y'),\n    rf_cell_type(rf_local_add('x', 'y')).alias('xy_sum'),\n    )\nsums\nrf_cell_type(x) rf_cell_type(y) xy_sum [float64] [int32] [float64]\nCombining tile columns of different cell types gets a little trickier when user defined NoData cell types are involved. Let’s create two tile columns: one with a NoData value of 1, and one with a NoData value of 2 (using our previously defined get_nodata_ct function).\nx_nd_1 = Tile((np.ones((100, 100))*3), get_nodata_ct(1))\nx_nd_2 = Tile((np.ones((100, 100))*3), get_nodata_ct(2))\nrf_nd = spark.createDataFrame([Row(x_nd_1=x_nd_1, x_nd_2=x_nd_2)])\nLet’s try adding the tile columns with different NoData values. When there is an inconsistent NoData value in the two columns, the NoData value of the right-hand side of the sum is kept. In this case, this means the result has a NoData value of 1.\nrf_nd_sum = rf_nd.withColumn('x_nd_sum', rf_local_add('x_nd_2', 'x_nd_1'))\ncell_types = rf_nd_sum.select(rf_cell_type('x_nd_sum')).distinct()\ncell_types\nrf_cell_type(x_nd_sum) [uint16ud2]\nReversing the order of the sum changes the NoData value of the resulting column to 2.\nrf_nd_sum = rf_nd.withColumn('x_nd_sum', rf_local_add('x_nd_1', 'x_nd_2'))\ncell_types = rf_nd_sum.select(rf_cell_type('x_nd_sum')).distinct()\ncell_types\nrf_cell_type(x_nd_sum) [uint16ud1]","title":"Combining Tiles with Different Data Types"},{"location":"/nodata-handling.html#nodata-values-in-aggregation","text":"Let’s use the same tile as before to demonstrate how NoData values affect tile aggregations.\ntile_size = 100\nx = np.zeros((tile_size, tile_size))\nfor i in range(4):\n    x[:, i*tile_size//4:(i+1)*tile_size//4] = i\nx = Tile(x, CellType.int16())\n\nrf = spark.createDataFrame([Row(tile=x)])\ndisplay(x)\nFirst we create the two new masked tile columns as before. One with only the value of 1 masked, and the other with and values of 1 and 2 masked.\nmasked_rf = rf.withColumn('tile_nd_1',\n                           rf_convert_cell_type('tile', get_nodata_ct(1))) \\\n              .withColumn('tile_nd_2',\n                          rf_convert_cell_type('tile_nd_1', get_nodata_ct(2)))\nThe results of rf_tile_sum vary on the tiles that were masked. This is because any cells with NoData values are ignored in the aggregation. Note that tile_nd_2 has the lowest sum, since it has the fewest amount of data cells.\nsums = masked_rf.select(rf_tile_sum('tile'), rf_tile_sum('tile_nd_1'), rf_tile_sum('tile_nd_2'))\nsums\nrf_tile_sum(tile) rf_tile_sum(tile_nd_1) rf_tile_sum(tile_nd_2) 15000.0 12500.0 7500.0","title":"NoData Values in Aggregation"},{"location":"/masking.html","text":"","title":"Masking"},{"location":"/masking.html#masking","text":"Masking is a common operation in raster processing. It is setting certain cells to the NoData value. This is usually done to remove low-quality observations from the raster processing. Another related use case is to “clip” a raster to a given polygon.\nIn this section we will demonstrate two common schemes for masking. In Sentinel 2, there is a separate classification raster that defines low quality areas. In Landsat 8, several quality factors are measured and the indications are packed into a single integer, which we have to unpack.","title":"Masking"},{"location":"/masking.html#masking-sentinel-2","text":"Let’s demonstrate masking with a pair of bands of Sentinel-2 data. The measurement bands we will use, blue and green, have no defined NoData. They share quality information from a separate file called the scene classification (SCL), which delineates areas of missing data and probable clouds. For more information on this, see the Sentinel-2 algorithm overview. Figure 3 tells us how to interpret the scene classification. For this example, we will exclude NoData, defective pixels, probable clouds, and cirrus clouds: values 0, 1, 8, 9, and 10.\nCredit: Sentinel-2 algorithm overview\nThe first step is to create a catalog with our band of interest and the SCL band. We read the data from the catalog, so all tiles are aligned across rows.\nfrom pyspark.sql import Row\n\nblue_uri = 'https://rasterframes.s3.amazonaws.com/samples/luray_snp/B02.tif'\ngreen_uri = 'https://rasterframes.s3.amazonaws.com/samples/luray_snp/B03.tif'\nscl_uri = 'https://rasterframes.s3.amazonaws.com/samples/luray_snp/SCL.tif'\ncat = spark.createDataFrame([Row(blue=blue_uri, green=green_uri, scl=scl_uri),])\nunmasked = spark.read.raster(cat, catalog_col_names=['blue', 'green', 'scl'])\nunmasked.printSchema()\nroot\n |-- blue_path: string (nullable = false)\n |-- green_path: string (nullable = false)\n |-- scl_path: string (nullable = false)\n |-- blue: struct (nullable = true)\n |    |-- tile_context: struct (nullable = true)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- green: struct (nullable = true)\n |    |-- tile_context: struct (nullable = true)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- scl: struct (nullable = true)\n |    |-- tile_context: struct (nullable = true)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\nunmasked.select(rf_cell_type('blue'), rf_cell_type('scl')).distinct()\nrf_cell_type(blue) rf_cell_type(scl) [uint16raw] [uint8raw]","title":"Masking Sentinel 2"},{"location":"/masking.html#define-celltype-for-masked-tile","text":"Because there is not a NoData already defined for the blue band, we must choose one. If we try to apply a masking function to a tile whose cell type has no NoData defined, an error will be thrown.\nIn this particular example, the minimum value of all cells in all tiles in the column is greater than zero, so we can use 0 as the NoData value. We will construct a new CellType object to represent this.\nblue_min = unmasked.agg(rf_agg_stats('blue').min.alias('blue_min'))\nprint('Nonzero minimum value in the blue band:', blue_min.first())\n\nblue_ct = unmasked.select(rf_cell_type('blue')).distinct().first()[0][0]\nmasked_blue_ct = CellType(blue_ct).with_no_data_value(0)\nmasked_blue_ct.cell_type_name\nNonzero minimum value in the blue band: Row(blue_min=3.0)\n'uint16ud0'\nWe next convert the blue band to this cell type.\nconverted = unmasked.select('scl', 'green', rf_convert_cell_type('blue', masked_blue_ct).alias('blue'))","title":"Define CellType for Masked Tile"},{"location":"/masking.html#apply-mask-from-quality-band","text":"Now we set cells of our blue column to NoData for all locations where the scl tile is in our set of undesirable values. This is the actual masking operation.\nfrom pyspark.sql.functions import lit\n\nmasked = converted.withColumn('blue_masked', rf_mask_by_values('blue', 'scl', [0, 1, 8, 9, 10]))\nmasked\nShowing only top 5 rows.\nscl green blue blue_masked\nWe can verify that the number of NoData cells in the resulting blue_masked column matches the total of the boolean mask tile to ensure our logic is correct.\nmasked.select(rf_no_data_cells('blue_masked'), rf_tile_sum(rf_local_is_in('scl', [0, 1, 8, 9, 10])))\nShowing only top 5 rows.\nrf_no_data_cells(blue_masked) rf_tile_sum(rf_local_is_in(scl, array(0, 1, 8, 9, 10))) 247 247.0 16 16.0 131 131.0 0 0.0 5 5.0\nIt’s also nice to view a sample. The white regions are areas of NoData.\nsample = masked.orderBy(-rf_no_data_cells('blue_masked')).select(rf_tile('blue_masked'), rf_tile('scl')).first()\ndisplay(sample[0])\nAnd the original SCL data. The bright yellow is a cloudy region in the original image.\ndisplay(sample[1])","title":"Apply Mask from Quality Band"},{"location":"/masking.html#transferring-mask","text":"We can now apply the same mask from the blue column to the green column. Note here we have supressed the step of explicitly checking what a “safe” NoData value for the green band should be.\nmasked.withColumn('green_masked', rf_mask(rf_convert_cell_type('green', masked_blue_ct), 'blue_masked'))  \\\n      .orderBy(-rf_no_data_cells('blue_masked'))\nShowing only top 5 rows.\nscl green blue blue_masked green_masked","title":"Transferring Mask"},{"location":"/masking.html#masking-landsat-8","text":"We will work with the Landsat scene here. For simplicity, we will just use two of the seven 30m bands. The quality mask for all bands is all contained in the BQA band.\nbase_url = 'https://landsat-pds.s3.us-west-2.amazonaws.com/c1/L8/153/075/LC08_L1TP_153075_20190718_20190731_01_T1/LC08_L1TP_153075_20190718_20190731_01_T1_'\ndata4 = base_url + 'B4.TIF'\ndata2 = base_url + 'B2.TIF'\nmask = base_url + 'BQA.TIF'\nl8_df = spark.read.raster([[data4, data2, mask]]) \\\n               .withColumnRenamed('proj_raster_0', 'data') \\\n               .withColumnRenamed('proj_raster_1', 'data2') \\\n               .withColumnRenamed('proj_raster_2', 'mask')\nMasking is described on the Landsat Missions page. It is pretty dense. Focus for this data set is the Collection 1 Level-1 for Landsat 8.\nThere are several inter-related factors to consider. In this exercise we will mask away the following.\nDesignated Fill = yes Cloud = yes Cloud Shadow Confidence = Medium or High Cirrus Confidence = Medium or High\nNote that you should consider your application and do your own exploratory analysis to determine the most appropriate mask!\nAccording to the information on the Landsat site this translates to masking by bit values in the BQA according to the following table.\nDescription Value Bits Bit values Designated fill yes 0 1 Cloud yes 4 1 Cloud shadow conf. med / hi 7-8 10, 11 (2, 3) Cirrus conf. med / hi 11-12 10, 11 (2, 3)\nIn this case, we will use the value of 0 as the NoData in the band data. Inspecting the associated MTL txt file By inspection, we can discover that the minimum value in the band will be 1, thus allowing our use of 0 for NoData.\nThe code chunk below works through each of the rows in the table above. The first expression sets the cell type to have the selected NoData. The rf_mask_by_bit and rf_mask_by_bits functions extract the selected bit or bits from the mask cells and compare them to the provided values.\nl8_df = l8_df.withColumn('data_masked', # set to cell type that has a nodata \n                   rf_convert_cell_type('data', CellType.uint16())) \\\n       .withColumn('data_masked', # fill yes \n                  rf_mask_by_bit('data_masked', 'mask', 0, 1)) \\\n       .withColumn('data_masked', # cloud yes\n                  rf_mask_by_bit('data_masked', 'mask', 4, 1)) \\\n       .withColumn('data_masked', # cloud shadow conf is medium or high\n                  rf_mask_by_bits('data_masked', 'mask', 7, 2, [2, 3])) \\\n       .withColumn('data_masked', # cloud shadow conf is medium or high\n                  rf_mask_by_bits('data_masked', 'mask', 11, 2, [2, 3])) \\\n       .withColumn('data2', # mask other data col against the other band\n                  rf_mask(rf_convert_cell_type('data2',CellType.uint16()), 'data_masked')) \\\n       .filter(rf_data_cells('data_masked') > 0) # remove any entirely ND rows\n\n# Inspect a sample\nl8_df.select('data', 'mask', 'data_masked', 'data2', rf_extent('data_masked')) \\\n     .filter(rf_data_cells('data_masked') > 32000)\nShowing only top 5 rows.\ndata mask data_masked data2 rf_extent(data_masked) [363525.0, -2327565.0, 371205.0, -2319885.0] [355845.0, -2488845.0, 363525.0, -2481165.0] [286725.0, -2450445.0, 294405.0, -2442765.0] [432645.0, -2419725.0, 440325.0, -2412045.0] [378885.0, -2381325.0, 386565.0, -2373645.0]","title":"Masking Landsat 8"},{"location":"/masking.html#clipping","text":"Clipping is the use of a polygon to determine the areas to mask in a raster. Typically the areas inside a polygon are retained and the cells outside are set to NoData. Given a geometry column on our DataFrame, we have to carry out three basic steps. First we have to ensure the vector geometry is correctly projected to the same CRS as the raster. We’ll continue with our Sentinel 2 example, creating a simple polygon. Buffering a point will create an approximate circle.\nto_rasterize = masked.withColumn('geom_4326', \n                            st_bufferPoint(\n                                st_point(lit(-78.0783132), lit(38.3184340)), \n                                lit(15000))) \\\n                .withColumn('geom_native', st_reproject('geom_4326', rf_mk_crs('epsg:4326'), rf_crs('blue_masked')))\nSecond, we will rasterize the geometry, or burn-in the geometry into the same grid as the raster.\nto_clip = to_rasterize.withColumn('clip_raster', \n                                 rf_rasterize('geom_native', rf_geometry('blue_masked'), lit(1), rf_dimensions('blue_masked').cols, rf_dimensions('blue_masked').rows))\n\n# visualize some of the edges of our circle\nto_clip.select('blue_masked', 'clip_raster') \\\n    .filter(rf_data_cells('clip_raster') > 20) \\\n    .orderBy(rf_data_cells('clip_raster'))\nShowing only top 5 rows.\nblue_masked clip_raster\nFinally, we create a new tile column with the blue band clipped to our circle. Again we will use the rf_mask function to pass the NoData regions along from the rasterized geometry.\nto_clip.select('blue_masked', \n               'clip_raster',\n               rf_mask('blue_masked', 'clip_raster').alias('blue_clipped')) \\\n           .filter(rf_data_cells('clip_raster') > 20) \\\n           .orderBy(rf_data_cells('clip_raster'))\nShowing only top 5 rows.\nblue_masked clip_raster blue_clipped\nThis kind of clipping technique is further used in zonal statistics.","title":"Clipping"},{"location":"/zonal-algebra.html","text":"","title":"Zonal Map Algebra"},{"location":"/zonal-algebra.html#zonal-map-algebra","text":"","title":"Zonal Map Algebra"},{"location":"/zonal-algebra.html#definition","text":"Zonal map algebra refers to operations over raster cells based on the definition of a zone. In concept, a zone is like a mask: a raster with a special value designating membership of the cell in the zone. In general, we assume that zones are defined by vector geometries.","title":"Definition"},{"location":"/zonal-algebra.html#analysis-plan","text":"We will compute average NDVI over the month of May 2018 for two US national parks: Cuyahoga Valley and Indiana Dunes. We will select data from the built-in catalog, join it with park geometries, read tiles for the bands needed, burn-in or rasterize the geometries to tiles, and compute the aggregate.","title":"Analysis Plan"},{"location":"/zonal-algebra.html#get-vector-data","text":"First we download vector from the US National Park Service open data portal, and take a look at the data.\nimport requests\nimport folium\n\nnps_filepath = '/tmp/2parks.geojson'\nnps_data_query_url = 'https://services1.arcgis.com/fBc8EJBxQRMcHlei/arcgis/rest/services/' \\\n                     'NPS_Park_Boundaries/FeatureServer/0/query' \\\n                     '?geometry=-87.601,40.923,-81.206,41.912&inSR=4326&outSR=4326' \\\n                     \"&where=UNIT_TYPE='National Park'&outFields=*&f=geojson\"\nr = requests.get(nps_data_query_url)\nwith open(nps_filepath,'wb') as f:\n    f.write(r.content)\n\nm = folium.Map()\nlayer = folium.GeoJson(nps_filepath)\nm.fit_bounds(layer.get_bounds())\nm.add_child(layer)\nm\nNow we read the park boundary vector data as a Spark DataFrame using the built-in geojson DataSource. The geometry is very detailed, and the EO cells are relatively coarse. To speed up the processing, the geometry we “simplify” by combining vertices within about 500 meters of each other. For more on this see the section on Shapely support in user defined functions.\npark_vector = spark.read.geojson(nps_filepath)\n\n@udf(MultiPolygonUDT())\ndef simplify(g, tol):\n    return g.simplify(tol)\n\npark_vector = park_vector.withColumn('geo_simp', simplify('geometry', lit(0.005))) \\\n                         .select('geo_simp', 'OBJECTID', 'UNIT_NAME') \\\n                         .hint('broadcast')","title":"Get Vector Data"},{"location":"/zonal-algebra.html#catalog-read","text":"Both parks are entirely contained in MODIS granule h11 v04. We will simply filter on this granule, rather than using a spatial relation.\ncat = spark.read.format('aws-pds-modis-catalog').load().repartition(50)\npark_cat = cat \\\n            .filter(\n                    (cat.granule_id == 'h11v04') &\n                    (cat.acquisition_date >= lit('2018-05-01')) &\n                    (cat.acquisition_date < lit('2018-06-01'))            \n                    ) \\\n            .crossJoin(park_vector)\n                \npark_cat.printSchema()\nroot\n |-- product_id: string (nullable = false)\n |-- acquisition_date: timestamp (nullable = false)\n |-- granule_id: string (nullable = false)\n |-- gid: string (nullable = false)\n |-- B01: string (nullable = true)\n |-- B01qa: string (nullable = true)\n |-- B02: string (nullable = true)\n |-- B02qa: string (nullable = true)\n |-- B03: string (nullable = true)\n |-- B03aq: string (nullable = true)\n |-- B04: string (nullable = true)\n |-- B04qa: string (nullable = true)\n |-- B05: string (nullable = true)\n |-- B05qa: string (nullable = true)\n |-- B06: string (nullable = true)\n |-- B06qa: string (nullable = true)\n |-- B07: string (nullable = true)\n |-- B07qa: string (nullable = true)\n |-- geo_simp: multipolygon (nullable = true)\n |-- OBJECTID: long (nullable = true)\n |-- UNIT_NAME: string (nullable = true)\nWe will combine the park geometry with the catalog, and read only the bands of interest to compute NDVI, which we discussed in a previous section.\nNow we have a dataframe with several months of MODIS data for a single granule. However, the granule covers a great deal of area outside our park boundaries zones. To deal with this we will, first reproject the park geometry to the same CRS as the imagery. Then we will filter to only the tiles intersecting the park zones.\nraster_cols = ['B01', 'B02',] # red and near-infrared respectively\npark_rf = spark.read.raster(\n        park_cat.select(['acquisition_date', 'granule_id'] + raster_cols + park_vector.columns),\n        catalog_col_names=raster_cols) \\\n    .withColumn('park_native', st_reproject('geo_simp', lit('EPSG:4326'), rf_crs('B01'))) \\\n    .filter(st_intersects('park_native', rf_geometry('B01'))) \n\npark_rf.printSchema()\nroot\n |-- B01_path: string (nullable = false)\n |-- B02_path: string (nullable = false)\n |-- B01: struct (nullable = true)\n |    |-- tile_context: struct (nullable = true)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- B02: struct (nullable = true)\n |    |-- tile_context: struct (nullable = true)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- acquisition_date: timestamp (nullable = false)\n |-- granule_id: string (nullable = false)\n |-- geo_simp: multipolygon (nullable = true)\n |-- OBJECTID: long (nullable = true)\n |-- UNIT_NAME: string (nullable = true)\n |-- park_native: geometry (nullable = true)","title":"Catalog Read"},{"location":"/zonal-algebra.html#define-zone-tiles","text":"Now we have the vector representation of the park boundary alongside the tiles of red and near infrared bands. Next, we need to create a tile representation of the park to allow us to limit the raster analysis to pixels within the park zone. This is similar to the masking operation demonstrated in Masking. We rasterize the geometries using rf_rasterize: this creates a new tile column aligned with the imagery, and containing the park’s OBJECTID attribute for cells intersecting the zone. Cells outside the park zones have a NoData value.\nrf_park_tile = park_rf \\\n    .withColumn('dims', rf_dimensions('B01')) \\\n    .withColumn('park_zone_tile', rf_rasterize('park_native', rf_geometry('B01'), 'OBJECTID', 'dims.cols', 'dims.rows')) \\\n    .persist()\n\nrf_park_tile.printSchema()\nroot\n |-- B01_path: string (nullable = false)\n |-- B02_path: string (nullable = false)\n |-- B01: struct (nullable = true)\n |    |-- tile_context: struct (nullable = true)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- B02: struct (nullable = true)\n |    |-- tile_context: struct (nullable = true)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- acquisition_date: timestamp (nullable = false)\n |-- granule_id: string (nullable = false)\n |-- geo_simp: multipolygon (nullable = true)\n |-- OBJECTID: long (nullable = true)\n |-- UNIT_NAME: string (nullable = true)\n |-- park_native: geometry (nullable = true)\n |-- dims: struct (nullable = true)\n |    |-- cols: integer (nullable = false)\n |    |-- rows: integer (nullable = false)\n |-- park_zone_tile: tile (nullable = true)","title":"Define Zone Tiles"},{"location":"/zonal-algebra.html#compute-zonal-statistics","text":"We compute NDVI as the normalized difference of near infrared (band 2) and red (band 1). The tiles are masked by the park_zone_tile, limiting the cells to those in the zone. To finish, we compute our desired statistics over the NVDI tiles that are limited by the zone.\nfrom pyspark.sql.functions import col\nfrom pyspark.sql import functions as F\n\nrf_ndvi = rf_park_tile \\\n    .withColumn('ndvi', rf_normalized_difference('B02', 'B01')) \\\n    .withColumn('ndvi_masked', rf_mask('ndvi', 'park_zone_tile'))\n\nzonal_mean = rf_ndvi \\\n        .groupby('OBJECTID', 'UNIT_NAME') \\\n        .agg(rf_agg_mean('ndvi_masked'))\n\nzonal_mean\nOBJECTID UNIT_NAME rf_agg_mean(ndvi_masked) 375 Indiana Dunes National Park 0.48937965870407857 324 Cuyahoga Valley National Park 0.6309672224560483","title":"Compute Zonal Statistics"},{"location":"/aggregation.html","text":"","title":"Aggregation"},{"location":"/aggregation.html#aggregation","text":"There are three types of aggregate functions available in RasterFrames: tile aggregate, DataFrame aggregate, and element-wise local aggregate. In the tile aggregate functions, we are computing a statistical summary per row of a tile column in a DataFrame. In the DataFrame aggregate functions, we are computing statistical summaries over all of the cell values and across all of the rows in the DataFrame or group. In the element-wise local aggregate functions, we are computing the element-wise statistical summary across a DataFrame or group of tiles. In the latter two cases, when vector data is the grouping column, the results are zonal statistics.","title":"Aggregation"},{"location":"/aggregation.html#tile-mean-example","text":"We can illustrate aggregate differences by computing an aggregate mean. First, we create a sample DataFrame of 2 tiles. The tiles will contain normally distributed cell values with the first row’s mean at 1.0 and the second row’s mean at 3.0. For details on use of the Tile class see the page on numpy interoperability.\nfrom pyrasterframes.rf_types import Tile, CellType\n\nt1 = Tile(1 + 0.1 * np.random.randn(5,5), CellType('float64raw'))\n\nt1.cells  # display the array in the Tile\narray([[1.029, 0.901, 0.897, 1.002, 0.932],\n       [1.013, 0.915, 0.968, 1.035, 1.072],\n       [0.882, 1.089, 0.845, 0.918, 0.943],\n       [1.019, 1.088, 1.095, 0.872, 1.064],\n       [0.902, 0.937, 1.106, 0.993, 0.93 ]])\nt5 = Tile(5 + 0.1 * np.random.randn(5,5), CellType('float64raw'))\nt5.cells\narray([[4.935, 4.815, 5.069, 4.968, 5.165],\n       [4.976, 4.883, 4.881, 5.074, 5.065],\n       [5.146, 5.136, 4.943, 5.14 , 5.021],\n       [4.918, 5.026, 5.213, 4.918, 4.719],\n       [5.027, 4.953, 4.947, 5.042, 5.039]])\nCreate a Spark DataFrame from the Tile objects.\nimport pyspark.sql.functions as F\nfrom pyspark.sql import Row\n\nrf = spark.createDataFrame([\n    Row(id=1, tile=t1),\n    Row(id=2, tile=t5)\n]).orderBy('id')\nWe use the rf_tile_mean function to compute the tile aggregate mean of cells in each row of column tile. The mean of each tile is computed separately, so the first mean is about 1.0 and the second mean is about 3.0. Notice that the number of rows in the DataFrame is the same before and after the aggregation.\nrf.select(F.col('id'), rf_tile_mean(F.col('tile')))\nid rf_tile_mean(tile) 1 0.9777736017127512 2 5.000752105081167\nWe use the rf_agg_mean function to compute the DataFrame aggregate, which averages values across the fifty cells in two rows. Note that only a single row is returned since the average is computed over the full DataFrame.\nrf.agg(rf_agg_mean(F.col('tile')))\nrf_agg_mean(tile) 2.9892628533969594\nWe use the rf_agg_local_mean function to compute the element-wise local aggregate mean across the two rows. For this aggregation, we are computing the mean of one value of 1.0 and one value of 3.0 to arrive at the element-wise mean, but doing so twenty-five times, one for each position in the tile.\nTo compute an element-wise local aggregate, tiles need to have the same dimensions. In this case, both tiles have 5 rows and 5 columns. If we tried to compute an element-wise local aggregate over the DataFrame without equal tile dimensions, we would get a runtime error.\nrf.agg(rf_agg_local_mean('tile')) \\\n    .first()[0].cells.data  # display the contents of the Tile array\narray([[2.982, 2.858, 2.983, 2.985, 3.049],\n       [2.994, 2.899, 2.925, 3.054, 3.068],\n       [3.014, 3.113, 2.894, 3.029, 2.982],\n       [2.969, 3.057, 3.154, 2.895, 2.892],\n       [2.964, 2.945, 3.026, 3.017, 2.984]])","title":"Tile Mean Example"},{"location":"/aggregation.html#cell-counts-example","text":"We can also count the total number of data and NoData cells over all the tiles in a DataFrame using rf_agg_data_cells and rf_agg_no_data_cells. There are ~3.8 million data cells and ~1.9 million NoData cells in this DataFrame. See the section on “NoData” handling for additional discussion on handling missing data.\nrf = spark.read.raster('https://rasterframes.s3.amazonaws.com/samples/MCD43A4.006/11/05/2018233/MCD43A4.A2018233.h11v05.006.2018242035530_B02.TIF')\nstats = rf.agg(rf_agg_data_cells('proj_raster'), rf_agg_no_data_cells('proj_raster'))\nstats\nrf_agg_data_cells(proj_raster) rf_agg_no_data_cells(proj_raster) 3825959 1934041","title":"Cell Counts Example"},{"location":"/aggregation.html#statistical-summaries","text":"The statistical summary functions return a summary of cell values: number of data cells, number of NoData cells, minimum, maximum, mean, and variance, which can be computed as a tile aggregate, a DataFrame aggregate, or an element-wise local aggregate.\nThe rf_tile_stats function computes summary statistics separately for each row in a tile column as shown below.\nrf = spark.read.raster('https://rasterframes.s3.amazonaws.com/samples/luray_snp/B02.tif')\nstats = rf.select(rf_tile_stats('proj_raster').alias('stats'))\n\nstats.printSchema()\nroot\n |-- stats: struct (nullable = true)\n |    |-- data_cells: long (nullable = false)\n |    |-- no_data_cells: long (nullable = false)\n |    |-- min: double (nullable = false)\n |    |-- max: double (nullable = false)\n |    |-- mean: double (nullable = false)\n |    |-- variance: double (nullable = false)\nstats.select('stats.min', 'stats.max', 'stats.mean', 'stats.variance')\nShowing only top 5 rows.\nmin max mean variance 239.0 2047.0 359.02950246710566 23843.848327795215 161.0 3811.0 300.751678466797 19873.49985985365 215.0 10201.0 1242.8558654785195 2931419.376503068 253.0 990.0 338.8358725761777 9432.898962465755 175.0 11004.0 984.6691741943379 1987081.8477228696\nThe rf_agg_stats function aggregates over all of the tiles in a DataFrame and returns a statistical summary of all cell values as shown below.\nstats = rf.agg(rf_agg_stats('proj_raster').alias('stats')) \\\n    .select('stats.min', 'stats.max', 'stats.mean', 'stats.variance')\nstats\nmin max mean variance 3.0 12103.0 542.1327946489893 685615.201702677\nThe rf_agg_local_stats function computes the element-wise local aggregate statistical summary as shown below. The DataFrame used in the previous two code blocks has unequal tile dimensions, so a different DataFrame is used in this code block to avoid a runtime error.\nrf = spark.createDataFrame([\n    Row(id=1, tile=t1),\n    Row(id=3, tile=t1 * 3),\n    Row(id=5, tile=t1 * 5)\n]).agg(rf_agg_local_stats('tile').alias('stats'))\n    \nagg_local_stats = rf.select('stats.min', 'stats.max', 'stats.mean', 'stats.variance').collect()\n\nfor r in agg_local_stats:\n    for stat in r.asDict():\n        print(stat, ':\\n', r[stat], '\\n')\nmin :\n Tile(dimensions=[5, 5], cell_type=CellType(float64, nan), cells=\n[[1.028512949249102 0.9006189997532675 0.8971728835747308\n  1.002068356214286 0.9321462409230148]\n [1.0126667891814458 0.9148072259708329 0.9684115387004245\n  1.0347924734353402 1.0716593316892458]\n [0.8817176739676914 1.0890033858198382 0.8448466821569003\n  0.918312790631051 0.9428310923100086]\n [1.0189912660713847 1.0876366426868298 1.0946450901390512\n  0.8723867769755788 1.0641414795651045]\n [0.9017652499597776 0.9373817930893357 1.1057028212801712\n  0.9925594019717625 0.9295611075026057]]) \n\nmax :\n Tile(dimensions=[5, 5], cell_type=CellType(float64, nan), cells=\n[[5.14256474624551 4.503094998766338 4.485864417873654 5.01034178107143\n  4.6607312046150735]\n [5.063333945907229 4.574036129854164 4.842057693502123 5.173962367176701\n  5.358296658446228]\n [4.408588369838457 5.445016929099191 4.224233410784501 4.591563953155255\n  4.714155461550043]\n [5.094956330356924 5.438183213434149 5.4732254506952565\n  4.361933884877894 5.320707397825522]\n [4.508826249798888 4.686908965446678 5.528514106400856 4.962797009858813\n  4.647805537513029]]) \n\nmean :\n Tile(dimensions=[5, 5], cell_type=CellType(float64, nan), cells=\n[[3.0855388477473062 2.7018569992598027 2.6915186507241926\n  3.006205068642858 2.7964387227690444]\n [3.038000367544337 2.7444216779124986 2.9052346161012736\n  3.1043774203060206 3.2149779950677373]\n [2.645153021903074 3.2670101574595147 2.534540046470701\n  2.7549383718931533 2.828493276930026]\n [3.056973798214154 3.2629099280604894 3.2839352704171536\n  2.6171603309267364 3.1924244386953133]\n [2.705295749879333 2.812145379268007 3.3171084638405133\n  2.9776782059152875 2.788683322507817]]) \n\nvariance :\n Tile(dimensions=[5, 5], cell_type=CellType(float64, nan), cells=\n[[2.820903698061562 2.1629722205775375 2.146451154724792\n  2.6777093080693373 2.317057638578418]\n [2.734650735762827 2.2316593618358693 2.5008557554349977\n  2.8554545682091454 3.062543261857707]\n [2.0731361508986543 3.1624756648721917 1.9033757769373931\n  2.2487956838309 2.3704812496706236]\n [2.76891520087937 3.154542577373542 3.1953276623080864 2.029489836378236\n  3.0197255694160265]\n [2.168481509426721 2.343159002707674 3.2602099439651457\n  2.6271311105134476 2.3042236068839212]])","title":"Statistical Summaries"},{"location":"/aggregation.html#histogram","text":"The rf_tile_histogram function computes a count of cell values within each row of tile and outputs a bins array with the schema below. In the graph below, we have plotted each bin’s value on the x-axis and count on the y-axis for the tile in the first row of the DataFrame.\nimport matplotlib.pyplot as plt\n\nrf = spark.read.raster('https://rasterframes.s3.amazonaws.com/samples/MCD43A4.006/11/05/2018233/MCD43A4.A2018233.h11v05.006.2018242035530_B02.TIF')\n\nhist_df = rf.select(rf_tile_histogram('proj_raster')['bins'].alias('bins'))\nhist_df.printSchema()\n\nbins_row = hist_df.first()\nvalues = [int(bin['value']) for bin in bins_row.bins]\ncounts = [int(bin['count']) for bin in bins_row.bins]\n\nplt.hist(values, weights=counts, bins=100)\nplt.show()\nroot\n |-- bins: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- value: double (nullable = false)\n |    |    |-- count: long (nullable = false)\nThe rf_agg_approx_histogram function computes a count of cell values across all of the rows of tile in a DataFrame or group. In the example below, the range of the y-axis is significantly wider than the range of the y-axis on the previous histogram since this histogram was computed for all cell values in the DataFrame.\nbins_list = rf.agg(\n    rf_agg_approx_histogram('proj_raster')['bins'].alias('bins')\n    ).collect()\nvalues = [int(row['value']) for row in bins_list[0].bins]\ncounts = [int(row['count']) for row in bins_list[0].bins]\n\nplt.hist(values, weights=counts, bins=100)\nplt.show()","title":"Histogram"},{"location":"/time-series.html","text":"","title":"Time Series"},{"location":"/time-series.html#time-series","text":"","title":"Time Series"},{"location":"/time-series.html#analysis-plan","text":"In this example, we will show how the flexibility of the DataFrame concept for raster data allows a simple and intuitive way to extract a time series from Earth observation data. We will continue our example from the Zonal Map Algebra page.\nWe will summarize the change in NDVI over the spring and early summer of 2018 in the Cuyahoga Valley National Park in Ohio, USA.","title":"Analysis Plan"},{"location":"/time-series.html#catalog-read","text":"As in our other example, we will query for a single known MODIS granule directly. We limit the vector data to the single park of interest. The longer time period selected should show the change in plant vigor as leaves emerge over the spring and into early summer. The definitions of cat and park_vector are as in the Zonal Map Algebra page.\npark_cat = cat \\\n            .filter(\n                    (cat.granule_id == 'h11v04') &\n                    (cat.acquisition_date > lit('2018-02-19')) &\n                    (cat.acquisition_date < lit('2018-07-01'))            \n                    ) \\\n            .crossJoin(park_vector.filter('UNIT_CODE == \"CUVA\"')) #only coyahuga","title":"Catalog Read"},{"location":"/time-series.html#vector-and-raster-data-interaction","text":"We follow the same steps as the Zonal Map Algebra analysis: reprojecting the park geometry, filtering for intersection, rasterizing the geometry, and masking the NDVI by the zone tiles. The code from that analysis is condensed here for reference.\nraster_cols = ['B01', 'B02',] # red and near-infrared respectively\n\nrf_park_tile = spark.read.raster(\n        park_cat.select(['acquisition_date', 'granule_id', 'geo_simp'] + raster_cols),\n        catalog_col_names=raster_cols) \\\n    .withColumn('park_native', st_reproject('geo_simp', lit('EPSG:4326'), rf_crs('B01'))) \\\n    .filter(st_intersects('park_native', rf_geometry('B01'))) \\\n    .withColumn('dims', rf_dimensions('B01')) \\\n    .withColumn('park_tile', rf_rasterize('park_native', rf_geometry('B01'), lit(1), 'dims.cols', 'dims.rows')) \\\n    .withColumn('ndvi', rf_normalized_difference('B02', 'B01')) \\\n    .withColumn('ndvi_masked', rf_mask('ndvi', 'park_tile'))","title":"Vector and Raster Data Interaction"},{"location":"/time-series.html#create-time-series","text":"We next aggregate across the cell values to arrive at an average NDVI for each week of the year. We use pyspark’s built in groupby and time functions with a RasterFrames aggregate function to do this. Note that the computation is creating a weighted average, which is weighted by the number of valid observations per week.\nfrom pyspark.sql.functions import col, year, weekofyear, month\n\ntime_series = rf_park_tile \\\n        .groupby(\n            year('acquisition_date').alias('year'), \n            weekofyear('acquisition_date').alias('week')) \\\n        .agg(rf_agg_mean('ndvi_masked').alias('ndvi'))\nFinally, we will take a look at the NDVI over time.\nimport matplotlib.pyplot as plt\n\ntime_series_pdf = time_series.toPandas()\ntime_series_pdf.sort_values('week', inplace=True)\nplt.plot(time_series_pdf['week'], time_series_pdf['ndvi'], 'go-')\nplt.xlabel('Week of year, 2018')\nplt.ylabel('NDVI')\nplt.title('Cuyahoga Valley NP Green-up')\nText(0.5, 1.0, 'Cuyahoga Valley NP Green-up')\nWe can see two fairly clear elbows in the curve at week 17 and week 21, indicating the start and end of the green up period. Estimation of such parameters is one technique phenology researchers use to monitor changes in climate and environment.","title":"Create Time Series"},{"location":"/raster-join.html","text":"","title":"Raster Join"},{"location":"/raster-join.html#raster-join","text":"","title":"Raster Join"},{"location":"/raster-join.html#description","text":"A common operation for raster data is reprojecting or warping the data to a different CRS with a specific transform. In many use cases, the particulars of the warp operation depend on another set of raster data. Furthermore, the warp is done to put both sets of raster data to a common set of grid to enable manipulation of the datasets together.\nIn RasterFrames, you can perform a Raster Join on two DataFrames containing raster data. The operation will perform a spatial join based on the CRS and extent data in each DataFrame. By default it is a left join and uses an intersection operator. For each candidate row, all tile columns on the right hand side are warped to match the left hand side’s CRS, extent, and dimensions. Warping relies on GeoTrellis library code. You can specify the resampling method to be applied as one of: nearest_neighbor, bilinear, cubic_convolution, cubic_spline, lanczos, average, mode, median, max, min, or sum. The operation is also an aggregate, with multiple intersecting right-hand side tiles merged into the result. There is no guarantee about the ordering of tiles used to select cell values in the case of overlapping tiles. When using the raster DataSource you will automatically get the CRS and extent information needed to do this operation.","title":"Description"},{"location":"/raster-join.html#example-code","text":"Because the raster join is a distributed spatial join, indexing of both DataFrames using the spatial index is crucial for performance.\n# Southern Mozambique December 29, 2016\nmodis = spark.read.raster('s3://astraea-opendata/MCD43A4.006/21/11/2016297/MCD43A4.A2016297.h21v11.006.2016306075821_B01.TIF',\n                          spatial_index_partitions=True) \\\n                  .withColumnRenamed('proj_raster', 'modis')\n\nlandsat8 = spark.read.raster('https://landsat-pds.s3.us-west-2.amazonaws.com/c1/L8/167/077/LC08_L1TP_167077_20161015_20170319_01_T1/LC08_L1TP_167077_20161015_20170319_01_T1_B4.TIF',\n                             spatial_index_partitions=True) \\\n                  .withColumnRenamed('proj_raster', 'landsat')\n\nrj = landsat8.raster_join(modis, resampling_method=\"cubic_convolution\")\n\n# Show some non-empty tiles\nrj.select('landsat', 'modis', 'crs', 'extent') \\\n  .filter(rf_data_cells('modis') > 0) \\\n  .filter(rf_tile_max('landsat') > 0)\nShowing only top 5 rows.\nlandsat modis crs extent [+proj=utm +zone=36 +datum=WGS84 +units=m +no_defs ] [525945.0, -2813925.0, 533625.0, -2806245.0] [+proj=utm +zone=36 +datum=WGS84 +units=m +no_defs ] [495225.0, -2652645.0, 502905.0, -2644965.0] [+proj=utm +zone=36 +datum=WGS84 +units=m +no_defs ] [641145.0, -2675685.0, 648825.0, -2668005.0] [+proj=utm +zone=36 +datum=WGS84 +units=m +no_defs ] [525945.0, -2798565.0, 533625.0, -2790885.0] [+proj=utm +zone=36 +datum=WGS84 +units=m +no_defs ] [541305.0, -2798565.0, 548985.0, -2790885.0]","title":"Example Code"},{"location":"/raster-join.html#additional-options","text":"The following optional arguments are allowed:\nleft_extent - the column on the left-hand DataFrame giving the extent of the tile columns left_crs - the column on the left-hand DataFrame giving the CRS of the tile columns right_extent - the column on the right-hand DataFrame giving the extent of the tile columns right_crs - the column on the right-hand DataFrame giving the CRS of the tile columns join_exprs - a single column expression as would be used in the on parameter of join resampling_method - resampling algorithm to use in reprojection of right-hand tile column\nNote that the join_exprs will override the join behavior described above. By default the expression is equivalent to:\nst_intersects(\n    st_geometry(left[left_extent]), \n    st_reproject(st_geometry(right[right_extent]), right[right_crs], left[left_crs])\n)\nResampling method to use can be specified by passing one of the following strings into resampling_method parameter. The point resampling methods are: \"nearest_neighbor\", \"bilinear\", \"cubic_convolution\", \"cubic_spline\", and \"lanczos\". The aggregating resampling methods are: \"average\", \"mode\", \"median\", \"max\", “min”, or \"sum\". Note the aggregating methods are intended for downsampling. For example a 0.25 factor and max method returns the maximum value in a 4x4 neighborhood.","title":"Additional Options"},{"location":"/machine-learning.html","text":"","title":"Machine Learning"},{"location":"/machine-learning.html#machine-learning","text":"RasterFrames provides facilities to train and predict with a wide variety of machine learning models through Spark ML Pipelines. This library provides a variety of pipeline components for supervised learning, unsupervised learning, and data preparation that can be used to represent and repeatably conduct a variety of tasks in machine learning.\nThe following sections provide some examples on how to integrate these workflows with RasterFrames.\nUnsupervised Machine Learning Imports and Data Preparation Create ML Pipeline Fit the Model and Score Visualize Prediction Supervised Machine Learning Create and Read Raster Catalog Data Prep Masking Poor Quality Cells Create ML Pipeline Train the Model Model Evaluation Visualize Prediction","title":"Machine Learning"},{"location":"/unsupervised-learning.html","text":"","title":"Unsupervised Machine Learning"},{"location":"/unsupervised-learning.html#unsupervised-machine-learning","text":"In this example, we will demonstrate how to fit and score an unsupervised learning model with a sample of Landsat 8 data.","title":"Unsupervised Machine Learning"},{"location":"/unsupervised-learning.html#imports-and-data-preparation","text":"We import various Spark components needed to construct our Pipeline.\nimport pandas as pd\nfrom pyrasterframes import TileExploder\nfrom pyrasterframes.rasterfunctions import *\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml import Pipeline\nThe first step is to create a Spark DataFrame of our imagery data. To achieve that we will create a catalog DataFrame using the pattern from the I/O page. In the catalog, each row represents a distinct area and time, and each column is the URI to a band’s image product. The resulting Spark DataFrame may have many rows per URI, with a column corresponding to each band.\nfilenamePattern = \"https://rasterframes.s3.amazonaws.com/samples/elkton/L8-B{}-Elkton-VA.tiff\"\ncatalog_df = pd.DataFrame([\n    {'b' + str(b): filenamePattern.format(b) for b in range(1, 8)}\n])\n\ntile_size = 256\ndf = spark.read.raster(catalog_df, catalog_col_names=catalog_df.columns, tile_size=tile_size)\ndf = df.withColumn('crs', rf_crs(df.b1)) \\\n       .withColumn('extent', rf_extent(df.b1))\ndf.printSchema()\nroot\n |-- b1_path: string (nullable = false)\n |-- b2_path: string (nullable = false)\n |-- b3_path: string (nullable = false)\n |-- b4_path: string (nullable = false)\n |-- b5_path: string (nullable = false)\n |-- b6_path: string (nullable = false)\n |-- b7_path: string (nullable = false)\n |-- b1: struct (nullable = true)\n |    |-- tile_context: struct (nullable = true)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- b2: struct (nullable = true)\n |    |-- tile_context: struct (nullable = true)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- b3: struct (nullable = true)\n |    |-- tile_context: struct (nullable = true)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- b4: struct (nullable = true)\n |    |-- tile_context: struct (nullable = true)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- b5: struct (nullable = true)\n |    |-- tile_context: struct (nullable = true)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- b6: struct (nullable = true)\n |    |-- tile_context: struct (nullable = true)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- b7: struct (nullable = true)\n |    |-- tile_context: struct (nullable = true)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- crs: struct (nullable = true)\n |    |-- crsProj4: string (nullable = false)\n |-- extent: struct (nullable = true)\n |    |-- xmin: double (nullable = false)\n |    |-- ymin: double (nullable = false)\n |    |-- xmax: double (nullable = false)\n |    |-- ymax: double (nullable = false)\nIn this small example, all the images in our catalog_df have the same CRS, which we verify in the code snippet below. The crs object will be useful for visualization later.\ncrses = df.select('crs.crsProj4').distinct().collect()\nprint('Found ', len(crses), 'distinct CRS: ', crses)\nassert len(crses) == 1\ncrs = crses[0]['crsProj4']\nFound  1 distinct CRS:  [Row(crsProj4='+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs ')]","title":"Imports and Data Preparation"},{"location":"/unsupervised-learning.html#create-ml-pipeline","text":"SparkML requires that each observation be in its own row, and features for each observation be packed into a single Vector. For this unsupervised learning problem, we will treat each pixel as an observation and each band as a feature. The first step is to “explode” the tiles into a single row per pixel. In RasterFrames, generally a pixel is called a cell.\nexploder = TileExploder()\nTo “vectorize” the band columns, we use the SparkML VectorAssembler. Each of the seven bands is a different feature.\nassembler = VectorAssembler() \\\n    .setInputCols(list(catalog_df.columns)) \\\n    .setOutputCol(\"features\")\nFor this problem, we will use the K-means clustering algorithm and configure our model to have 5 clusters.\nkmeans = KMeans().setK(5).setFeaturesCol('features')\nWe can combine the above stages into a single Pipeline.\npipeline = Pipeline().setStages([exploder, assembler, kmeans])","title":"Create ML Pipeline"},{"location":"/unsupervised-learning.html#fit-the-model-and-score","text":"Fitting the pipeline actually executes exploding the tiles, assembling the features vectors, and fitting the K-means clustering model.\nmodel = pipeline.fit(df)\nWe can use the transform function to score the training data in the fitted pipeline model. This will add a column called prediction with the closest cluster identifier.\nclustered = model.transform(df)\nNow let’s take a look at some sample output.\nclustered.select('prediction', 'extent', 'column_index', 'row_index', 'features')\nShowing only top 5 rows.\nprediction extent column_index row_index features 0 [703986.502389, 4249551.61978, 709549.093643, 4254601.8671] 0 0 [9470.0,8491.0,7805.0,6697.0,17507.0,10338.0,7235.0] 0 [703986.502389, 4249551.61978, 709549.093643, 4254601.8671] 1 0 [9566.0,8607.0,8046.0,6898.0,18504.0,11545.0,7877.0] 2 [703986.502389, 4249551.61978, 709549.093643, 4254601.8671] 2 0 [9703.0,8808.0,8377.0,7222.0,20556.0,13207.0,8686.0] 2 [703986.502389, 4249551.61978, 709549.093643, 4254601.8671] 3 0 [9856.0,8983.0,8565.0,7557.0,19479.0,13203.0,9065.0] 1 [703986.502389, 4249551.61978, 709549.093643, 4254601.8671] 4 0 [10105.0,9270.0,8851.0,7912.0,19074.0,12737.0,8947.0]\nIf we want to inspect the model statistics, the SparkML API requires us to go through this unfortunate contortion to access the clustering results:\ncluster_stage = model.stages[2]\nWe can then compute the sum of squared distances of points to their nearest center, which is elemental to most cluster quality metrics.\nmetric = cluster_stage.computeCost(clustered)\nprint(\"Within set sum of squared errors: %s\" % metric)\nWithin set sum of squared errors: 249577233410.3331","title":"Fit the Model and Score"},{"location":"/unsupervised-learning.html#visualize-prediction","text":"We can recreate the tiled data structure using the metadata added by the TileExploder pipeline stage.\nfrom pyrasterframes.rf_types import CellType\n\nretiled = clustered.groupBy('extent', 'crs') \\\n    .agg(\n        rf_assemble_tile('column_index', 'row_index', 'prediction',\n            tile_size, tile_size, CellType.int8())\n)\nNext we will write the output to a GeoTiff file. Doing so in this case works quickly and well for a few specific reasons that may not hold in all cases. We can write the data at full resolution, by omitting the raster_dimensions argument, because we know the input raster dimensions are small. Also, the data is all in a single CRS, as we demonstrated above. Because the catalog_df is only a single row, we know the output GeoTIFF value at a given location corresponds to a single input. Finally, the retiled DataFrame only has a single Tile column, so the band interpretation is trivial.\nimport rasterio\noutput_tif = 'unsupervised.tif'\n\nretiled.write.geotiff(output_tif, crs=crs)\n\nwith rasterio.open(output_tif) as src:\n    for b in range(1, src.count + 1):\n        print(\"Tags on band\", b, src.tags(b))\n    display(src)\nTags on band 1 {'RF_COL': 'prediction'}\n<open DatasetReader name='unsupervised.tif' mode='r'>","title":"Visualize Prediction"},{"location":"/supervised-learning.html","text":"","title":"Supervised Machine Learning"},{"location":"/supervised-learning.html#supervised-machine-learning","text":"In this example we will demonstrate how to fit and score a supervised learning model with a sample of Sentinel-2 data and hand-drawn vector labels over different land cover types.","title":"Supervised Machine Learning"},{"location":"/supervised-learning.html#create-and-read-raster-catalog","text":"The first step is to create a Spark DataFrame containing our imagery data. To achieve that we will create a catalog DataFrame. In the catalog, each row represents a distinct area and time, and each column is the URI to a band’s image product. In this example our catalog just has one row. After reading the catalog, the resulting Spark DataFrame may have many rows per URI, with a column corresponding to each band.\nThe imagery for feature data will come from eleven bands of 60 meter resolution Sentinel-2 imagery. We also will use the scene classification (SCL) data to identify high quality, non-cloudy pixels.\nuri_base = 'https://rasterframes.s3.amazonaws.com/samples/luray_snp/{}.tif'\nbands = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B09', 'B11', 'B12']\ncols = ['SCL'] + bands\n\ncatalog_df = pd.DataFrame([\n    {b: uri_base.format(b) for b in cols}\n])\n\ntile_size = 256\ndf = spark.read.raster(catalog_df, catalog_col_names=cols, tile_dimensions=(tile_size, tile_size)) \\\n\t\t\t\t\t  .repartition(100)\n\ndf = df.select(\n    rf_crs(df.B01).alias('crs'),\n    rf_extent(df.B01).alias('extent'),\n    rf_tile(df.SCL).alias('scl'),\n    rf_tile(df.B01).alias('B01'),\n    rf_tile(df.B02).alias('B02'),\n    rf_tile(df.B03).alias('B03'),\n    rf_tile(df.B04).alias('B04'),\n    rf_tile(df.B05).alias('B05'),\n    rf_tile(df.B06).alias('B06'),\n    rf_tile(df.B07).alias('B07'),\n    rf_tile(df.B08).alias('B08'),\n    rf_tile(df.B09).alias('B09'),\n    rf_tile(df.B11).alias('B11'),\n    rf_tile(df.B12).alias('B12'),\n)\ndf.printSchema()\nroot\n |-- crs: struct (nullable = true)\n |    |-- crsProj4: string (nullable = false)\n |-- extent: struct (nullable = true)\n |    |-- xmin: double (nullable = false)\n |    |-- ymin: double (nullable = false)\n |    |-- xmax: double (nullable = false)\n |    |-- ymax: double (nullable = false)\n |-- scl: tile (nullable = true)\n |-- B01: tile (nullable = true)\n |-- B02: tile (nullable = true)\n |-- B03: tile (nullable = true)\n |-- B04: tile (nullable = true)\n |-- B05: tile (nullable = true)\n |-- B06: tile (nullable = true)\n |-- B07: tile (nullable = true)\n |-- B08: tile (nullable = true)\n |-- B09: tile (nullable = true)\n |-- B11: tile (nullable = true)\n |-- B12: tile (nullable = true)","title":"Create and Read Raster Catalog"},{"location":"/supervised-learning.html#data-prep","text":"","title":"Data Prep"},{"location":"/supervised-learning.html#label-data","text":"The land classification labels are based on a small set of hand drawn polygons in the GeoJSON file here. The property id indicates the type of land cover in each area. For these integer values, 1 is forest, 2 is cropland, and 3 is developed areas.\nWe will create a very small Spark DataFrame of the label shapes and then join it to the raster DataFrame. Such joins are typically expensive, but in this case both datasets are quite small. To speed up the join for the small vector DataFrame, we put the broadcast hint on it, which will tell Spark to put a copy of it on each Spark executor.\nAfter the raster and vector data are joined, we will convert the vector shapes into tiles using the rf_rasterize function. This procedure is sometimes called “burning in” a geometry into a raster. The values in the resulting tile cells are the id property of the GeoJSON, which we will use as labels in our supervised learning task. In areas where the geometry does not intersect, the cells will contain NoData.\ncrses = df.select('crs.crsProj4').distinct().collect()\nprint('Found ', len(crses), 'distinct CRS.')\ncrs = crses[0][0]\n\nfrom pyspark import SparkFiles\nspark.sparkContext.addFile('https://rasterframes.s3.amazonaws.com/samples/luray_snp/luray-labels.geojson') \n\nlabel_df = spark.read.geojson(SparkFiles.get('luray-labels.geojson')) \\\n    .select('id', st_reproject('geometry', lit('EPSG:4326'), lit(crs)).alias('geometry')) \\\n    .hint('broadcast')\n\ndf_joined = df.join(label_df, st_intersects(st_geometry('extent'), 'geometry')) \\\n    .withColumn('dims', rf_dimensions('B01'))\n\ndf_labeled = df_joined.withColumn('label', \n   rf_rasterize('geometry', st_geometry('extent'), 'id', 'dims.cols', 'dims.rows')\n)\nFound  1 distinct CRS.","title":"Label Data"},{"location":"/supervised-learning.html#masking-poor-quality-cells","text":"To filter only for good quality pixels, we follow roughly the same procedure as demonstrated in the quality masking section of the chapter on NoData. Instead of actually setting NoData values in the unwanted cells of any of the imagery bands, we will just filter out the mask cell values later in the process.\nfrom pyspark.sql.functions import lit\n\ndf_labeled = df_labeled \\\n    .withColumn('mask', rf_local_is_in('scl', [0, 1, 8, 9, 10]))\n\n# at this point the mask contains 0 for good cells and 1 for defect, etc\n# convert cell type and set value 1 to NoData\ndf_mask = df_labeled.withColumn('mask',\n  rf_with_no_data(rf_convert_cell_type('mask', 'uint8'), 1.0)\n)\n\ndf_mask.printSchema()\nroot\n |-- crs: struct (nullable = true)\n |    |-- crsProj4: string (nullable = false)\n |-- extent: struct (nullable = true)\n |    |-- xmin: double (nullable = false)\n |    |-- ymin: double (nullable = false)\n |    |-- xmax: double (nullable = false)\n |    |-- ymax: double (nullable = false)\n |-- scl: tile (nullable = true)\n |-- B01: tile (nullable = true)\n |-- B02: tile (nullable = true)\n |-- B03: tile (nullable = true)\n |-- B04: tile (nullable = true)\n |-- B05: tile (nullable = true)\n |-- B06: tile (nullable = true)\n |-- B07: tile (nullable = true)\n |-- B08: tile (nullable = true)\n |-- B09: tile (nullable = true)\n |-- B11: tile (nullable = true)\n |-- B12: tile (nullable = true)\n |-- id: long (nullable = true)\n |-- geometry: geometry (nullable = true)\n |-- dims: struct (nullable = true)\n |    |-- cols: integer (nullable = false)\n |    |-- rows: integer (nullable = false)\n |-- label: tile (nullable = true)\n |-- mask: tile (nullable = true)","title":"Masking Poor Quality Cells"},{"location":"/supervised-learning.html#create-ml-pipeline","text":"We import various Spark components that we need to construct our Pipeline. These are the objects that will work in sequence to conduct the data preparation and modeling.\nfrom pyrasterframes import TileExploder\nfrom pyrasterframes.rf_types import NoDataFilter\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\nSparkML requires that each observation be in its own row, and those observations be packed into a single Vector object. The first step is to “explode” the tiles into a single row per cell or pixel with the TileExploder (see also rf_explode_tiles). If a tile cell contains a NoData it will become a null value after the exploder stage. Then we use the NoDataFilter to filter out any rows that missing or null values, which will cause an error during training. Finally we use the SparkML VectorAssembler to create that Vector.\nRecall above we set undesirable pixels to NoData, so the NoDataFilter will remove them at this stage. We apply the filter to the mask column and the label column, the latter being used during training. When it is time to score the model, the pipeline will ignore the fact that there is no label column on the input DataFrame.\nexploder = TileExploder()\n\nnoDataFilter = NoDataFilter() \\\n  .setInputCols(['label', 'mask'])\n\nassembler = VectorAssembler() \\\n  .setInputCols(bands) \\\n  .setOutputCol(\"features\")\nWe are going to use a decision tree for classification. You can swap out one of the other multi-class classification algorithms if you like. With the algorithm selected we can assemble our modeling pipeline.\nclassifier = DecisionTreeClassifier() \\\n  .setLabelCol('label') \\\n  .setFeaturesCol(assembler.getOutputCol())\n\npipeline = Pipeline() \\\n  .setStages([exploder, noDataFilter, assembler, classifier])\n\npipeline.getStages()\n[TileExploder_f1966143c9b6,\n NoDataFilter_4b9771867f97,\n VectorAssembler_2ace59571e4f,\n DecisionTreeClassifier_42394cbc7759]","title":"Create ML Pipeline"},{"location":"/supervised-learning.html#train-the-model","text":"The next step is to actually run each step of the Pipeline we created, including fitting the decision tree model. We filter the DataFrame for only tiles intersecting the label raster because the label shapes are relatively sparse over the imagery. It would be logically equivalent to either include or exclude thi step, but it is more efficient to filter because it will mean less data going into the pipeline.\nmodel_input = df_mask.filter(rf_tile_sum('label') > 0).cache()\nmodel = pipeline.fit(model_input)","title":"Train the Model"},{"location":"/supervised-learning.html#model-evaluation","text":"To view the model’s performance, we first call the pipeline’s transform method on the training dataset. This transformed dataset will have the model’s prediction included in each row. We next construct an evaluator and pass it the transformed dataset to easily compute the performance metric. We can also create custom metrics using a variety of DataFrame or SQL transformations.\nprediction_df = model.transform(df_mask) \\\n                       .drop(assembler.getOutputCol()).cache()\nprediction_df.printSchema()\n\neval = MulticlassClassificationEvaluator(\n    predictionCol=classifier.getPredictionCol(),\n    labelCol=classifier.getLabelCol(),\n    metricName='accuracy'\n)\n\naccuracy = eval.evaluate(prediction_df)\nprint(\"\\nAccuracy:\", accuracy)\nroot\n |-- crs: struct (nullable = true)\n |    |-- crsProj4: string (nullable = false)\n |-- extent: struct (nullable = true)\n |    |-- xmin: double (nullable = false)\n |    |-- ymin: double (nullable = false)\n |    |-- xmax: double (nullable = false)\n |    |-- ymax: double (nullable = false)\n |-- id: long (nullable = true)\n |-- geometry: geometry (nullable = true)\n |-- dims: struct (nullable = true)\n |    |-- cols: integer (nullable = false)\n |    |-- rows: integer (nullable = false)\n |-- column_index: integer (nullable = false)\n |-- row_index: integer (nullable = false)\n |-- scl: double (nullable = false)\n |-- B01: double (nullable = false)\n |-- B02: double (nullable = false)\n |-- B03: double (nullable = false)\n |-- B04: double (nullable = false)\n |-- B05: double (nullable = false)\n |-- B06: double (nullable = false)\n |-- B07: double (nullable = false)\n |-- B08: double (nullable = false)\n |-- B09: double (nullable = false)\n |-- B11: double (nullable = false)\n |-- B12: double (nullable = false)\n |-- label: double (nullable = false)\n |-- mask: double (nullable = false)\n |-- rawPrediction: vector (nullable = true)\n |-- probability: vector (nullable = true)\n |-- prediction: double (nullable = false)\n\n\nAccuracy: 0.9744700250740825\nAs an example of using the flexibility provided by DataFrames, the code below computes and displays the confusion matrix.\ncnf_mtrx = prediction_df.groupBy(classifier.getPredictionCol()) \\\n    .pivot(classifier.getLabelCol()) \\\n    .count() \\\n    .sort(classifier.getPredictionCol())\ncnf_mtrx\nprediction 1.0 2.0 3.0 1.0 6616 6 54 2.0 10 1808 115 3.0 41 110 4401","title":"Model Evaluation"},{"location":"/supervised-learning.html#visualize-prediction","text":"Because the pipeline included a TileExploder, we will recreate the tiled data structure. The explosion transformation includes metadata enabling us to recreate the tiles. See the rf_assemble_tile function documentation for more details. In this case, the pipeline is scoring on all areas, regardless of whether they intersect the label polygons. This is simply done by removing the label column, as discussed above.\nscored = model.transform(df_mask.drop('label'))\n\nretiled = scored \\\n    .groupBy('extent', 'crs') \\\n    .agg(\n        rf_assemble_tile('column_index', 'row_index', 'prediction', tile_size, tile_size).alias('prediction'),\n        rf_assemble_tile('column_index', 'row_index', 'B04', tile_size, tile_size).alias('red'),\n        rf_assemble_tile('column_index', 'row_index', 'B03', tile_size, tile_size).alias('grn'),\n        rf_assemble_tile('column_index', 'row_index', 'B02', tile_size, tile_size).alias('blu')\n    )\nretiled.printSchema()\nroot\n |-- extent: struct (nullable = true)\n |    |-- xmin: double (nullable = false)\n |    |-- ymin: double (nullable = false)\n |    |-- xmax: double (nullable = false)\n |    |-- ymax: double (nullable = false)\n |-- crs: struct (nullable = true)\n |    |-- crsProj4: string (nullable = false)\n |-- prediction: tile (nullable = true)\n |-- red: tile (nullable = true)\n |-- grn: tile (nullable = true)\n |-- blu: tile (nullable = true)\nTake a look at a sample of the resulting prediction and the corresponding area’s red-green-blue composite image. Note that because each prediction tile is rendered independently, the colors may not have the same meaning across rows.\nscaling_quantiles = retiled.agg(\n                            rf_agg_approx_quantiles('red', [0.03, 0.97]).alias('red_q'),\n                            rf_agg_approx_quantiles('grn', [0.03, 0.97]).alias('grn_q'),\n                            rf_agg_approx_quantiles('blu', [0.03, 0.97]).alias('blu_q')\n                        ).first()\n\nretiled.select(\n                rf_render_png(\n                    rf_local_clamp('red', *scaling_quantiles['red_q']).alias('red'),\n                    rf_local_clamp('grn', *scaling_quantiles['grn_q']).alias('grn'),\n                    rf_local_clamp('blu', *scaling_quantiles['blu_q']).alias('blu')\n                  ).alias('tci'),\n                rf_render_color_ramp_png('prediction', 'ClassificationBoldLandUse').alias('prediction')\n            )\nShowing only top 5 rows.\ntci prediction","title":"Visualize Prediction"},{"location":"/numpy-pandas.html","text":"","title":"NumPy and Pandas"},{"location":"/numpy-pandas.html#numpy-and-pandas","text":"In the Python Spark API, the work of distributed computing over the DataFrame is done on many executors (the Spark term for workers) inside Java virtual machines (JVM). Most calls to pyspark are passed to a Java process via the py4j library. The user can also ask for data inside the JVM to be brought over to the Python driver (the Spark term for the client application). When dealing with tiles, the driver will receive this data as a lightweight wrapper object around a NumPy ndarray. It is also possible to write lambda functions against NumPy arrays and evaluate them in the Spark DataFrame.","title":"NumPy and Pandas"},{"location":"/numpy-pandas.html#performance-considerations","text":"When working with large, distributed datasets in Spark, attention is required when invoking actions on the data. In general, transformations are lazily evaluated in Spark, meaning the code runs fast and it doesn’t move any data around. But actions cause the evaluation to happen, meaning all the lazily planned transformations are going to be computed and data is going to be processed and moved around. In general, if a pyspark function returns a DataFrame, it is probably a transformation, and if not, it is an action.\nWhen many actions are invoked, a lot of data can flow from executors to the driver. In pyspark, the data then has to move from the driver JVM to the Python process running the driver. When that happens, if there are any tiles in the data, they will be converted to a Python Tile object. In practical work with Earth observation data, the tiles are frequently 256 by 256 arrays, which may be 100kb or more each. Individually they are small, but a DataFrame can easily have dozens of such tile columns and millions of rows.\nAll of this discussion reinforces two important principles for working with Spark: understanding the cost of an action and using aggreates, summaries, or samples to manage the cost of actions.","title":"Performance Considerations"},{"location":"/numpy-pandas.html#the-tile-class","text":"In Python, tiles are represented with the rf_types.Tile class. This is a NumPy ndarray with two dimensions, along with some additional metadata allowing correct conversion to the GeoTrellis cell type.\nfrom pyrasterframes.rf_types import Tile\nimport numpy as np\n\nt = Tile(np.random.randn(4, 4))\nprint(str(t))\nTile(dimensions=[4, 4], cell_type=CellType(float64, nan), cells=\n[[-1.6976645785431788 1.438251238090458 0.4955673143482285\n  0.07987907645882293]\n [1.0223194602865753 0.12571050448043788 1.407536778526134\n  0.4216599411257001]\n [0.2354512884489902 0.8334623245039772 -0.19812995197824723\n  0.42326157852859847]\n [0.18779749791929626 0.0837735225844472 -0.34216861550958694\n  0.8240808442602573]])\nYou can access the NumPy array with the cells member of Tile.\nt.cells.shape, t.cells.nbytes\n((4, 4), 128)","title":"The Tile Class"},{"location":"/numpy-pandas.html#dataframe-topandas","text":"As discussed in the raster writing chapter, a pretty display of Pandas DataFrame containing tiles is available by importing the rf_ipython submodule. In addition, as discussed in the vector data chapter, any geometry type in the Spark DataFrame will be converted into a Shapely geometry. Taken together, we can easily get the spatial information and raster data as a NumPy array, all within a Pandas DataFrame.\nimport pyrasterframes.rf_ipython\nfrom pyspark.sql.functions import lit, col\n\ncat = spark.read.format('aws-pds-modis-catalog').load() \\\n        .filter(\n            (col('granule_id') == 'h11v04') &\n            (col('acquisition_date') > lit('2018-02-19')) &\n            (col('acquisition_date') < lit('2018-02-22'))\n        )\n\nspark_df = spark.read.raster(cat, catalog_col_names=['B01']) \\\n                .select(\n                    'acquisition_date',\n                    'granule_id',\n                    rf_tile('B01').alias('tile'),\n                    rf_geometry('B01').alias('tile_geom')\n                    )\n\npandas_df = spark_df.limit(10).toPandas()\npandas_df.iloc[0].apply(lambda v: type(v))\nacquisition_date    <class 'pandas._libs.tslibs.timestamps.Timesta...\ngranule_id                                              <class 'str'>\ntile                           <class 'pyrasterframes.rf_types.Tile'>\ntile_geom                  <class 'shapely.geometry.polygon.Polygon'>\nName: 0, dtype: object","title":"DataFrame toPandas"},{"location":"/numpy-pandas.html#user-defined-functions","text":"As we demonstrated with vector data, we can also make use of the Tile type to create user-defined functions (UDF) that can take a tile as input, return a tile as output, or both. Here is a trivial and inefficient example of doing both. A serious performance implication of user defined functions in Python is that all the executors must move the Java objects to Python, evaluate the function, and then move the Python objects back to Java. Use the many built-in functions wherever possible, and ask the community if you have an idea for a function that should be included.\nWe will demonstrate an example of creating a UDF that is logically equivalent to a built-in function. We’ll quickly show that the resulting tiles are approximately equivalent. The reason they are not exactly the same is that one is computed in Python and the other is computed in Java.\nfrom pyrasterframes.rf_types import TileUDT\nfrom pyspark.sql.functions import udf\n\n@udf(TileUDT())\ndef my_udf(t):\n    import numpy as np\n    return Tile(np.log1p(t.cells))\n\nudf_df = spark_df.limit(1).select(\n            my_udf('tile').alias('udf_result'),\n            rf_log1p('tile').alias('built_in_result')\n        ).toPandas()\n\nrow = udf_df.iloc[0]\ndiff = row['udf_result'] - row['built_in_result']\nprint(type(diff))\nnp.abs(diff.cells).max()\n<class 'pyrasterframes.rf_types.Tile'>\n4.767482870704498e-07\nWe can also inspect an image of the difference between the two tiles, which is just random noise. Both tiles have the same structure of NoData, as exhibited by the white areas.\ndiff.show(0, 100)\n<AxesSubplot:>","title":"User Defined Functions"},{"location":"/numpy-pandas.html#creating-a-spark-dataframe","text":"You can also create a Spark DataFrame with a column full of Tile objects or Shapely geomtery objects.\nThe example below will create a Pandas DataFrame with ten rows of noise tiles and random Points. We will then create a Spark DataFrame from it.\nimport pandas as pd\nfrom shapely.geometry import Point\n\npandas_df = pd.DataFrame([{\n    'tile': Tile(np.random.randn(100, 100)),\n    'geom': Point(-90 + 90 * np.random.random((2, 1)))\n    } for _ in range(10)\n])\n\nspark_df = spark.createDataFrame(pandas_df)\n\nspark_df.printSchema()\nspark_df.count()\nroot\n |-- tile: tile (nullable = true)\n |-- geom: point (nullable = true)\n10","title":"Creating a Spark DataFrame"},{"location":"/ipython.html","text":"","title":"IPython/Jupyter Extensions"},{"location":"/ipython.html#ipython-jupyter-extensions","text":"The pyrasterframes.rf_ipython module injects a number of visualization extensions into the IPython environment, enhancing visualization of DataFrames and Tiles.\nBy default, the last expression’s result in a IPython cell is passed to the IPython.display.display function. This function in turn looks for a DisplayFormatter associated with the type, which in turn converts the instance to a display-appropriate representation, based on MIME type. For example, each DisplayFormatter may plain/text version for the IPython shell, and a text/html version for a Jupyter Notebook.\nThis will be our setup for the following examples:\nfrom pyrasterframes import *\nfrom pyrasterframes.rasterfunctions import *\nfrom pyrasterframes.utils import create_rf_spark_session\nimport pyrasterframes.rf_ipython\nfrom IPython.display import display\nimport os.path\nspark = create_rf_spark_session()\ndef scene(band):\n    b = str(band).zfill(2) # converts int 2 to '02'\n    return 'https://modis-pds.s3.amazonaws.com/MCD43A4.006/11/08/2019059/' \\\n             'MCD43A4.A2019059.h11v08.006.2019072203257_B{}.TIF'.format(b)\nrf = spark.read.raster(scene(2), tile_dimensions=(256, 256))","title":"IPython/Jupyter Extensions"},{"location":"/ipython.html#tile-samples","text":"We have some convenience methods to quickly visualize tiles (see discussion of the RasterFrame schema for orientation to the concept) when inspecting a subset of the data in a Notebook.\nIn an IPython or Jupyter interpreter, a Tile object will be displayed as an image with limited metadata.\nsample_tile = rf.select(rf_tile('proj_raster').alias('tile')).first()['tile']\nsample_tile # or `display(sample_tile)`","title":"Tile Samples"},{"location":"/ipython.html#dataframe-samples","text":"Within an IPython or Jupyter interpreter, a Spark and Pandas DataFrames containing a column of tiles will be rendered as the samples discussed above. Simply import the rf_ipython submodule to enable enhanced HTML rendering of these DataFrame types.\nrf # or `display(rf)`, or `rf.display()`\nShowing only top 5 rows.\nproj_raster_path proj_raster https://modis-pds.s3.amazonaws.com/MCD43A4.006/11/08/2019059/MCD43A4.A2019059.h11v08.006.2019072203257_B02.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/11/08/2019059/MCD43A4.A2019059.h11v08.006.2019072203257_B02.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/11/08/2019059/MCD43A4.A2019059.h11v08.006.2019072203257_B02.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/11/08/2019059/MCD43A4.A2019059.h11v08.006.2019072203257_B02.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/11/08/2019059/MCD43A4.A2019059.h11v08.006.2019072203257_B02.TIF","title":"DataFrame Samples"},{"location":"/ipython.html#changing-number-of-rows","text":"By default the RasterFrame sample display renders 5 rows. Because the IPython.display.display function doesn’t pass parameters to the underlying rendering functions, we have to provide a different means of passing parameters to the rendering code. Pandas approach to this is to use global settings via set_option/get_option. We take a more functional approach and have the user invoke an explicit display method:\nrf.display(num_rows=1, truncate=True)\nShowing only top 1 rows.\nproj_raster_path proj_raster https://modis-pds.s3.amazonaws.com/MCD43...","title":"Changing Number of Rows"},{"location":"/ipython.html#pandas","text":"There is similar rendering support injected into the Pandas by the rf_ipython module, for Pandas Dataframes having Tiles in them:\n# Limit copy of data from Spark to a few tiles.\npandas_df = rf.select(rf_tile('proj_raster'), rf_extent('proj_raster')).limit(4).toPandas()\npandas_df # or `display(pandas_df)`\nrf_tile(proj_raster) rf_extent(proj_raster) 0 (-7072005.3050801195, 993342.4642358534, -6953397.249648972, 1111950.519667) 1 (-7546437.526804707, 163086.07621782666, -7427829.47137356, 281694.1316489733) 2 (-6834789.194217826, 281694.1316489733, -6716181.138786679, 400302.18708011997) 3 (-7427829.47137356, 163086.07621782666, -7309221.415942413, 281694.1316489733)","title":"Pandas"},{"location":"/ipython.html#sample-colorization","text":"RasterFrames uses the “Viridis” color ramp as the default color profile for tile column. There are other options for reasoning about how color should be applied in the results.","title":"Sample Colorization"},{"location":"/ipython.html#color-composite","text":"As shown in Writing Raster Data section section, composites can be constructed for visualization:\nfrom IPython.display import Image # For telling IPython how to interpret the PNG byte array\n# Select red, green, and blue, respectively\nthree_band_rf = spark.read.raster(source=[[scene(1), scene(4), scene(3)]])\ncomposite_rf = three_band_rf.withColumn('png',\n                    rf_render_png('proj_raster_0', 'proj_raster_1', 'proj_raster_2'))\npng_bytes = composite_rf.select('png').first()['png'] \nImage(png_bytes)","title":"Color Composite"},{"location":"/ipython.html#custom-color-ramp","text":"You can also apply a different color ramp to a single-channel Tile using the rf_render_color_ramp_png function. See the function documentation for information about the available color maps.\nrf.select(rf_render_color_ramp_png('proj_raster', 'Magma'))\nShowing only top 5 rows.\nrf_render_png(proj_raster)","title":"Custom Color Ramp"},{"location":"/languages.html","text":"","title":"Scala and SQL"},{"location":"/languages.html#scala-and-sql","text":"One of the great powers of RasterFrames is the ability to express computation in multiple programming languages. The content in this manual focuses on Python because it is the most commonly used language in data science and GIS analytics. However, Scala (the implementation language of RasterFrames) and SQL (commonly used in many domains) are also fully supported. Examples in Python can be mechanically translated into the other two languages without much difficulty once the naming conventions are understood.\nIn the sections below we will show the same example program in each language. To do so we will compute the average NDVI per month for a single tile in Tanzania.","title":"Scala and SQL"},{"location":"/languages.html#python","text":"","title":"Python"},{"location":"/languages.html#step-1-load-the-catalog","text":"modis = spark.read.format('aws-pds-modis-catalog').load()","title":"Step 1: Load the catalog"},{"location":"/languages.html#step-2-down-select-data-by-month","text":"red_nir_monthly_2017 = modis \\\n    .select(\n        col('granule_id'),\n        month('acquisition_date').alias('month'),\n        col('B01').alias('red'),\n        col('B02').alias('nir')\n    ) \\\n    .where(\n        (year('acquisition_date') == 2017) & \n        (dayofmonth('acquisition_date') == 15) & \n        (col('granule_id') == 'h21v09')\n    )\nred_nir_monthly_2017.printSchema()\nroot\n |-- granule_id: string (nullable = false)\n |-- month: integer (nullable = false)\n |-- red: string (nullable = true)\n |-- nir: string (nullable = true)","title":"Step 2: Down-select data by month"},{"location":"/languages.html#step-3-read-tiles","text":"red_nir_tiles_monthly_2017 = spark.read.raster(\n    red_nir_monthly_2017,\n    catalog_col_names=['red', 'nir'],\n    tile_dimensions=(256, 256)\n)","title":"Step 3: Read tiles"},{"location":"/languages.html#step-4-compute-aggregates","text":"result = red_nir_tiles_monthly_2017 \\\n    .where(st_intersects(\n        st_reproject(rf_geometry(col('red')), rf_crs(col('red')).crsProj4, rf_mk_crs('EPSG:4326')),\n        st_makePoint(lit(34.870605), lit(-4.729727)))\n    ) \\\n    .groupBy('month') \\\n    .agg(rf_agg_stats(rf_normalized_difference(col('nir'), col('red'))).alias('ndvi_stats')) \\\n    .orderBy(col('month')) \\\n    .select('month', 'ndvi_stats.*')\nresult\nShowing only top 5 rows.\nmonth data_cells no_data_cells min max mean variance 1 65523 13 -0.2519809825673534 0.8644836272040303 0.4062596673810191 0.01407280838385605 2 65521 15 -0.21232123212321233 0.8872390789756832 0.46738863804011127 0.011822698212885174 3 64425 1111 -0.36211340206185566 0.9208261617900172 0.5811071411891395 0.011570245465885753 4 64236 1300 -0.17252657399836469 0.922397476340694 0.5254596885897274 0.01087259231886667 5 60819 4717 -0.19951338199513383 0.916626036079961 0.46471430090039234 0.01215478443067744","title":"Step 4: Compute aggregates"},{"location":"/languages.html#sql","text":"For convenience, we’re going to evaluate SQL from the Python environment. The SQL fragments should work in the spark-sql shell just the same.\ndef sql(stmt):\n    return spark.sql(stmt)","title":"SQL"},{"location":"/languages.html#step-1-load-the-catalog","text":"sql(\"CREATE OR REPLACE TEMPORARY VIEW modis USING `aws-pds-modis-catalog`\")","title":"Step 1: Load the catalog"},{"location":"/languages.html#step-2-down-select-data-by-month","text":"sql(\"\"\"\nCREATE OR REPLACE TEMPORARY VIEW red_nir_monthly_2017 AS\nSELECT granule_id, month(acquisition_date) as month, B01 as red, B02 as nir\nFROM modis\nWHERE year(acquisition_date) = 2017 AND day(acquisition_date) = 15 AND granule_id = 'h21v09'\n\"\"\")\nsql('DESCRIBE red_nir_monthly_2017')\ncol_name data_type comment granule_id string month int red string nir string","title":"Step 2: Down-select data by month"},{"location":"/languages.html#step-3-read-tiles","text":"sql(\"\"\"\nCREATE OR REPLACE TEMPORARY VIEW red_nir_tiles_monthly_2017\nUSING raster\nOPTIONS (\n    catalog_table='red_nir_monthly_2017',\n    catalog_col_names='red,nir',\n    tile_dimensions='256,256'\n    )\n\"\"\")","title":"Step 3: Read tiles"},{"location":"/languages.html#step-4-compute-aggregates","text":"grouped = sql(\"\"\"\nSELECT month, ndvi_stats.* FROM (\n    SELECT month, rf_agg_stats(rf_normalized_difference(nir, red)) as ndvi_stats\n    FROM red_nir_tiles_monthly_2017\n    WHERE st_intersects(st_reproject(rf_geometry(red), rf_crs(red), 'EPSG:4326'), st_makePoint(34.870605, -4.729727))\n    GROUP BY month\n    ORDER BY month\n)\n\"\"\")\ngrouped\nShowing only top 5 rows.\nmonth data_cells no_data_cells min max mean variance 1 65523 13 -0.2519809825673534 0.8644836272040303 0.4062596673810191 0.01407280838385605 2 65521 15 -0.21232123212321233 0.8872390789756832 0.46738863804011127 0.011822698212885174 3 64425 1111 -0.36211340206185566 0.9208261617900172 0.5811071411891395 0.011570245465885753 4 64236 1300 -0.17252657399836469 0.922397476340694 0.5254596885897274 0.01087259231886667 5 60819 4717 -0.19951338199513383 0.916626036079961 0.46471430090039234 0.01215478443067744","title":"Step 4: Compute aggregates"},{"location":"/languages.html#scala","text":"The latest Scala API documentation is available here:\nScala API Documentation","title":"Scala"},{"location":"/languages.html#step-1-load-the-catalog","text":"import geotrellis.proj4.LatLng\nimport org.locationtech.rasterframes._\nimport org.locationtech.rasterframes.datasource.raster._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.functions._\n\n\nimplicit val spark = SparkSession.builder()\n  .master(\"local[*]\")\n  .appName(\"RasterFrames\")\n  .withKryoSerialization\n  .getOrCreate()\n  .withRasterFrames\n\nimport spark.implicits._\n\nval modis = spark.read.format(\"aws-pds-modis-catalog\").load()","title":"Step 1: Load the catalog"},{"location":"/languages.html#step-2-down-select-data-by-month","text":"val red_nir_monthly_2017 = modis\n  .select($\"granule_id\", month($\"acquisition_date\") as \"month\", $\"B01\" as \"red\", $\"B02\" as \"nir\")\n  .where(year($\"acquisition_date\") === 2017 && (dayofmonth($\"acquisition_date\") === 15) && $\"granule_id\" === \"h21v09\")","title":"Step 2: Down-select data by month"},{"location":"/languages.html#step-3-read-tiles","text":"val red_nir_tiles_monthly_2017 = spark.read.raster\n  .fromCatalog(red_nir_monthly_2017, \"red\", \"nir\")\n  .load()","title":"Step 3: Read tiles"},{"location":"/languages.html#step-4-compute-aggregates","text":"val result = red_nir_tiles_monthly_2017\n  .where(st_intersects(\n    st_reproject(rf_geometry($\"red\"), rf_crs($\"red\"), LatLng),\n    st_makePoint(34.870605, -4.729727)\n  ))\n  .groupBy(\"month\")\n  .agg(rf_agg_stats(rf_normalized_difference($\"nir\", $\"red\")) as \"ndvi_stats\")\n  .orderBy(\"month\")\n  .select(\"month\", \"ndvi_stats.*\")\n  \nresult.show()","title":"Step 4: Compute aggregates"},{"location":"/reference.html","text":"","title":"Function Reference"},{"location":"/reference.html#function-reference","text":"RasterFrames provides a rich set of columnar function for processing geospatial raster data. In Spark SQL, the functions are already registered in the SQL engine; they are usually prefixed with rf_. In Python, they are available in the pyrasterframes.rasterfunctions module.\nThe convention in this document will be to define the function signature as below, with its return type, the function name, and named arguments with their types.\nReturnDataType function_name(InputDataType argument1, InputDataType argument2)\nFor the Scala documentation on these functions, see RasterFunctions. The full Scala API documentation can be found here.\nimport pyrasterframes\nfrom pyrasterframes.utils import create_rf_spark_session\nfrom pyrasterframes.rasterfunctions import *\nfrom IPython.display import display\nimport os.path\n\nspark = create_rf_spark_session()","title":"Function Reference"},{"location":"/reference.html#list-of-available-sql-and-python-functions","text":"Vector Operations st_reproject st_extent st_geometry rf_xz2_index rf_z2_index Tile Metadata and Mutation rf_dimensions rf_cell_type rf_tile rf_extent rf_crs rf_proj_raster rf_mk_crs rf_convert_cell_type rf_interpret_cell_type_as rf_resample Tile Creation rf_make_zeros_tile rf_make_ones_tile rf_make_constant_tile rf_rasterize rf_array_to_tile rf_assemble_tile Masking and NoData rf_mask rf_mask_by_value rf_mask_by_values rf_mask_by_bit rf_mask_by_bits rf_inverse_mask rf_inverse_mask_by_value rf_is_no_data_tile rf_local_no_data rf_local_data rf_local_data rf_with_no_data Local Map Algebra rf_local_add rf_local_subtract rf_local_multiply rf_local_divide rf_normalized_difference rf_local_less rf_local_less_equal rf_local_greater rf_local_greater_equal rf_local_equal rf_local_unequal rf_local_is_in rf_local_extract_bits rf_local_min rf_local_max rf_local_clamp rf_where rf_rescale rf_standardize rf_round rf_abs rf_exp rf_exp10 rf_exp2 rf_expm1 rf_log rf_log10 rf_log2 rf_log1p rf_sqrt Tile Statistics rf_tile_sum rf_tile_mean rf_tile_min rf_tile_max rf_no_data_cells rf_data_cells rf_exists rf_for_all rf_tile_stats rf_tile_histogram Aggregate Tile Statistics rf_agg_mean rf_agg_data_cells rf_agg_no_data_cells rf_agg_stats rf_agg_approx_histogram rf_agg_approx_quantiles rf_agg_extent rf_agg_reprojected_extent Tile Local Aggregate Statistics rf_agg_local_max rf_agg_local_min rf_agg_local_mean rf_agg_local_data_cells rf_agg_local_no_data_cells rf_agg_local_stats Converting Tiles rf_explode_tiles rf_explode_tiles_sample rf_tile_to_array_int rf_tile_to_array_double rf_render_ascii rf_render_matrix rf_render_png rf_render_color_ramp_png rf_agg_overview_raster rf_rgb_composite\nTo import RasterFrames functions into the environment, import from pyrasterframes.rasterfunctions.\nfrom pyrasterframes.rasterfunctions import *\nFunctions starting with rf_, which are for raster, and st_, which are for vector geometry, become available for use with DataFrames. You can view all of the available functions with the following.\n[fn for fn in dir() if fn.startswith('rf_') or fn.startswith('st_')]","title":"List of Available SQL and Python Functions"},{"location":"/reference.html#vector-operations","text":"Various LocationTech GeoMesa user-defined functions (UDFs) dealing with geomtery type columns are provided in the SQL engine and within the pyrasterframes.rasterfunctions Python module. These are documented in the LocationTech GeoMesa Spark SQL documentation. These functions are all prefixed with st_.\nRasterFrames provides some additional functions for vector geometry operations.","title":"Vector Operations"},{"location":"/reference.html#st-reproject","text":"Geometry st_reproject(Geometry geom, String origin_crs, String destination_crs)\nReproject the vector geom from origin_crs to destination_crs. Both _crs arguments are either proj4 strings, EPSG codes or OGC WKT for coordinate reference systems.","title":"st_reproject"},{"location":"/reference.html#st-extent","text":"Struct[Double xmin, Double xmax, Double ymin, Double ymax] st_extent(Geometry geom)\nExtracts the bounding box (extent/envelope) of the geometry.\nSee also GeoMesa st_envelope which returns a Geometry type.","title":"st_extent"},{"location":"/reference.html#st-geometry","text":"Geometry st_geometry(Struct[Double xmin, Double xmax, Double ymin, Double ymax] extent)\nConvert an extent to a Geometry. The extent likely comes from st_extent or rf_extent.","title":"st_geometry"},{"location":"/reference.html#rf-xz2-index","text":"Long rf_xz2_index(Geometry geom, CRS crs)\nLong rf_xz2_index(Extent extent, CRS crs)\nLong rf_xz2_index(ProjectedRasterTile proj_raster)\nConstructs a XZ2 index in WGS84/EPSG:4326 from either a Geometry, Extent, ProjectedRasterTile and its CRS. This function is useful for range partitioning.","title":"rf_xz2_index"},{"location":"/reference.html#rf-z2-index","text":"Long rf_z2_index(Geometry geom, CRS crs)\nLong rf_z2_index(Extent extent, CRS crs)\nLong rf_z2_index(ProjectedRasterTile proj_raster)\nConstructs a Z2 index in WGS84/EPSG:4326 from either a Geometry, Extent, ProjectedRasterTile and its CRS. First the native extent is extracted or computed, and then center is used as the indexing location. This function is useful for range partitioning. See Reading Raster Data section for details on how to have an index automatically added when reading raster data.","title":"rf_z2_index"},{"location":"/reference.html#tile-metadata-and-mutation","text":"Functions to access and change the particulars of a tile: its shape and the data type of its cells. See section on “NoData” handling for additional discussion of cell types.","title":"Tile Metadata and Mutation"},{"location":"/reference.html#rf-dimensions","text":"Struct[Int, Int] rf_dimensions(Tile tile)\nGet number of columns and rows in the tile, as a Struct of cols and rows.","title":"rf_dimensions"},{"location":"/reference.html#rf-cell-type","text":"Struct[String] rf_cell_type(Tile tile)\nGet the cell type of the tile. The cell type can be changed with rf_convert_cell_type.","title":"rf_cell_type"},{"location":"/reference.html#rf-tile","text":"Tile rf_tile(ProjectedRasterTile proj_raster)\nGet the fully realized (non-lazy) tile from a ProjectedRasterTile struct column.","title":"rf_tile"},{"location":"/reference.html#rf-extent","text":"Struct[Double xmin, Double xmax, Double ymin, Double ymax] rf_extent(ProjectedRasterTile proj_raster)\nStruct[Double xmin, Double xmax, Double ymin, Double ymax] rf_extent(RasterSource proj_raster)\nFetches the extent (bounding box or envelope) of a ProjectedRasterTile or RasterSource type tile columns.","title":"rf_extent"},{"location":"/reference.html#rf-crs","text":"Struct rf_crs(ProjectedRasterTile proj_raster)\nStruct rf_crs(RasterSource proj_raster)\nStruct rf_crs(String crs_spec)\nFetch CRS structure representing the coordinate reference system of a ProjectedRasterTile or RasterSource type tile columns, or from a column of strings in the form supported by rf_mk_crs.","title":"rf_crs"},{"location":"/reference.html#rf-proj-raster","text":"ProjectedRasterTile rf_proj_raster(Tile tile, Extent extent, CRS crs)\nConstruct a proj_raster structure from individual Tile, Extent, and CRS columns.","title":"rf_proj_raster"},{"location":"/reference.html#rf-mk-crs","text":"Struct rf_mk_crs(String crsText)\nConstruct a CRS structure from one of its string representations. Three forms are supported:\nEPSG code: EPSG:<integer> Proj4 string: +proj <proj4 parameters> WKT String with embedded EPSG code: GEOGCS[\"<name>\", <datum>, <prime meridian>, <angular unit> {,<twin axes>} {,<authority>}]\nExample: SELECT rf_mk_crs('EPSG:4326')","title":"rf_mk_crs"},{"location":"/reference.html#rf-convert-cell-type","text":"Tile rf_convert_cell_type(Tile tile_col, CellType cell_type)\nTile rf_convert_cell_type(Tile tile_col, String cell_type)\nConvert tile_col to a different cell type. In Python you can pass a CellType object to cell_type.","title":"rf_convert_cell_type"},{"location":"/reference.html#rf-interpret-cell-type-as","text":"Tile rf_interpret_cell_type_as(Tile tile_col, CellType cell_type)\nTile rf_interpret_cell_type_as(Tile tile_col, String cell_type)\nChange the interpretation of the tile_col’s cell values according to specified cell_type. In Python you can pass a CellType object to cell_type.","title":"rf_interpret_cell_type_as"},{"location":"/reference.html#rf-resample","text":"Tile rf_resample(Tile tile, Double factor, [String method])\nTile rf_resample(Tile tile, Int factor, [String method])\nTile rf_resample(Tile tile, Tile shape_tile, [String method])\nIn SQL, three parameters are required for rf_resample.:\nTile rf_resample(Tile tile, Double factor, String method)\nTile rf_resample(Tile tile, Int factor, String method)\nTile rf_resample(Tile tile, Tile shape_tile, String method)\nTile rf_resample_nearest(Tile tile, Double factor)\nTile rf_resample_nearest(Tile tile, Int factor)\nTile rf_resample_nearest(Tile tile, Tile shape_tile)\nChange the tile dimension by upsampling or downsampling. Passing a numeric factor will scale the number of columns and rows in the tile: 1.0 is the same number of columns and row; less than one downsamples the tile; and greater than one upsamples the tile. Passing a tile as the second argument resamples such that the output has the same dimension (number of columns and rows) as shape_tile.\nThere are two categories: point resampling methods and aggregating resampling methods. Resampling method to use can be specified by one of the following strings, possibly in a column. The point resampling methods are: \"nearest_neighbor\", \"bilinear\", \"cubic_convolution\", \"cubic_spline\", and \"lanczos\". The aggregating resampling methods are: \"average\", \"mode\", \"median\", \"max\", “min”, or \"sum\".\nNote the aggregating methods are intended for downsampling. For example a 0.25 factor and max method returns the maximum value in a 4x4 neighborhood.\nIf tile has an integer CellType, the returned tile will be coerced to a floating point with the following methods: bilinear, cubic_convolution, cubic_spline, lanczos, average, and median.","title":"rf_resample"},{"location":"/reference.html#tile-creation","text":"Functions to create a new Tile column, either from scratch or from existing data not yet in a tile.","title":"Tile Creation"},{"location":"/reference.html#rf-make-zeros-tile","text":"Tile rf_make_zeros_tile(Int tile_columns, Int tile_rows, [CellType cell_type])\nTile rf_make_zeros_tile(Int tile_columns, Int tile_rows, [String cell_type_name])\nCreate a tile of shape tile_columns by tile_rows full of zeros, with the optional cell type; default is float64. See this discussion on cell types for info on the cell_type argument. All arguments are literal values and not column expressions.","title":"rf_make_zeros_tile"},{"location":"/reference.html#rf-make-ones-tile","text":"Tile rf_make_ones_tile(Int tile_columns, Int tile_rows, [CellType cell_type])\nTile rf_make_ones_tile(Int tile_columns, Int tile_rows, [String cell_type_name])\nCreate a tile of shape tile_columns by tile_rows full of ones, with the optional cell type; default is float64. See this discussion on cell types for info on the cell_type argument. All arguments are literal values and not column expressions.","title":"rf_make_ones_tile"},{"location":"/reference.html#rf-make-constant-tile","text":"Tile rf_make_constant_tile(Numeric constant, Int tile_columns, Int tile_rows,  [CellType cell_type])\nTile rf_make_constant_tile(Numeric constant, Int tile_columns, Int tile_rows,  [String cell_type_name])\nCreate a tile of shape tile_columns by tile_rows full of constant, with the optional cell type; default is float64. See this discussion on cell types for info on the cell_type argument. All arguments are literal values and not column expressions.","title":"rf_make_constant_tile"},{"location":"/reference.html#rf-rasterize","text":"Tile rf_rasterize(Geometry geom, Geometry tile_bounds, Int value, Int tile_columns, Int tile_rows)\nConvert a vector Geometry geom into a Tile representation. The value will be “burned-in” to the returned tile where the geom intersects the tile_bounds. Returned tile will have shape tile_columns by tile_rows. Values outside the geom will be assigned a NoData value. Returned tile has cell type int32, note that value is of type Int.\nParameters tile_columns and tile_rows are literals, not column expressions. The others are column expressions.","title":"rf_rasterize"},{"location":"/reference.html#rf-array-to-tile","text":"Tile rf_array_to_tile(Array arrayCol, Int numCols, Int numRows)\nPython only. Create a tile from a Spark SQL Array, filling values in row-major order.","title":"rf_array_to_tile"},{"location":"/reference.html#rf-assemble-tile","text":"Tile rf_assemble_tile(Int colIndex, Int rowIndex, Numeric cellData, Int numCols, Int numRows, [CellType cell_type])\nTile rf_assemble_tile(Int colIndex, Int rowIndex, Numeric cellData, Int numCols, Int numRows, [String cell_type_name])\nSQL: Tile rf_assemble_tile(Int colIndex, Int rowIndex, Numeric cellData, Int numCols, Int numRows)\nCreate tiles of dimension numCols by numRows from a column of cell data with location indices. This function is the inverse of rf_explode_tiles. Intended use is with a groupby, producing one row with a new tile per group. In Python, the numCols, numRows and cellType arguments are literal values, others are column expressions. See this discussion on cell types for info on the optional cell_type argument. The default is float64. SQL implementation does not accept a cell_type argument. It returns a float64 cell type tile by default.","title":"rf_assemble_tile"},{"location":"/reference.html#masking-and-nodata","text":"See the masking page for conceptual discussion of masking operations.\nThere are statistical functions of the count of data and NoData values per tile and aggregate over a tile column: rf_data_cells, rf_no_data_cells, rf_agg_data_cells, and rf_agg_no_data_cells.\nMasking is a raster operation that sets specific cells to NoData based on the values in another raster.","title":"Masking and NoData"},{"location":"/reference.html#rf-mask","text":"Tile rf_mask(Tile tile, Tile mask, bool inverse)\nWhere the mask contains NoData, replace values in the tile with NoData.\nReturned tile cell type will be coerced to one supporting NoData if it does not already.\ninverse is a literal not a Column. If inverse is true, return the tile with NoData in locations where the mask does not contain NoData. Equivalent to rf_inverse_mask.\nSee also rf_rasterize.","title":"rf_mask"},{"location":"/reference.html#rf-mask-by-value","text":"Tile rf_mask_by_value(Tile data_tile, Tile mask_tile, Int mask_value, bool inverse)\nGenerate a tile with the values from data_tile, with NoData in cells where the mask_tile is equal to mask_value.\ninverse is a literal not a Column. If inverse is true, return the data_tile with NoData in locations where the mask_tile value is not equal to mask_value. Equivalent to rf_inverse_mask_by_value.","title":"rf_mask_by_value"},{"location":"/reference.html#rf-mask-by-values","text":"Tile rf_mask_by_values(Tile data_tile, Tile mask_tile, Array mask_values)\nTile rf_mask_by_values(Tile data_tile, Tile mask_tile, list mask_values)\nGenerate a tile with the values from data_tile, with NoData in cells where the mask_tile is in the mask_values Array or list. mask_values can be a pyspark.sql.ArrayType or a list.","title":"rf_mask_by_values"},{"location":"/reference.html#rf-mask-by-bit","text":"Tile rf_mask_by_bits(Tile tile, Tile mask_tile, Int bit_position, Bool mask_value)\nApplies a mask using bit values in the mask_tile. Working from the right, the bit at bit_position is extracted from cell values of the mask_tile. In all locations where these are equal to the mask_value, the returned tile is set to NoData; otherwise the original tile cell value is returned.\nThis is a single-bit version of rf_mask_by_bits.","title":"rf_mask_by_bit"},{"location":"/reference.html#rf-mask-by-bits","text":"Tile rf_mask_by_bits(Tile tile, Tile mask_tile, Int start_bit, Int num_bits, Array mask_values) \nTile rf_mask_by_bits(Tile tile, Tile mask_tile, Int start_bit, Int num_bits, list mask_values)\nApplies a mask from blacklisted bit values in the mask_tile. Working from the right, the bits from start_bit to start_bit + num_bits are extracted from cell values of the mask_tile. In all locations where these are in the mask_values, the returned tile is set to NoData; otherwise the original tile cell value is returned.\nThis function is not available in the SQL API. The below is equivalent:\nSELECT rf_mask_by_values(\n            tile, \n            rf_local_extract_bits(mask_tile, start_bit, num_bits), \n            mask_values\n            ),","title":"rf_mask_by_bits"},{"location":"/reference.html#rf-inverse-mask","text":"Tile rf_inverse_mask(Tile tile, Tile mask)\nWhere the mask does not contain NoData, replace values in tile with NoData.","title":"rf_inverse_mask"},{"location":"/reference.html#rf-inverse-mask-by-value","text":"Tile rf_inverse_mask_by_value(Tile data_tile, Tile mask_tile, Int mask_value)\nGenerate a tile with the values from data_tile, with NoData in cells where the mask_tile is not equal to mask_value. In other words, only keep data_tile cells in locations where the mask_tile is equal to mask_value.","title":"rf_inverse_mask_by_value"},{"location":"/reference.html#rf-is-no-data-tile","text":"Boolean rf_is_no_data_tile(Tile)\nReturns true if tile contains only NoData. By definition returns false if cell type does not support NoData. To count NoData cells or data cells, see rf_no_data_cells, rf_data_cells, rf_agg_no_data_cells, rf_agg_data_cells, rf_agg_local_no_data_cells, and rf_agg_local_data_cells. This function is distinguished from rf_for_all, which tests that values are not NoData and nonzero.","title":"rf_is_no_data_tile"},{"location":"/reference.html#rf-local-no-data","text":"Tile rf_local_no_data(Tile tile)\nReturns a tile with values of 1 in each cell where the input tile contains NoData. Otherwise values are 0.","title":"rf_local_no_data"},{"location":"/reference.html#rf-local-data","text":"Tile rf_local_no_data(Tile tile)\nReturns a tile with values of 0 in each cell where the input tile contains NoData. Otherwise values are 1.","title":"rf_local_data"},{"location":"/reference.html#rf-local-data","text":"","title":"rf_local_data"},{"location":"/reference.html#rf-with-no-data","text":"Tile rf_with_no_data(Tile tile, Double no_data_value)\nPython only. Return a tile column marking as NoData all cells equal to no_data_value.\nThe no_data_value argument is a literal Double, not a Column expression.\nIf input tile had a NoData value already, the behaviour depends on if its cell type is floating point or not. For floating point cell type tile, NoData values on the input tile remain NoData values on the output. For integral cell type tiles, the previous NoData values become literal values.","title":"rf_with_no_data"},{"location":"/reference.html#local-map-algebra","text":"Local map algebra raster operations are element-wise operations on a single tile (unary), between a tile and a scalar, between two tiles, or across many tiles.\nWhen these operations encounter a NoData value in either operand, the cell in the resulting tile will have a NoData.\nThe binary local map algebra functions have similar variations in the Python API depending on the left hand side type:\nrf_local_op: applies op to two columns; the right hand side can be a tile or a numeric column. rf_local_op_double: applies op to a tile and a literal scalar, coercing the tile to a floating point type rf_local_op_int: applies op to a tile and a literal scalar, without coercing the tile to a floating point type\nThe SQL API does not require the rf_local_op_double or rf_local_op_int forms (just rf_local_op).\nLocal map algebra operations for more than two tiles are implemented to work across rows in the DataFrame. As such, they are aggregate functions.","title":"Local Map Algebra"},{"location":"/reference.html#rf-local-add","text":"Tile rf_local_add(Tile tile1, Tile rhs)\nTile rf_local_add_int(Tile tile1, Int rhs)\nTile rf_local_add_double(Tile tile1, Double rhs)\nReturns a tile column containing the element-wise sum of tile1 and rhs.","title":"rf_local_add"},{"location":"/reference.html#rf-local-subtract","text":"Tile rf_local_subtract(Tile tile1, Tile rhs)\nTile rf_local_subtract_int(Tile tile1, Int rhs)\nTile rf_local_subtract_double(Tile tile1, Double rhs)\nReturns a tile column containing the element-wise difference of tile1 and rhs.","title":"rf_local_subtract"},{"location":"/reference.html#rf-local-multiply","text":"Tile rf_local_multiply(Tile tile1, Tile rhs)\nTile rf_local_multiply_int(Tile tile1, Int rhs)\nTile rf_local_multiply_double(Tile tile1, Double rhs)\nReturns a tile column containing the element-wise product of tile1 and rhs. This is not the matrix multiplication of tile1 and rhs.","title":"rf_local_multiply"},{"location":"/reference.html#rf-local-divide","text":"Tile rf_local_divide(Tile tile1, Tile rhs)\nTile rf_local_divide_int(Tile tile1, Int rhs)\nTile rf_local_divide_double(Tile tile1, Double rhs)\nReturns a tile column containing the element-wise quotient of tile1 and rhs.","title":"rf_local_divide"},{"location":"/reference.html#rf-normalized-difference","text":"Tile rf_normalized_difference(Tile tile1, Tile tile2)\nCompute the normalized difference of the the two tiles: (tile1 - tile2) / (tile1 + tile2). Result is always floating point cell type. This function has no scalar variant.","title":"rf_normalized_difference"},{"location":"/reference.html#rf-local-less","text":"Tile rf_local_less(Tile tile1, Tile rhs)\nTile rf_local_less_int(Tile tile1, Int rhs)\nTile rf_local_less_double(Tile tile1, Double rhs)\nReturns a tile column containing the element-wise evaluation of tile1 is less than rhs.","title":"rf_local_less"},{"location":"/reference.html#rf-local-less-equal","text":"Tile rf_local_less_equal(Tile tile1, Tile rhs)\nTile rf_local_less_equal_int(Tile tile1, Int rhs)\nTile rf_local_less_equal_double(Tile tile1, Double rhs)\nReturns a tile column containing the element-wise evaluation of tile1 is less than or equal to rhs.","title":"rf_local_less_equal"},{"location":"/reference.html#rf-local-greater","text":"Tile rf_local_greater(Tile tile1, Tile rhs)\nTile rf_local_greater_int(Tile tile1, Int rhs)\nTile rf_local_greater_double(Tile tile1, Double rhs)\nReturns a tile column containing the element-wise evaluation of tile1 is greater than rhs.","title":"rf_local_greater"},{"location":"/reference.html#rf-local-greater-equal","text":"Tile rf_local_greater_equal(Tile tile1, Tile rhs)\nTile rf_local_greater_equal_int(Tile tile1, Int rhs)\nTile rf_local_greater_equal_double(Tile tile1, Double rhs)\nReturns a tile column containing the element-wise evaluation of tile1 is greater than or equal to rhs.","title":"rf_local_greater_equal"},{"location":"/reference.html#rf-local-equal","text":"Tile rf_local_equal(Tile tile1, Tile rhs)\nTile rf_local_equal_int(Tile tile1, Int rhs)\nTile rf_local_equal_double(Tile tile1, Double rhs)\nReturns a tile column containing the element-wise equality of tile1 and rhs.","title":"rf_local_equal"},{"location":"/reference.html#rf-local-unequal","text":"Tile rf_local_unequal(Tile tile1, Tile rhs)\nTile rf_local_unequal_int(Tile tile1, Int rhs)\nTile rf_local_unequal_double(Tile tile1, Double rhs)\nReturns a tile column containing the element-wise inequality of tile1 and rhs.","title":"rf_local_unequal"},{"location":"/reference.html#rf-local-is-in","text":"Tile rf_local_is_in(Tile tile, Array array)\nTile rf_local_is_in(Tile tile, list l)\nReturns a tile column with cell values of 1 where the tile cell value is in the provided array or list. The array is a Spark SQL Array. A python list of numeric values can also be passed.","title":"rf_local_is_in"},{"location":"/reference.html#rf-local-extract-bits","text":"Tile rf_local_extract_bits(Tile tile, Int start_bit, Int num_bits)\nTile rf_local_extract_bits(Tile tile, Int start_bit)\nExtract value from specified bits of the cells’ underlying binary data. Working from the right, the bits from start_bit to start_bit + num_bits are extracted from cell values of the tile. The start_bit is zero indexed. If num_bits is not provided, a single bit is extracted.\nA common use case for this function is covered by rf_mask_by_bits.","title":"rf_local_extract_bits"},{"location":"/reference.html#rf-local-min","text":"Tile rf_local_min(Tile tile, Tile max)\nTile rf_local_min(Tile tile, Numeric max)\nPerforms cell-wise minimum two tiles or a tile and a scalar.","title":"rf_local_min"},{"location":"/reference.html#rf-local-max","text":"Tile rf_local_max(Tile tile, Tile max)\nTile rf_local_max(Tile tile, Numeric max)\nPerforms cell-wise maximum two tiles or a tile and a scalar.","title":"rf_local_max"},{"location":"/reference.html#rf-local-clamp","text":"Tile rf_local_clamp(Tile tile, Tile min, Tile max)\nTile rf_local_clamp(Tile tile, Numeric min, Tile max)\nTile rf_local_clamp(Tile tile, Tile min, Numeric max)\nTile rf_local_clamp(Tile tile, Numeric min, Numeric max)\nReturn the tile with its values limited to a range defined by min and max, inclusive.","title":"rf_local_clamp"},{"location":"/reference.html#rf-where","text":"Tile rf_where(Tile condition, Tile x, Tile y)\nReturn a tile with cell values chosen from x or y depending on condition. Operates cell-wise in a similar fashion to Spark SQL when and otherwise.","title":"rf_where"},{"location":"/reference.html#rf-rescale","text":"Tile rf_rescale(Tile tile)\nTile rf_rescale(Tile tile, Double min, Double max)\nRescale cell values such that the minimum is zero and the maximum is one. Other values will be linearly interpolated into the range. If specified, the min parameter will become the zero value and the max parameter will become 1. See rf_agg_stats. Values outside the range will be set to 0 or 1. If min and max are not specified, the tile-wise minimum and maximum are used; this can result in inconsistent values across rows in a tile column.","title":"rf_rescale"},{"location":"/reference.html#rf-standardize","text":"rf_standardize(Tile tile)\nrf_standardize(Tile tile, Double mean, Double stddev)\nStandardize cell values such that the mean is zero and the standard deviation is one. If specified, the mean and stddev are applied to all tiles in the column. See rf_agg_stats. If not specified, each tile will be standardized according to the statistics of its cell values; this can result in inconsistent values across rows in a tile column.","title":"rf_standardize"},{"location":"/reference.html#rf-round","text":"Tile rf_round(Tile tile)\nRound cell values to the nearest integer without changing the cell type.","title":"rf_round"},{"location":"/reference.html#rf-abs","text":"Tile rf_abs(Tile tile)\nCompute the absolute value of cell value.","title":"rf_abs"},{"location":"/reference.html#rf-exp","text":"Tile rf_exp(Tile tile)\nPerforms cell-wise exponential.","title":"rf_exp"},{"location":"/reference.html#rf-exp10","text":"Tile rf_exp10(Tile tile)\nCompute 10 to the power of cell values.","title":"rf_exp10"},{"location":"/reference.html#rf-exp2","text":"Tile rf_exp2(Tile tile)\nCompute 2 to the power of cell values.","title":"rf_exp2"},{"location":"/reference.html#rf-expm1","text":"Tile rf_expm1(Tile tile)\nPerforms cell-wise exponential, then subtract one. Inverse of log1p.","title":"rf_expm1"},{"location":"/reference.html#rf-log","text":"Tile rf_log(Tile tile)\nPerforms cell-wise natural logarithm.","title":"rf_log"},{"location":"/reference.html#rf-log10","text":"Tile rf_log10(Tile tile)\nPerforms cell-wise logarithm with base 10.","title":"rf_log10"},{"location":"/reference.html#rf-log2","text":"Tile rf_log2(Tile tile)\nPerforms cell-wise logarithm with base 2.","title":"rf_log2"},{"location":"/reference.html#rf-log1p","text":"Tile rf_log1p(Tile tile)\nPerforms natural logarithm of cell values plus one. Inverse of rf_expm1.","title":"rf_log1p"},{"location":"/reference.html#rf-sqrt","text":"Tile rf_sqrt(Tile tile)\nPerform cell-wise square root.","title":"rf_sqrt"},{"location":"/reference.html#tile-statistics","text":"The following functions compute a statistical summary per row of a tile column. The statistics are computed across the cells of a single tile, within each DataFrame Row.","title":"Tile Statistics"},{"location":"/reference.html#rf-tile-sum","text":"Double rf_tile_sum(Tile tile)\nComputes the sum of cells in each row of column tile, ignoring NoData values.","title":"rf_tile_sum"},{"location":"/reference.html#rf-tile-mean","text":"Double rf_tile_mean(Tile tile)\nComputes the mean of cells in each row of column tile, ignoring NoData values.","title":"rf_tile_mean"},{"location":"/reference.html#rf-tile-min","text":"Double rf_tile_min(Tile tile)\nComputes the min of cells in each row of column tile, ignoring NoData values.","title":"rf_tile_min"},{"location":"/reference.html#rf-tile-max","text":"Double rf_tile_max(Tile tile)\nComputes the max of cells in each row of column tile, ignoring NoData values.","title":"rf_tile_max"},{"location":"/reference.html#rf-no-data-cells","text":"Long rf_no_data_cells(Tile tile)\nReturn the count of NoData cells in the tile.","title":"rf_no_data_cells"},{"location":"/reference.html#rf-data-cells","text":"Long rf_data_cells(Tile tile)\nReturn the count of data cells in the tile.","title":"rf_data_cells"},{"location":"/reference.html#rf-exists","text":"Boolean rf_exists(Tile tile)\nReturns true if any cells in the tile are true (non-zero and not NoData).","title":"rf_exists"},{"location":"/reference.html#rf-for-all","text":"Boolean rf_for_all(Tile tile)\nReturns true if all cells in the tile are true (non-zero and not NoData). See also `rf_is_no_data_tile, which tests that all cells are NoData.","title":"rf_for_all"},{"location":"/reference.html#rf-tile-stats","text":"Struct[Long, Long, Double, Double, Double, Double] rf_tile_stats(Tile tile)\nComputes the following statistics of cells in each row of column tile: data cell count, NoData cell count, minimum, maximum, mean, and variance. The minimum, maximum, mean, and variance are computed ignoring NoData values. Resulting column has the below schema.\nspark.sql(\"SELECT rf_tile_stats(rf_make_ones_tile(5, 5, 'float32')) as tile_stats\").printSchema()","title":"rf_tile_stats"},{"location":"/reference.html#rf-tile-histogram","text":"Struct[Array[Struct[Double, Long]]] rf_tile_histogram(Tile tile)\nComputes a count of cell values within each row of tile. The bins array is of tuples of histogram values and counts. Typically values are plotted on the x-axis and counts on the y-axis. Resulting column has the below schema. Related is the rf_agg_approx_histogram which computes the statistics across all rows in a group.\nspark.sql(\"SELECT rf_tile_histogram(rf_make_ones_tile(5, 5, 'float32')) as tile_histogram\").printSchema()","title":"rf_tile_histogram"},{"location":"/reference.html#aggregate-tile-statistics","text":"These functions compute statistical summaries over all of the cell values and across all the rows in the DataFrame or group.","title":"Aggregate Tile Statistics"},{"location":"/reference.html#rf-agg-mean","text":"Double rf_agg_mean(Tile tile)\nSQL: rf_agg_stats(tile).mean\nAggregates over the tile and return the mean of cell values, ignoring NoData. Equivalent to rf_agg_stats.mean.","title":"rf_agg_mean"},{"location":"/reference.html#rf-agg-data-cells","text":"Long rf_agg_data_cells(Tile tile)\nSQL: rf_agg_stats(tile).data_cells\nAggregates over the tile and return the count of data cells. Equivalent to rf_agg_stats.dataCells.","title":"rf_agg_data_cells"},{"location":"/reference.html#rf-agg-no-data-cells","text":"Long rf_agg_no_data_cells(Tile tile)\nSQL: rf_agg_stats(tile).no_data_cells\nAggregates over the tile and return the count of NoData cells. Equivalent to rf_agg_stats.noDataCells. C.F. rf_no_data_cells a row-wise count of no data cells.","title":"rf_agg_no_data_cells"},{"location":"/reference.html#rf-agg-stats","text":"Struct[Long, Long, Double, Double, Double, Double] rf_agg_stats(Tile tile)\nAggregates over the tile and returns statistical summaries of cell values: number of data cells, number of NoData cells, minimum, maximum, mean, and variance. The minimum, maximum, mean, and variance ignore the presence of NoData.","title":"rf_agg_stats"},{"location":"/reference.html#rf-agg-approx-histogram","text":"Struct[Array[Struct[Double, Long]]] rf_agg_approx_histogram(Tile tile)\nAggregates over all of the rows in DataFrame of tile and returns a count of each cell value to create a histogram with values are plotted on the x-axis and counts on the y-axis. Related is the rf_tile_histogram function which operates on a single row at a time.","title":"rf_agg_approx_histogram"},{"location":"/reference.html#rf-agg-approx-quantiles","text":"Array[Double] rf_agg_approx_quantiles(Tile tile, List[float] probabilities, float relative_error)\nNot supported in SQL.\nCalculates the approximate quantiles of a tile column of a DataFrame. probabilities is a list of float values at which to compute the quantiles. These must belong to [0, 1]. For example 0 is the minimum, 0.5 is the median, 1 is the maximum. Returns an array of values approximately at the specified probabilities.","title":"rf_agg_approx_quantiles"},{"location":"/reference.html#rf-agg-extent","text":"Extent rf_agg_extent(Extent extent)\nCompute the naive aggregate extent over a column. Assumes CRS homogeneity. With mixed CRS in the column, or if you are unsure, use rf_agg_reprojected_extent.","title":"rf_agg_extent"},{"location":"/reference.html#rf-agg-reprojected-extent","text":"Extent rf_agg_reprojected_extent(Extent extent, CRS source_crs, String dest_crs)\nCompute the aggregate extent over the extent and source_crs columns. The dest_crs is given as a string. Each row’s extent will be reprojected to the dest_crs before aggregating.","title":"rf_agg_reprojected_extent"},{"location":"/reference.html#tile-local-aggregate-statistics","text":"Local statistics compute the element-wise statistics across a DataFrame or group of tiles, resulting in a tile that has the same dimension.\nWhen these functions encounter NoData in a cell location, it will be ignored.","title":"Tile Local Aggregate Statistics"},{"location":"/reference.html#rf-agg-local-max","text":"Tile rf_agg_local_max(Tile tile)\nCompute the cell-local maximum operation over tiles in a column.","title":"rf_agg_local_max"},{"location":"/reference.html#rf-agg-local-min","text":"Tile rf_agg_local_min(Tile tile)\nCompute the cell-local minimum operation over tiles in a column.","title":"rf_agg_local_min"},{"location":"/reference.html#rf-agg-local-mean","text":"Tile rf_agg_local_mean(Tile tile)\nCompute the cell-local mean operation over tiles in a column.","title":"rf_agg_local_mean"},{"location":"/reference.html#rf-agg-local-data-cells","text":"Tile rf_agg_local_data_cells(Tile tile)\nCompute the cell-local count of data cells over tiles in a column. Returned tile has a cell type of int32.","title":"rf_agg_local_data_cells"},{"location":"/reference.html#rf-agg-local-no-data-cells","text":"Tile rf_agg_local_no_data_cells(Tile tile)\nCompute the cell-local count of NoData cells over tiles in a column. Returned tile has a cell type of int32.","title":"rf_agg_local_no_data_cells"},{"location":"/reference.html#rf-agg-local-stats","text":"Struct[Tile, Tile, Tile, Tile, Tile] rf_agg_local_stats(Tile tile)\nCompute cell-local aggregate count, minimum, maximum, mean, and variance for a column of tiles. Returns a struct of five tiles.","title":"rf_agg_local_stats"},{"location":"/reference.html#converting-tiles","text":"RasterFrames provides several ways to convert a tile into other data structures. See also functions for creating tiles.","title":"Converting Tiles"},{"location":"/reference.html#rf-explode-tiles","text":"Int, Int, Numeric* rf_explode_tiles(Tile* tile)\nCreate a row for each cell in tile columns. Many tile columns can be passed in, and the returned DataFrame will have one numeric column per input. There will also be columns for column_index and row_index. Inverse of rf_assemble_tile. When using this function, be sure to have a unique identifier for rows in order to successfully invert the operation.","title":"rf_explode_tiles"},{"location":"/reference.html#rf-explode-tiles-sample","text":"Int, Int, Numeric* rf_explode_tiles_sample(Double sample_frac, Long seed, Tile* tile)\nPython only. As with rf_explode_tiles, but taking a randomly sampled subset of cells. Equivalent to the rf_explode-tiles, but allows a random subset of the data to be selected. Parameter sample_frac should be between 0.0 and 1.0.","title":"rf_explode_tiles_sample"},{"location":"/reference.html#rf-tile-to-array-int","text":"Array rf_tile_to_array_int(Tile tile)\nConvert Tile column to Spark SQL Array, in row-major order. Float cell types will be coerced to integral type by flooring.","title":"rf_tile_to_array_int"},{"location":"/reference.html#rf-tile-to-array-double","text":"Array rf_tile_to_arry_double(Tile tile)\nConvert tile column to Spark Array, in row-major order. Integral cell types will be coerced to floats.","title":"rf_tile_to_array_double"},{"location":"/reference.html#rf-render-ascii","text":"String rf_render_ascii(Tile tile)\nPretty print the tile values as plain text.","title":"rf_render_ascii"},{"location":"/reference.html#rf-render-matrix","text":"String rf_render_matrix(Tile tile)\nRender Tile cell values as a string of numeric values, for debugging purposes.","title":"rf_render_matrix"},{"location":"/reference.html#rf-render-png","text":"Array rf_render_png(Tile red, Tile green, Tile blue)\nConverts three tile columns to a three-channel PNG-encoded image bytearray. First evaluates rf_rgb_composite on the given tile columns, and then encodes the result. For more about rendering these in a Jupyter or IPython environment, see @Writing Raster Data.","title":"rf_render_png"},{"location":"/reference.html#rf-render-color-ramp-png","text":"Array rf_render_png(Tile tile, String color_ramp_name)\nConverts given tile into a PNG image, using a color ramp of the given name to convert cells into pixels. color_ramp_name can be one of the following:\n“BlueToOrange” “LightYellowToOrange” “BlueToRed” “GreenToRedOrange” “LightToDarkSunset” “LightToDarkGreen” “HeatmapYellowToRed” “HeatmapBlueToYellowToRedSpectrum” “HeatmapDarkRedToYellowWhite” “HeatmapLightPurpleToDarkPurpleToWhite” “ClassificationBoldLandUse” “ClassificationMutedTerrain” “Magma” “Inferno” “Plasma” “Viridis” “Greyscale2” “Greyscale8” “Greyscale32” “Greyscale64” “Greyscale128” “Greyscale256”\nFurther descriptions of these color ramps can be found in the Geotrellis Documentation. For more about rendering these in a Jupyter or IPython environment, see @Writing Raster Data.","title":"rf_render_color_ramp_png"},{"location":"/reference.html#rf-agg-overview-raster","text":"Tile rf_agg_overview_raster(Tile proj_raster_col, int cols, int rows, Extent aoi)\nTile rf_agg_overview_raster(Tile tile_col, int cols, int rows, Extent aoi, Extent tile_extent_col, CRS tile_crs_col)\nConstruct an overview tile of size cols by rows. Data is filtered to the specified aoi which is given in web mercator. Uses bi-linear sampling method. The tile_extent_col and tile_crs_col arguments are optional if the first argument has its Extent and CRS embedded.","title":"rf_agg_overview_raster"},{"location":"/reference.html#rf-rgb-composite","text":"Tile rf_rgb_composite(Tile red, Tile green, Tile blue)\nMerges three bands into a single byte-packed RGB composite. It first scales each cell to fit into an unsigned byte, in the range 0-255, and then merges all three channels to fit into a 32-bit unsigned integer. This is useful when you want an RGB tile to render or to process with other color imagery tools.","title":"rf_rgb_composite"},{"location":"/release-notes.html","text":"","title":"Release Notes"},{"location":"/release-notes.html#release-notes","text":"","title":"Release Notes"},{"location":"/release-notes.html#0-9-x","text":"","title":"0.9.x"},{"location":"/release-notes.html#0-9-1","text":"Upgraded to Spark 2.4.7 Added pyspark.sql.DataFrame.display(num_rows:int, truncate:bool) extension method when rf_ipython is imported. Added users’ manual section on IPython display enhancements. Added method_name parameter to the rf_resample method. BREAKING: In SQL, the function rf_resample now takes 3 arguments. You can use rf_resample_nearest with two arguments or refactor to rf_resample(t, v, \"nearest\"). Added resample method parameter to SQL and Python APIs. See updated docs. Upgraded many of the pyrasterframes dependencies, including: descartes, fiona, folium, geopandas, matplotlib, numpy, pandas, rasterio, shapely Changed rasterframes.prefer-gdal configuration parameter to default to False, as JVM GeoTIFF performs just as well for COGs as the GDAL one. Fixed #545.","title":"0.9.1"},{"location":"/release-notes.html#0-9-0","text":"Upgraded to GeoTrellis 3.3.0. This includes a number of breaking changes enumerated as a part of the PR’s change log. These include: Add Int type parameter to Grid Add Int type parameter to CellGrid Add Int type parameter to GridBounds… or TileBounds Use GridBounds.toGridType to coerce from Int to Long type parameter Update imports for layers, particularly geotrellis.spark.tiling to geotrellis.layer Update imports for geotrellis.spark.io to geotrellis.spark.store... Removed FixedRasterExtent Removed FixedDelegatingTile Removed org.locationtech.rasterframes.util.Shims Change Extent.jtsGeom to Extent.toPolygon Change TileLayerMetadata.gridBounds to TileLayerMetadata.tileBounds Add geotrellis-gdal dependency Remove any conversions between JTS geometry and old geotrellis.vector geometry Changed org.locationtech.rasterframes.encoders.StandardEncoders.crsEncoder to crsSparkEncoder Change (cols, rows) dimension destructuring to Dimensions(cols, rows) Revisit use of Tile equality since it’s more strict Update reference.conf to use geotrellis.raster.gdal namespace. Replace all uses of TileDimensions with geotrellis.raster.Dimensions[Int]. Upgraded to gdal-warp-bindings 1.0.0. Upgraded to Spark 2.4.5 Formally abandoned support for Python 2. Python 2 is dead. Long live Python 2. Introduction of type hints in Python API. Add functions for changing cell values based on either conditions or to achieve a distribution of values. (#449) Add rf_local_min, rf_local_max, and rf_local_clip functions. Add cell value scaling functions rf_rescale and rf_standardize. Add rf_where function, similar in spirit to numpy’s where, or a cell-wise version of Spark SQL’s when and otherwise. Add rf_sqrt function to compute cell-wise square root.","title":"0.9.0"},{"location":"/release-notes.html#0-8-x","text":"","title":"0.8.x"},{"location":"/release-notes.html#0-8-5","text":"Added rf_z2_index for constructing a Z2 index on types with bounds. Breaking: rf_spatial_index renamed rf_xz2_index to differentiate between XZ2 and Z2 variants. Added withSpatialIndex to RasterSourceDataSource to pre-partition tiles based on tile extents mapped to a Z2 space-filling curve Add rf_mask_by_bit, rf_mask_by_bits and rf_local_extract_bits to deal with bit packed quality masks. Updated the masking documentation to demonstrate the use of these functions. Added toDF extension method to MultibandGeoTiff Added rf_agg_extent and rf_agg_reprojected_extent to compute the aggregate extent of a column Added rf_proj_raster for constructing a proj_raster structure from individual CRS, Extent, and Tile columns. Added rf_render_color_ramp_png to compute PNG byte array for a single tile column, with specified color ramp. In rf_ipython, improved rendering of dataframe binary contents with PNG preamble. Throw an IllegalArgumentException when attempting to apply a mask to a Tile whose CellType has no NoData defined. (#409) Add rf_agg_approx_quantiles function to compute cell quantiles across an entire column.","title":"0.8.5"},{"location":"/release-notes.html#0-8-4","text":"Upgraded to Spark 2.4.4 Add rf_mask_by_values and rf_local_is_in raster functions; added optional inverse argument to rf_mask functions. (#403, #384) Added forced truncation of WKT types in Markdown/HTML rendering. (#408) Add rf_local_is_in raster function. (#400) Added partitioning to catalogs before processing in RasterSourceDataSource (#397) Fixed bug where rf_tile_dimensions would cause unnecessary reading of tiles. (#394) Breaking (potentially): removed GeoTiffCollectionRelation due to usage limitation and overlap with RasterSourceDataSource functionality.","title":"0.8.4"},{"location":"/release-notes.html#0-8-3","text":"Updated to GeoTrellis 2.3.3 and Proj4j 1.1.0. Fixed issues with LazyLogger and shading assemblies (#293) Updated rf_crs to accept string columns containing CRS specifications. (#366) Added rf_spatial_index function. (#368) Breaking (potentially): removed pyrasterframes.create_spark_session in lieu of pyrasterframes.utils.create_rf_spark_session","title":"0.8.3"},{"location":"/release-notes.html#0-8-2","text":"Added ability to pass config options to convenience PySpark session constructor. (#361) Bumped Spark dependency to version 2.3.4. (#350) Fixed handling of aggregate extent and image size on GeoTIFF writing. (#362) Fixed issue with RasterSourceDataSource swallowing exceptions. (#267) Fixed SparkML memory pressure issue caused by unnecessary reevaluation, overallocation, and primitive boxing. (#343) Fixed Parquet serialization issue with RasterRefs (#338) Fixed TileExploder, rf_agg_local_mean and TileColumnSupport to support proj_raster struct (#287, #163, #333). Various documentation improvements. Breaking (potentially): Synchronized parameter naming in Python and Scala for spark.read.raster (#329).","title":"0.8.2"},{"location":"/release-notes.html#0-8-1","text":"Added rf_local_no_data, rf_local_data and rf_interpret_cell_type_as raster functions. Added: rf_rgb_composite and rf_render_png. Added toMarkdown and toHTML extension methods for DataFrame, and registered them with the IPython formatter system when rf_ipython is imported. New documentation theme (thanks @jonas!). Fixed: Removed false return type guarantee in cases where an Expression accepts either Tile or ProjectedRasterTile (#295)","title":"0.8.1"},{"location":"/release-notes.html#0-8-0","text":"Super-duper new Python-centric RasterFrames Users’ Manual! Upgraded to the following core dependencies: Spark 2.3.3, GeoTrellis 2.3.0, GeoMesa 2.2.1, JTS 1.16.0. Build pyrasterframes binary distribution for pip installation. Added support for rendering RasterFrame types in IPython/Jupyter. Added new tile functions rf_round, rf_abs, rf_log, rf_log10, rf_log2, rf_log1p, rf_exp, rf_exp10, rf_exp2, rf_expm1, rf_resample. Support Python-side Tile User-Defined Type backed by numpy ndarray or ma.MaskedArray. Support Python-side Shapely geometry User-Defined Type. SQL API support for rf_assemble_tile and rf_array_to_tile. Introduced at the source level the concept of a RasterSource and RasterRef, enabling lazy/delayed read of sub-scene tiles. Added withKryoSerialization extension methods on SparkSession.Builder and SparkConf. Added rf_render_matrix debugging function. Added RasterFrameLayer.withExtent extension method. Added SinglebandGeoTiff.toDF extension method. Added DataFrame.rasterJoin extension method for merging two dataframes with tiles in disparate CRSs. Added rf_crs for ProjectedRasterTile columns. Added st_extent (for Geometry types) and rf_extent (for ProjectedRasterTile and RasterSource columns). Added st_geometry (for Extent types) and rf_geometry (for ProjectedRasterTile and RasterSource columns). Reworked build scripts for RasterFrames Jupyter Notebook. Breaking: The type RasterFrame renamed RasterFrameLayer to be reflect its intended purpose. Breaking: All asRF methods renamed to asLayer. Breaking: Root package changed from org.locationtech.rasterframes to org.locationtech.rasterframes. Breaking: Removed envelope, in lieu of st_extent, rf_extent or st_envelope Breaking: Renamed rf_extent_geometry to st_geometry Breaking: Renamed rf_tile_dimensions to rf_dimensions Breaking: Renamed rf_reproject_geometry to st_reproject Breaking: With the upgrade to JTS 1.16.0, all imports of com.vividsolutions.jts need to be changed to org.locationtech.jts. Deprecation: Tile column functions (in RasterFunctions) and SQL registered names have all been renamed to follow snake_case conventions, with an rf_ prefix, matching SQL and Python. A temporary compatibility shim is included so that code built against 0.7.1 and earlier still work. These will be marked as deprecated. Breaking: In Scala and SQL, ..._scalar functions (e.g. local_add_scalar) have been removed. Non-scalar forms now dynamically detect type of right hand side. Breaking: tileToArray has been replaced with _tile_to_array_double and _tile_to_array_int. Breaking: Renamed bounds_geometry to rf_extent_geometry. Breaking: renamed agg_histogram to rf_agg_approx_histogram, local_agg_stats to rf_agg_local_stats, local_agg_max to rf_agg_local_max, local_agg_min to rf_agg_local_min, local_agg_mean to rf_agg_local_mean, local_agg_data_cells to rf_agg_local_data_cells, local_agg_no_data_cells to rf_agg_local_no_data_cells. Breaking: CellHistogram no longer carries along approximate statistics, due to confusing behavior. Use rf_agg_stats instead. Introduced LocalCellStatistics class to wrap together results from LocalStatsAggregate. Breaking: TileDimensions moved from astraea.spark.rasterframes to org.locationtech.rasterframes.model. Breaking: Renamed RasterFrame.withBounds to RasterFrameLayer.withGeometry for consistency with DataSource schemas.","title":"0.8.0"},{"location":"/release-notes.html#known-issues","text":"#188: Error on deserialization of a Tile with a bool cell type to the Python side; see issue description for work around.","title":"Known issues"},{"location":"/release-notes.html#0-7-x","text":"","title":"0.7.x"},{"location":"/release-notes.html#0-7-1","text":"Fixed ColorRamp pipeline in MultibandRender Fixed Python wrapper for explodeTiles","title":"0.7.1"},{"location":"/release-notes.html#0-7-0","text":"Now an incubating project under Eclipse Foundation LocationTech! GitHub repo moved to locationtech/rasterframes. PySpark support! See pyrasterframes/python/README.rst to get started. Exposed Spark JTS spatial operations in Python. Added RasterFrames-enabled Jupyter Notebook Docker Container package. See deployment/README.md for details. Updated to GeoMesa version 2.0.1. Added convertCellType, normalizedDifference mask and inverseMask operations on tile columns. Added tile column + scalar operations: localAddScalar, localSubtractScalar, localMultiplyScalar, localDivideScalar Added rasterize and reprojectGeometry operations on geometry columns. Added for for writing GeoTIFFs from RasterFrames via DataFrameWriter. Added spark.read.geotrellis.withNumPartitions(Int) for setting the initial number of partitions to use when reading a layer. Added spark.read.geotrellis.withTileSubdivisions(Int) for evenly subdividing tiles before they become rows in a RasterFrame. Added experimental package for sandboxing new feature ideas. Added experimental GeoJSON DataSource with schema inferfence on feature properties. Added Scala, SQL, and Python tile-scalar arithmetic operations: localAddScalar, localSubtractScalar, localMultipyScalar, localDivideScalar. Added Scala, SQL, and Python tile functions for logical comparisons both tile-tile and tile-scalar variants: localLess, localLessEqual, localGreater, localGreaterEqual, localEqual, and localUnequal. Added SlippyExport experimental feature for exporting the contents of a RasterFrame as a SlippyMap tile image directory structure and Leaflet/OpenMaps-enabled HTML file. Added experimental DataSource implementations for MODIS and Landsat 8 catalogs on AWS PDS. Change: Default interpoation for toRaster and toMultibandRaster has been changed from Bilinear to NearestNeighbor. Breaking: Renamed/moved astraea.spark.rasterframes.functions.CellStatsAggregateFunction.Statistics to astraea.spark.rasterframes.stats.CellStatistics. Breaking: HistogramAggregateFunction now generates the new type astraea.spark.rasterframes.stats.CellHistogram. Breaking: box2D renamed envelope.","title":"0.7.0"},{"location":"/release-notes.html#0-6-x","text":"","title":"0.6.x"},{"location":"/release-notes.html#0-6-1","text":"Added support for reading striped GeoTiffs (#64). Moved extension methods associated with querying tagged columns to DataFrameMethods for supporting temporal and spatial columns on non-RasterFrame DataFrames. GeoTIFF and GeoTrellis DataSources automatically initialize RasterFrames. Added RasterFrame.toMultibandRaster. Added utility for rendering multiband tile as RGB composite PNG. Added RasterFrame.withRFColumnRenamed to lessen boilerplate in maintaining RasterFrame type tag.","title":"0.6.1"},{"location":"/release-notes.html#0-6-0","text":"Upgraded to Spark 2.2.1. Added VersionShims to allow for Spark 2.1.x backwards compatibility. Introduced separate rasterframes-datasource library for hosting sources from which to read RasterFrames. Implemented basic (but sufficient) temporal and spatial filter predicate push-down feature for the GeoTrellis layer datasource. Added Catalyst expressions specifically for spatial relations, allowing for some polymorphism over JTS types. Added a GeoTrellis Catalog DataSource for inspecting available layers and associated metadata at a URI Added GeoTrellis Layer DataSource for reading GeoTrellis layers from any SPI-registered GeoTrellis backend (which includes HDFS, S3, Accumulo, HBase, Cassandra, etc.). Ability to save a RasterFrame as a GeoTrellis layer to any SPI-registered GeoTrellis backends. Multi-column RasterFrames are written as Multiband tiles. Addd a GeoTiff DataSource for directly loading a (preferably Cloud Optimized) GeoTiff as a RasterFrame, each row containing tiles as they are internally organized. Fleshed out support for MultibandTile and TileFeature support in datasource. Added typeclass for specifying merge operations on TileFeature data payload. Added withTemporalComponent convenince method for creating appending a temporal key column with constant value. Breaking: Renamed withExtent to withBounds, and now returns a JTS Polygon. Added EnvelopeEncoder for encoding JTS Envelope type. Refactored build into separate core and docs, paving way for pyrasterframes polyglot module. Added utility extension method withPrefixedColumnNames to DataFrame.","title":"0.6.0"},{"location":"/release-notes.html#known-issues","text":"Writing multi-column RasterFrames to GeoTrellis layers requires all tiles to be of the same cell type.","title":"Known Issues"},{"location":"/release-notes.html#0-5-x","text":"","title":"0.5.x"},{"location":"/release-notes.html#0-5-12","text":"Added withSpatialIndex to introduce a column assigning a z-curve index value based on the tile’s centroid in EPSG:4326. Added column-appending convenience methods: withExtent, withCenter, withCenterLatLng Documented example of creating a GeoTrellis layer from a RasterFrame. Added Spark 2.2.0 forward-compatibility. Upgraded to GeoTrellis 1.2.0-RC2.","title":"0.5.12"},{"location":"/release-notes.html#0-5-11","text":"Significant performance improvement in explodeTiles (1-2 orders of magnitude). See #38 Fixed bugs in NoData handling when converting to Double tiles.","title":"0.5.11"},{"location":"/release-notes.html#0-5-10","text":"Upgraded to shapeless 2.3.2 Fixed #36, #37","title":"0.5.10"},{"location":"/release-notes.html#0-5-9","text":"Ported to sbt 1.0.3 Added sbt-generated astraea.spark.rasterframes.RFBuildInfo Fixed bug in computing aggMean when one or more tiles are null Deprecated rfIinit in favor of SparkSession.withRasterFrames or SQLContext.withRasterFrames extension methods","title":"0.5.9"},{"location":"/release-notes.html#0-5-8","text":"Upgraded to GeoTrellis 1.2.0-RC1 Added REPLsent-based tour of RasterFrames Moved Giter8 template to separate repository s22s/raster-frames.g8 due to sbt limitations Updated Getting Started to reference new Giter8 repo Changed SQL function name rf_stats and rf_histogram to rf_aggStats and rf_aggHistogram for consistency with DataFrames API","title":"0.5.8"},{"location":"/release-notes.html#0-5-7","text":"Created faster implementation of aggregate statistics. Fixed bug in deserialization of TileUDTs originating from ConstantTiles Fixed bug in serialization of NoDataFilter within SparkML pipeline Refactoring of UDF organization Various documentation tweaks and updates Added Giter8 template","title":"0.5.7"},{"location":"/release-notes.html#0-5-6","text":"TileUDFs are encoded using directly into Catalyst–without Kryo–resulting in an insane decrease in serialization time for small tiles (int8, <= 128²), and pretty awesome speedup for all other cell types other than float32 (marginal slowing). While not measured, memory footprint is expected to have gone down.","title":"0.5.6"},{"location":"/release-notes.html#0-5-5","text":"aggStats and tileMean functions rewritten to compute simple statistics directly rather than using StreamingHistogram tileHistogramDouble and tileStatsDouble were replaced by tileHistogram and tileStats Added tileSum, tileMin and tileMax functions Added aggMean, aggDataCells and aggNoDataCells aggregate functions. Added localAggDataCells and localAggNoDataCells cell-local (tile generating) fuctions Added tileToArray and arrayToTile Overflow fix in LocalStatsAggregateFunction","title":"0.5.5"}]}